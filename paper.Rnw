\documentclass[AMS,STIX2COL]{WileyNJD-v2}

% Packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{xcolor}

% Commands
\definecolor{orange}{rgb}{0.94, 0.59, 0.19}
\definecolor{purple}{rgb}{0.5, 0.0, 0.5}
\definecolor{teal}{rgb}{0, 0.502, 0.502}
\newcommand{\hh}[1]{\textcolor{orange}{#1}}
\newcommand{\kgc}[1]{\textcolor{purple}{#1}}
\newcommand{\kge}[1]{\textcolor{teal}{#1}}
\newcommand{\data}{sine data}

% Set up for ASA data science journal
\articletype{\kge{Research Article}}
\received{\kge{XX August 2020}}
\revised{\kge{XX XX XXXX}}
\accepted{\kge{XX XX XXXX}}
\raggedbottom

\begin{document}

% Paper header (title and authors)
\title{Visual Diagnostics of a Model Explainer -- Tools for the Assessment of LIME Explanations}
\author[1]{Katherine Goode*}
\author[1,2]{Heike Hofmann}
\authormark{Goode and Hofmann}
\address[1]{\orgdiv{Department of Statistics}, \orgname{Iowa State University}, \orgaddress{\state{Iowa}, \country{United States}}}
\address[2]{\orgdiv{Center for Statistics and Applications in Forensic Evidence (CSAFE)}, \orgname{Iowa State University}, \orgaddress{\state{Iowa}, \country{United States}}}
\corres{*Corresponding author. \email{kgoode@iastate.edu}}

% Abstract and keywords
\abstract[Summary]{The importance of providing explanations for predictions made by black-box models has led to the development of model explainer methods such as LIME (local interpretable model-agnostic explanations) \citep{ribeiro:2016}. LIME uses a surrogate model to explain the relationship between predictor variables and predictions from a black-box model in a local region around a prediction of interest. However, the quality of the resulting explanations relies on how well the explainer model captures the black-box model in a specified local region. Here we introduce three visual diagnostics to assess the quality of LIME explanations: (1) explanation scatterplots, (2) assessment metric plots, and (3) feature heatmaps. We apply the visual diagnostics to a forensics bullet matching dataset to show examples where LIME explanations depend on the tuning parameters and the explainer model oversimplifies the black-box model. Our examples raise concerns about claims made of LIME \citep{ribeiro:2016} that are similar to other criticisms in the literature \citep{alvarezmelis:2018, laugel:2018, molnar:2019}.}
\keywords{LIME, black-box models, interpretability, diagnostics}

% Citation information
\jnlcitation{
\cname{\author{Goode K.}, \author{H. Hofmann}, }
\cyear{2019},
\ctitle{Visual Diagnostics of a Model Explainer -- Tools for the Assessment of LIME Explanations},
\cjournal{Stat Anal Data Min: The ASA Data Sci Journal},
\cvol{volume, number and page}.}

\maketitle

<<setup, echo = FALSE, include = FALSE>>=

# Specify global options
knitr::opts_chunk$set(echo = FALSE,
                      fig.keep = 'high',
                      message = FALSE)

# Load packages
library(caret)
library(cowplot)
library(dplyr)
library(forcats)
library(ggforce)
library(ggplot2)
library(ggpcp) #devtools::install_github("yaweige/ggpcp", build_vignettes = TRUE)
library(glmnet)
library(gower)
library(graphics)
library(gretchenalbrecht) #devtools::install_github("dicook/gretchenalbrecht")
library(latex2exp)
library(lime) #devtools::install_github("goodekat/lime")
library(limeaid) #devtools::install_github("goodekat/limeaid")
library(purrr)
library(randomForest)
library(stringr)
library(tidyr)

@

\section{Introduction}

In the field of statistics, there are two main uses for models: inference and prediction. Machine learning models are often used for the latter purpose. These models have proven to perform well in a wide range of prediction problems, but the accuracy of many machine learning models comes at the cost of interpretability due to their algorithmic complexity (hence the phrase "black-box models"). Model interpretability allows for the understanding and assessment of how a model produces predictions. The lack of the ability to understand and assess a model makes it difficult to trust the model, especially in areas with high stakes decisions such as medical and forensics sciences. The increased use of machine learning models in applications and the introduction of the General Data Protection Regulation (GDPR) in 2018 \citep{goodman:2016} has resulted in a dramatic increase in explainable machine learning research, which focuses on developing ways to explain output from machine learning algorithms.

Throughout this paper, we distinguish between interpretability and explanability of models. We define {\it interpretability} as the ability to directly use model parameters to understand the relationships in the data captured by the model: e.g., a linear model coefficient associated with a predictor variable indicates the amount the response variable changes based on a change in the predictor variable. In contrast, define {\it explanability} as the ability to use the model in an indirect manner to understand the relationships in the data captured by the model: e.g., partial dependence plots depict the marginal relationship between model predictions and predictor variables \citep{friedman:2001}.

Numerous methods have been proposed to provide explanations for black-box model predictions \citep{gilpin:2018, guidotti:2018, ming:2017, molnar:2019}. Some are specific to one type of model (e.g. \citep{simonyan:2013} and \citep{urbanek:2008}), and others are model-agnostic (e.g. \citep{fisher:2018} and \citep{strumbelj:2014}). In this paper, we focus on one specific model-agnostic method: LIME \citep{ribeiro:2016}.

LIME (local interpretable model-agnostic explanations) is a method that uses a surrogate model to relate predictor variables to black-box model predictions (i.e. a model explainer) \citep{ribeiro:2016}. We distinguish between the terms of model explainer and explainer model: by \emph{model explainer} we denote the method for explaining a complex model using a surrogate model, while the \emph{explainer model}, or simply the \emph{explainer}, is the surrogate model.

While some model explainers focus on understanding a model at the global level, LIME claims to provide explanations for individual predictions (local). Additionally, LIME is designed to work with any model (model-agnostic) and to produce easily understandable results (explanations) \citep{ribeiro:2016}. Conceptually, LIME fits a simple (interpretable) model, the explainer model,  meant to capture the behavior of the (complex) black-box model in a local region around a prediction of interest. The  simple model then provides interpretable estimates for variables that most influenced the prediction made by the complex model.

<<concept-data>>=
# Simulate example data
set.seed(20190624)
lime_data <-
  data.frame(
    feature1 = sort(runif(250, 0, 1)),
    feature2 = sample(x = 1:250, size = 250, replace = FALSE)) %>%
  mutate(
    feature1_stnd = (feature1 - mean(feature1)) / sd(feature1),
    feature2_stnd = (feature2 - mean(feature2)) / sd(feature2),
    prediction = if_else(feature1 >= 0 & feature1 < 0.1,
                         (0.3 * feature1) + rnorm(n(), 0, 0.01),
                 if_else(feature1 >= 0.1 & feature1 < 0.3,
                         rbeta(n(), 1, 0.5),
                 if_else(feature1 >= 0.3 & feature1 < 0.5,
                         sin(pi* feature1) + rnorm(n(), 0, 0.5),
                 if_else(feature1 >= 0.5 & feature1 < 0.8,
                         -(sin(pi* feature1) + rnorm(n(), 0, 0.1)) + 1,
                 if_else(feature1 >= 0.8 & feature1 < 0.9,
                         0.5 + runif(n(), -0.5, 0.5),
                         0.5 + rnorm(n(), 0, 0.3))))))) %>%
  bind_rows(
    data.frame(feature1 = rep(1, 30) + rnorm(5, 0, 0.05),
           feature2 = rep(245, 30) + rnorm(5, 0, 1),
           prediction = rep(0, 30) + rnorm(5, 0, 0.05)) %>%
  mutate(feature1_stnd = (feature1 - mean(feature1)) / sd(feature1),
         feature2_stnd = (feature2 - mean(feature2)) / sd(feature2)))

# Specify a prediction of interest
prediction_of_interest <-
  data.frame(feature1 = 0.07, feature2 = 200) %>%
  mutate(feature1_stnd = (feature1 - mean(lime_data$feature1)) /
           sd(lime_data$feature1),
         feature2_stnd = (feature2 - mean(lime_data$feature2)) /
           sd(lime_data$feature2),
         prediction = 0.05,
         color = factor("Prediction \nof Interest"))

# Specify the gower exponents
good_gower_power <- 50
bad_gower_power <- 1

# Compute the good distances between the prediction of interest
# and all other observations
lime_data$distance_good <-
  (1 - gower_dist(x = prediction_of_interest %>%
                    select(feature1_stnd, feature2_stnd),
                  y = lime_data %>%
                    select(feature1_stnd,
                           feature2_stnd)))^good_gower_power

# Compute the bad distances between the prediction of interest
# and all other observations
lime_data$distance_bad <-
  (1 - gower_dist(x = prediction_of_interest %>%
                    select(feature1_stnd, feature2_stnd),
                  y = lime_data %>%
                    select(feature1_stnd,
                           feature2_stnd)))^bad_gower_power

# Prepare the data for plotting
lime_data_gathered <- lime_data %>%
  gather(feature, feature_stnd_value,
         feature1_stnd:feature2_stnd) %>%
  gather(distance, distance_value, distance_good:distance_bad) %>%
  mutate(distance = fct_recode(distance,
                               "good" = "distance_good",
                               "bad" = "distance_bad"),
         feature = fct_recode(feature,
                              "Feature 1" = "feature1_stnd",
                              "Feature 2" = "feature2_stnd"))

# Prepare the prediction of interest data for plotting
prediction_of_interest_gathered <- prediction_of_interest %>%
  select(-feature1, -feature2) %>%
  gather(feature, feature_value, feature1_stnd, feature2_stnd) %>%
  mutate(feature = fct_recode(feature,
                              "Feature 1" = "feature1_stnd",
                              "Feature 2" = "feature2_stnd"))

@

<<concept-explainers>>=

# Fit the good interpretable explainer model
explainer_good <-
  glmnet(x = lime_data %>% select(feature1_stnd, feature2_stnd)
         %>% as.matrix(),
         y = lime_data$prediction,
         alpha = 0,
         lambda = 1,
         weights = lime_data$distance_good)

# Fit the bad interpretable explainer model
explainer_bad <-
  glmnet(x = lime_data %>% select(feature1_stnd, feature2_stnd)
         %>% as.matrix(),
         y = lime_data$prediction,
         alpha = 0,
         lambda = 1,
         weights = lime_data$distance_bad)

# Join the coefficients from the explainer model into a dataframe
coefs_data <- data.frame(case = c("good", "bad"),
                         b0 = c(coef(explainer_good)[1],
                                coef(explainer_bad)[1]),
                         b1 = c(coef(explainer_good)[2],
                                coef(explainer_bad)[2]),
                         b2 = c(coef(explainer_good)[3],
                                coef(explainer_bad)[3])) %>%
  mutate(feature1 = b0 + b2*prediction_of_interest$feature2_stnd,
         feature2 = b0 + b1*prediction_of_interest$feature1_stnd) %>%
  gather(key = feature, value = int, feature1:feature2) %>%
  mutate(slope = c(coef(explainer_good)[2],
                   coef(explainer_good)[3],
                   coef(explainer_bad)[2],
                   coef(explainer_bad)[3]),
         feature = fct_recode(feature,
                              "Feature 1" = "feature1",
                              "Feature 2" = "feature2"))

@

\begin{figure*}[!thp]
<<concept-plot-good, out.width = '\\textwidth', fig.width = 7.95, fig.height = 3.75, warning = FALSE>>=

# Plot of good explainer model
ggplot() +
  facet_grid(. ~ feature, switch = "x") +
  geom_point(data = lime_data_gathered,
             mapping = aes(x = feature_stnd_value,
                           y = prediction,
                           size = distance_value,
                           alpha = distance_value,
                           color = distance)) +
  geom_abline(data = coefs_data %>% filter(case == "good"),
              mapping = aes(intercept = int, slope = slope),
              size = 1) +
  scale_alpha(range = c(0.3, 1)) +
  scale_color_manual(values = c(NA, "grey30"), guide = "none") +
  geom_point(data = prediction_of_interest_gathered,
             mapping = aes(x = feature_value,
                           y = prediction,
                           fill = color),
             color = "black",
             size = 5,
             shape = 23,
             alpha = 0.75) +
  scale_fill_manual(values = "#FAAA72") +
  geom_text(
    data = data.frame(feature_stnd_value = 1.2,
                      prediction = 1.6,
                      feature = c("Feature 1", "Feature 2"),
                      slope = c(paste("Slope:",
                                      coefs_data %>%
                                        filter(case == "good",
                                               feature == "Feature 1") %>%
                                        pull(slope) %>%
                                        round(3)),
                                paste("Slope:",
                                      coefs_data %>%
                                        filter(case == "good",
                                               feature == "Feature 2") %>%
                                        pull(slope) %>%
                                        round(3)))),
    mapping = aes(x = feature_stnd_value,
                  y = prediction,
                  label = slope),
    family = "Times",
    size = 4) +
  labs(x = "",
       y = "Black-Box Prediction",
       title = "Conceptual Depiction of a Faithful Local Explainer Model",
       subtitle = paste("Gower Distance Metric Exponent:", good_gower_power),
       fill = "",
       alpha = "Weight",
       size = "Weight") +
  theme_linedraw(base_family = "Times", base_size = 10) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        strip.placement = "outside",
        strip.background = element_rect(color = "white", fill = "white"),
        strip.text = element_text(color = 'black', size = 10)) +
  guides(fill = guide_legend(order = 1),
         size = guide_legend(order = 2),
         alpha = guide_legend(order = 2))

@
\caption{A conceptual depiction of a faithful LIME explainer model in the immediate neighborhood of a prediction of interest. The predictions from a hypothetical black-box model are plotted against the standardized values of the two hypothetical predictor variables. The diamond shaped points represent the location of a prediction of interest. The size and opacity of the circular points indicate the weight assigned based on the distance to the prediction of interest computed using the inverse of the Gower distance metric raised to the power of \Sexpr{good_gower_power}. The black lines represent  a weighted ridge regression model used as an explainer model that reasonably captures the relationship between the black-box predictions and the features in a local region around the prediction of interest. That is, the explainer is faithful to the complex model and produces a reasonable explanation that Feature 1 plays a more important role in the prediction of interest than Feature 2 since the magnitude of the slope associated with Feature 1 is larger.}
\label{fig:concept-plot-good}

\vspace*{\floatsep}
 
<<concept-plot-bad, out.width = '\\textwidth', fig.width = 7.95, fig.height = 3.75, warning = FALSE>>=

# Plot of good explainer model
ggplot() +
  facet_grid(. ~ feature, switch = "x") +
  geom_point(data = lime_data_gathered,
             mapping = aes(x = feature_stnd_value,
                           y = prediction,
                           size = distance_value,
                           alpha = distance_value,
                           color = distance)) +
  geom_abline(data = coefs_data %>% filter(case == "bad"),
              mapping = aes(intercept = int, slope = slope),
              size = 1) +
  scale_alpha(range = c(0.3, 1)) +
  scale_color_manual(values = c("grey30", NA), guide = "none") +
  geom_point(data = prediction_of_interest_gathered,
             mapping = aes(x = feature_value,
                           y = prediction,
                           fill = color),
             color = "black",
             size = 5,
             shape = 23,
             alpha = 0.75) +
  scale_fill_manual(values = "#FAAA72") +
  geom_text(
    data = data.frame(feature_stnd_value = 1.2,
                      prediction = 1.6,
                      feature = c("Feature 1", "Feature 2"),
                      slope = c(paste("Slope:",
                                      coefs_data %>%
                                        filter(case == "bad",
                                               feature == "Feature 1") %>%
                                        pull(slope) %>%
                                        round(3)),
                                paste("Slope:",
                                      coefs_data %>%
                                        filter(case == "bad",
                                               feature == "Feature 2") %>%
                                        pull(slope) %>%
                                        round(3)))),
    mapping = aes(x = feature_stnd_value,
                  y = prediction,
                  label = slope),
    family = "Times",
    size = 4) +
  labs(x = "",
       y = "Black-Box Prediction",
       title = "Conceptual Depiction of an Unfaithful Local Explainer Model",
       subtitle = paste("Gower Distance Metric Exponent:", bad_gower_power),
       fill = "",
       alpha = "Weight",
       size = "Weight") +
  theme_linedraw(base_family = "Times", base_size = 10) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        strip.placement = "outside",
        strip.background = element_rect(color = "white", fill = "white"),
        strip.text = element_text(color = 'black', size = 10)) +
  guides(fill = guide_legend(order = 1),
         size = guide_legend(order = 2),
         alpha = guide_legend(order = 2))

@
\caption{A conceptual depiction of an unfaithful local explainer model in the immediate neighborhood of a prediction of interest. A ridge regression model is fit to the same hypothetical black-box model predictions and predictor variables as in \autoref{fig:concept-plot-good}, but the weights are computed using a Gower exponent of 1. The explainer model is unfaithful to the complex model in any immediate neighborhood of the prediction of interest.}
\label{fig:concept-plot-bad}
\end{figure*}

\autoref{fig:concept-plot-good} provides a visualization of this conceptual understanding of LIME. The two plots show the predictions from a hypothetical black-box model plotted against the two hypothetical model predictor variables. The diamond shaped points represent the location of the prediction of interest. A Gower distance metric \citep{gower:1971} is used to define locality such that the size of the points represent the inverse of the Gower distance raised to some value and indicate the proximity to the prediction of interest. In the example of \autoref{fig:concept-plot-good}, an exponent of \Sexpr{good_gower_power} is used to emphasize a very local region around the prediction of interest. A ridge regression model weighted by the proximity values is used as the explainer model with the black-box predictions as the response variable and standardized versions of two features in the data as predictor variables. Standardized features allow direct comparisons of the model coefficients. The explainer model is depicted by the black lines in the figure.

The plot on the left of \autoref{fig:concept-plot-good} shows the relationship between the black-box predictions and Feature 1. Here, the explainer model is plotted with Feature 2 fixed at the observed value of the prediction of interest. The explainer model captures the relationship in an  immediate neighborhood around the prediction of interest with a slope of \Sexpr{coefs_data %>% filter(case == "good", feature == "Feature 1") %>% pull(slope) %>% round(3)}. The plot on the right shows no global or local relationships between the black-box predictions and Feature 2. Here, the explainer model is plotted with Feature 1 set as the observed value of the prediction of interest, and it has an appropriately small slope of \Sexpr{coefs_data %>% filter(case == "good", feature == "Feature 2") %>% pull(slope) %>% round(3)}. The magnitude of the slope associated with Feature 1 is larger than the slope of Feature 2, which suggests that Feature 1 plays a more important role in the prediction made by the black-box model for the prediction of interest. This explanation agrees with a visual assessment of the relationships between the predictions and predictor variables.

The concept of LIME is relatively simple: use an interpretable model to approximate a complex model in a local region. However, a practical implementation of LIME is not straightforward, and research is being done to improve the procedure \citep{laugel:2018}. The current implementations of LIME \citep{ribeiro:2020} \kgc{Is there a better way to cite a Python package?} \hh{might be good to model the python package reference after the R package references, have a loog at reference limepy} \kgc{I kept thinking about this, and since the Python pakcgae doesn't have a manual, I ended up combining your limepy reference with the second second suggestion from https://academia.stackexchange.com/questions/14010/how-do-you-cite-a-github-repository. For now, I listed Ribeiro as the author, because he is the main contributor, but there are 40 other contributors to the package. Is it okay to just list Ribeiro?} \citep{pedersen:2020} offer various tuning parameters (see Section \ref{background}) that affect the explainer model and ultimately, the explanation. Since the explainer model is an approximation of the complex model and not a direct interpretation, the explanations produced by an explainer model are subject to the quality of the approximation. Thus, in order to achieve accurate explanations, the tuning parameters selected need to be assessed. 

We consider an example where the choice of exponent for the weights is crucial to the quality of the explanation. The plots in Figure \ref{fig:concept-plot-bad} show the same data as \autoref{fig:concept-plot-good}, but the Gower distance metric exponent is decreased to \Sexpr{bad_gower_power} (the default exponent in the \emph{lime} R package). This causes the observations that are further away from the prediction of interest to be given larger weights than before. In \autoref{fig:concept-plot-good}, the explainer model captures the relationship between the black-box predictions in an immediate neighborhood of the prediction of interest. This cannot be said of the explainer model in \autoref{fig:concept-plot-bad}. In addition, the magnitude of the slope associated with Feature 2  (\Sexpr{coefs_data %>% filter(case == "bad", feature == "Feature 2") %>% pull(slope) %>% round(3) %>% abs()}) is larger than that of Feature 1 (\Sexpr{coefs_data %>% filter(case == "bad", feature == "Feature 1") %>% pull(slope) %>% round(3) %>% abs()}). Thus, this explainer model actually provides a misleading explanation that Feature 2 plays a more important role in the prediction of interest.

Several sources in the literature discuss the performance of LIME. One of the biggest difficulties with LIME is determining how to specify a local region \citep{laugel:2018} \citep{molnar:2019}. This is due to an unclear definition of a "local region" and how to apply LIME to achieve an appropriate local region as demonstrated by \autoref{fig:concept-plot-bad}. \citet{alvarezmelis:2018} raise a concern pertaining to the robustness of explanations from LIME and other model explainers: they find that even small changes in predictor variables can lead to very different LIME explanations. Additionally, \citet{ribeiro:2016} acknowledge that if a linear model is used as the explainer, LIME relies on a linear approximation of the explainer model to the complex model and state "if the underlying model is highly non-linear even in the locality of the prediction, there may not be a faithful explanation".

As a result of the various ways LIME can fail, it is important to assess LIME explanations. We suggest the use of visual diagnostics for assessment. In this paper, we lay out the set of claims about LIME made by \citet{ribeiro:2016} and propose three visualizations for the assessment of these claims: (1) \emph{explanation scatterplot}, (2) \emph{feature heatmap}, (3) \emph{assessment metric plot}. While LIME is implemented for image, tabular, and text data, we only focus on tabular data. For additional simplicity, we only discuss classification prediction models with a dichotomous response variable and continuous predictor variables. However, the proposed diagnostics may be extended to a wider range of situations.

The remainder of the paper is structured as follows. Section \ref{background} provides background and claims made by \citet{ribeiro:2016} about LIME. We introduce the suggested diagnostic plots in Section \ref{diagnostics}. Then in Section \ref{application}, we demonstrate the use of the diagnostics to assess LIME explanations for a random forest  fit to a forensics bullet matching dataset. Section \ref{discussion} concludes with a discussion on extensions and limitations of the diagnostic plots and concerns about LIME in regards to the claims made by \citet{ribeiro:2016} brought about by the visualization examples in this paper that agree with \citet{alvarezmelis:2018}, \citet{laugel:2018}, and \citet{molnar:2019}.

All runs of LIME in this paper are executed in a forked version\footnote{https://github.com/goodekat/lime} of the R package \emph{lime} (version \Sexpr{packageVersion("lime")}) by \kge{\citet{pedersen:2020}}. The forked version is functionally indistinguishable from \kge{\citeauthor{pedersen:2020}'s} implementation but allows us to export internal values relevant for an assessment of the explainer. The diagnostic plots included in this paper are created using the R package \emph{limeaid} \citep{goode:2020}.

\section{Background on LIME} \label{background}

\citet{ribeiro:2016} provide an implementation of LIME in a Python package \cite{limepy}. An adaption of the Python package in R has been implemented and made available by Thomas Lin-Pedersen \citep{pedersen:2020}. The discussion of parameter choices and implementation details of LIME in this paper are based on the R package.

The general form of the LIME algorithm can be divided into three steps \citep[see also][]{laugel:2018}:

\begin{enumerate}

\item \emph{Data Simulation and Interpretable Transformation}: Simulate a dataset from the original data used to fit the black-box model. Apply a transformation to the simulated data and the prediction of interest that will allow for interpretable explanations.

\item \emph{Explainer Model Fitting}: Apply the black-box model to the simulated data to obtain predictions. Compute the distance between each of the simulated observations and the prediction of interest. Perform feature selection. Fit an interpretable model with the black-box predictions from the simulated data as the response, the selected features from the transformed simulated data as the predictors, and the distances as weights. This model is the explainer model.

\item \emph{Explainer Model Interpretation}: Interpret the explainer model to determine which features played the most important role in the prediction of interest.

\end{enumerate}

During the application of LIME, the user is asked to select various tuning parameter options: the number of features to return in the explanation, the simulation method, the feature selection method, and how the weights are computed. An overview of the options available for the tuning parameters is included in Appendix~\ref{lime-details}.

In the original paper, \citet{ribeiro:2016} make the following set of claims regarding the performance of LIME:

\begin{itemize}
\item \emph{Interpretability}: The explainer model can be easily interpreted to provide meaningful explanations.
\item \emph{Faithfulness}: The explainer model sufficiently captures the relationship between the complex model predictions and the features in the local region around a prediction of interest to produce explanations that are faithful to the complex model.
\item \emph{Linearity}: By using a ridge regression model as the explainer model, it is assumed that there is a linear relationship between complex model predictions and the features in the local region around a prediction of interest.
\item \emph{Localness}: The explanations produced by LIME are local in regards to a prediction of interest.
\end{itemize}

The assumption of interpretability only depends on the complexity of the model used as explainer model. If the model is too complex to provide meaningful explanations (e.g. there are too many variables in the model), it is clear that the assumption of interpretability is violated. The other three assumptions are not as easy to assess, and for those, we suggest the use of diagnostic plots. 

\section{Visual Diagnostics for LIME} \label{diagnostics}

<<sine-data, echo = FALSE, include = FALSE>>=

# Functions for rotating the data
rot_x <- function(x, y, theta) (x * cos(theta)) - (y * sin(theta))
rot_y <- function(x, y, theta) (x * sin(theta)) + (y * cos(theta))

# Generate the data
theta = -0.9
min = -10
max = 10
set.seed(20190913)
sine_data <- data.frame(x1 = runif(600, min, max),
                        x2 = sort(runif(600, min, max))) %>%
  mutate(x1new = rot_x(x1, x2, theta),
         x2new = rot_y(x1, x2, theta),
         y = ifelse(x2new > 5 * sin(x1new), "blue", "red")) %>%
  slice(sample(1:n())) %>%
  mutate(x3 = rnorm(600),
         case = 1:600) %>%
  select(case, everything())

# Separate the data into training and testing parts
set.seed(20191003)
rs <- sample(1:600, 500, replace = FALSE)
sine_data_train <- sine_data[rs,]
sine_data_test <- sine_data[-rs,]

# Fit a random forest
set.seed(20191003)
rfsine <- randomForest(x = sine_data_train %>% select(x1, x2, x3),
                       y = factor(sine_data_train$y))

# Obtain predictions on the training and testing data
sine_data_train$rfpred <- predict(rfsine)
sine_data_train <- cbind(sine_data_train, predict(rfsine, type = "prob"))
sine_data_train <- sine_data_train %>% 
  rename(rfprob_blue = blue, rfprob_red = red)
sine_data_test$rfpred <- predict(rfsine, sine_data_test %>% select(x1, x2, x3))
sine_data_test <- cbind(sine_data_test,
                        predict(rfsine, sine_data_test
                                %>% select(x1, x2, x3),
                                type = "prob"))
sine_data_test <- sine_data_test %>% 
  rename(rfprob_blue = blue, rfprob_red = red)

# Extract the prediction of interest from the sine test data
sine_poi <- sine_data_test %>% filter(y != rfpred, x1 > 0, x1 < 5, x2 > 5)

@

\begin{figure*}[!thp]
\centering
<<sine-plot-data, out.width = '\\textwidth', fig.width = 8, fig.height = 3.1>>=

# Create points representing the rotated since curve
sinefun_data <- data.frame(xnew = seq(min(sine_data$x1new),
                                      max(sine_data$x1new),
                                      by = 0.01)) %>%
  mutate(ynew = 5 * sin(xnew)) %>%
  mutate(x = rot_x(xnew, ynew, -theta),
         y = rot_y(xnew, ynew, -theta)) %>%
  filter(y >= -10, y <= 10)

# Specify colors for predictions
sinecolor_red = "firebrick"
sinecolor_blue = "steelblue"

# Plot the training data observed classes
sine_plot_obs <- ggplot(sine_data_train, aes(x = x1, y = x2, color = y)) +
  geom_point(alpha = 0.8) +
  geom_path(data = sinefun_data, aes(x = x, y = y),
            color = "black") +
  scale_colour_manual(values = c(sinecolor_blue, sinecolor_red)) +
  theme_bw(base_family = "Times", base_size = 10) +
  labs(x = TeX("$x_1$"),
       y = TeX("$x_2$"),
       color = TeX("y"),
       title = "Training Data")

# Plot the testing data rf predictions
sine_plot_pred <- ggplot() +
  geom_point(data = sine_poi %>%
               mutate(shape = "Prediction \nof Interest"),
             mapping = aes(x = x1,
                           y = x2,
                           #fill = rfprob_blue,
                           shape = shape),
             size = 5,
             alpha = 0.8,
             color = "black") +
  geom_point(data = sine_data_test %>% filter(y != rfpred),
             mapping = aes(x = x1, y = x2),
             shape = 1,
             size = 3,
             color = "black") +
  geom_point(data = sine_data_test,
             mapping =  aes(x = x1,
                            y = x2,
                            color = rfprob_blue,
                            fill = rfprob_blue),
             shape = 21,
             alpha = 0.8) +
  geom_path(data = sinefun_data, aes(x = x, y = y),
            color = "black") +
  scale_color_gradient2(low = sinecolor_red,
                        mid = '#f7f7f7',
                        high = sinecolor_blue,
                        midpoint = 0.5) +
  scale_fill_gradient2(low = sinecolor_red,
                        mid = '#f7f7f7',
                        high = sinecolor_blue,
                        midpoint = 0.5) +
  scale_shape_manual(values = 23) +
  theme_bw(base_family = "Times", base_size = 10) +
  labs(x = TeX("$x_1$"),
       y = TeX("$x_2$"),
       color = "Random \nForest \nProbability \nfor 'blue'",
       fill = "Random \nForest \nProbability \nfor 'blue'",
       title = "Testing Data",
       shape = "") +
  guides(shape = guide_legend(order = 1))

# Join the plots
plot_grid(sine_plot_obs, sine_plot_pred,
          nrow = 1,
          rel_widths = c(0.4825, 0.5175))

@
\caption{Plots of $x_2$ versus $x_1$ from the training (left) and testing (right) sets of the \data \ introduced in Section \ref{diagnostics}. The true classification boundary is shown as the solid black line in both plots. The color of the training data  represents the value of the observed response variable ($y$). The color of the testing data  represents the random forest probability that an observation belongs to the category of blue. The \Sexpr{dim(sine_data_test %>% filter(y != rfpred))[1]} cases that are misclassified by the random forest are identified by black circles. The prediction of interest to explain is indicated by a diamond.}
\label{fig:sine-plot-data}
\end{figure*}

<<sine-lime>>=

# Apply LIME with various tuning parameters to the sine data
if (!file.exists("../data/sine_lime_explain.rds")) {

    # Apply lime with various input options
    sine_lime_explain <- apply_lime(
      train = sine_data_train %>% select(x1, x2, x3),
      test = sine_data_test %>% select(x1, x2, x3),
      model = rfsine,
      label = "blue",
      n_features = 2,
      sim_method = c('quantile_bins', "kernel_density"),
      nbins = 2:6,
      feature_select = "auto",
      dist_fun = "gower",
      kernel_width = NULL,
      gower_pow = 1,
      return_perms = TRUE,
      all_fs = TRUE,
      seed = 20190914)

    saveRDS(sine_lime_explain, "../data/sine_lime_explain.rds")

  } else {

    sine_lime_explain <- readRDS("../data/sine_lime_explain.rds")

  }

@

<<sine-lime-default>>=

# Extract the explanations from the default lime application
sine_lime_default <- sine_lime_explain$explain %>% filter(nbins == 4)

# Extract the explanations from the default lime application
# for the prediction of interest only
sine_poi_lime_default <- sine_lime_default %>%
  filter(case == sine_poi %>% pull(case)) %>%
  mutate(case = c("Prediction of Interest", "Prediction of Interest"))
@

In this section, we introduce three visual diagnostic plots for the assessment of LIME. The plots focus on different levels of application of LIME (e.g. on explanation versus a set of explanations) to assess the LIME claims from different perspectives:

\begin{enumerate}
\item \emph{Explanation Scatterplot} (Section \ref{exp-scatter}): Comparison of the explainer and complex models for an individual prediction of interest.
\item \emph{Feature Heatmap} (Section \ref{feat-heat}): Comparison of features selected by LIME across applications of LIME with different tuning parameters.
\item \emph{Assessment Metric Plot} (Section \ref{assess-metric}): Comparison of performance metrics for LIME across applications of LIME with different tuning parameters. 
\end{enumerate}

\paragraph{The \data}

To demonstrate the visual diagnostics, we generate an example dataset that will be referred to as the \data. The \data \ contains \Sexpr{dim(sine_data)[1]} observations with three features and one response variable. The features, $x_1$, $x_2$, and $x_3$, are randomly sampled from Unif(\Sexpr{min}, \Sexpr{max}), Unif(\Sexpr{min}, \Sexpr{max}), and $\mbox{N}(0,1)$ distributions, respectively. A binary response variable $y$ is  created using a rotated sine curve. In particular, let $x'_1=x_1\cos(\theta)-x_2\sin(\theta)$ and $x'_2=x_1\sin(\theta)+x_2\cos(\theta)$ where $\theta=-0.9$. Then $y$ is defined as
\begin{eqnarray}\label{eq:data}
  y=\begin{cases}
  \mbox{blue} & \mbox{ if } x'_2 > \sin\left(x'_1\right) \\
  \mbox{red} & \mbox{ if } x'_2 \le \sin\left(x'_1\right) \ . %\nolabel
  \end{cases}
\end{eqnarray}
Note that due to the creation of $y$ in this manner, $y$ is dependent on $x_1$ and $x_2$ and independent of $x_3$. The dataset is divided into training and testing sets of \Sexpr{dim(sine_data_train)[1]} and \Sexpr{dim(sine_data_test)[1]} observations, respectively.  A random forest  is fit using the R package \emph{randomForest} (version \Sexpr{packageVersion("randomForest")}) \citep{liaw:2002} with the default settings. The model is applied to the test set to obtain predictions. 

\autoref{fig:sine-plot-data} shows scatterplots of $x_2$ versus $x_1$ from the training data (left) and the testing data (right). Both plots include the true classification boundary of the rotated sine function plotted as the solid black line. The training data are colored by the observed response variable ($y$), and the testing data are colored by random forest  prediction probabilities. The random forest  misclassifies \Sexpr{dim(sine_data_test %>% filter(y != rfpred))[1]} points, which are all located near  the classification boundary. These are identified by open circles in \autoref{fig:sine-plot-data}. 

From these scatterplots, we also see that the global relationship between response $y$ and features $x_1$ and $x_2$ is linear: the probability for label blue increases with the difference between features $x_2$ and $x_1$. Locally, the relationship between $y$ and features $x_2$ and $x_1$ varies a lot more around the line of identity. Here, the relationship is determined by the sine waves. However, the sine is a good-natured function that can be approximated well linearly in local regions.

We apply LIME using six sets of tuning parameters to all observations in the \data \ test set to observe variability across tuning parameters. A quantile bin based simulation method is used for five of the LIME applications with the number of bins varying from 2 to 6 by application. We use 6 bins as the maximum, because the complexity of the explanations increases with the number of bins. The sixth application of LIME uses a kernel density simulation method. The default methods for feature selection (forward selection) and the computation of the weights (Gower distance raised to an exponent of 1) are used for all applications. (See Appendix~\ref{lime-details} for descriptions of the tuning parameters.)

For the presentation of the explanation scatterplot, we focus on the misclassified point with $(x_1, x_2, x_3)$ coordinates of (\Sexpr{round(sine_poi$x1, 2)}, \Sexpr{round(sine_poi$x2, 2)}, \Sexpr{round(sine_poi$x3, 2)}) indicated by a diamond in \autoref{fig:sine-plot-data}. Misclassified points are often of interest to explain since they may provide information about ways to improve the model. For the introduction of the other two  plots, we consider the LIME explanations for  all observations in the \data \ test data.

<<warning = FALSE>>=
# Obtain the simulated data associated with the poi
sine_poi_perms <- sine_poi_lime_default %>%
  slice(1) %>%
  select(perms_raw, perms_pred_complex, perms_numerified, weights) %>%
  mutate(perms_numerified =
           map(perms_numerified,
               .f = function(x) rename(x,
                                       "x1num" = "x1",
                                       "x2num" = "x2",
                                       "x3num" = "x3"))) %>%
  unnest(cols = c(perms_raw, perms_pred_complex, perms_numerified, weights)) %>%
  select(-red) %>%
  rename(rfpred = blue) %>%
  mutate(case = factor(1:n())) %>%
  select(case, everything())

# Determine the bin cuts for the default lime application
sine_lime_default_bin_cuts <- sine_lime_default %>%
  select(feature, feature_desc) %>%
  unique() %>%
  separate(feature_desc, c("other", "bin_cut"), sep = " <= ") %>%
  select(-other) %>%
  na.omit() %>%
  mutate(bin_cut = as.numeric(bin_cut)) %>%
  arrange(feature) %>%
  mutate(case = rep(1:3, 2)) %>%
  spread(feature, bin_cut)
@

<<sine-poi-explainer, warning = FALSE>>=

# Determine the lime explanation cutoffs
sine_poi_bounds <- sine_poi_lime_default %>%
  select(case, feature, feature_value, feature_desc) %>%
  separate(feature_desc, c("other", "upper"), sep = " <= ") %>%
  separate(other, c("lower", "feature2"), sep = " < ") %>%
  mutate(upper = ifelse(is.na(upper), "Inf", upper)) %>%
  select(-feature2) %>%
  mutate_at(.vars = c("lower", "upper"), .funs = as.numeric)

# Extract the coefficients from the explainer
sine_b0 <- sine_poi_lime_default$model_intercept[1]
sine_b1 <- sine_poi_lime_default$feature_weight[2]
sine_b2 <- sine_poi_lime_default$feature_weight[1]

# Extract the bounds of the bins
x1_lower <- sine_poi_bounds %>% filter(feature == "x1") %>% pull(lower)
x1_upper <- sine_poi_bounds %>% filter(feature == "x1") %>% pull(upper)
x2_lower <- sine_poi_bounds %>% filter(feature == "x2") %>% pull(lower)
x2_upper <- sine_poi_bounds %>% filter(feature == "x2") %>% pull(upper)

# Function for computing the predicted value via the explainer model
sine_explainer <- function(z1, z2) sine_b0 + sine_b1 * z1 + sine_b2 * z2

@

<<sine-perms-subset>>=
# Subset the simulated data to the region where the poi is located
sine_poi_region_perms <- sine_poi_perms %>%
  filter(x2 >= sine_poi_bounds %>% filter(feature == "x2") %>% pull(lower),
         x1 >= sine_poi_bounds %>% filter(feature == "x1") %>% pull(lower),
         x1 < sine_poi_bounds %>% filter(feature == "x1") %>% pull(upper))
@

\begin{figure*}[!thp]
<<sine-exp-scatter, out.width = '\\textwidth', fig.width = 12, fig.height = 4.68, warning = FALSE>>=

sine_poi_exp <- sine_poi_lime_default %>% plot_features() + theme(text = element_text(family = "Times", size = 10))

exp_scatter <- 
  plot_explain_scatter(sine_poi_lime_default, alpha = 0.4) +
  facet_grid(switch = "both") +
  labs(x = TeX("x_1"),
       y = TeX("x_2"),
       size = "Weight",
       shape = "",
       fill = "Random \nForest \nProbability \nfor 'blue'",
       color = "Random \nForest \nProbability \nfor 'blue'",
       linetype = "",
       title = "Explanation Scatterplot") +
  theme_bw(base_family = "Times", base_size = 10) +
  guides(shape = guide_legend(order = 1),
         size = guide_legend(nrow = 2, byrow = T)) +
  theme(aspect.ratio = 1) 

plot_grid(sine_poi_exp, exp_scatter) 
@
\caption{(left) Visualization from the \emph{lime} R package of the LIME explanation for the \data \ prediction of interest. The bars represent the explainer model coefficients. (right) \emph{Explanation Scatterplot}. The points represent the simulated data colored by the random forest probabilities and sized by the assigned LIME weights. \hh{XXX could you make the complex prediction **much** lighter for this example? The diamond and the lines are hard to see, but should be the focus.} \kgc{Is that light enough? Feel free to edit the alpha value in the plot\_explain\_scatter function as you feel necessary. I also made the lines depicting the explainer model wider in limeaid, and there is an option 'line\_size' that can be adjusted.} The prediction of interest is shown as the diamond shaped point. The explainer model indicator variables associated with $x_1$ and $x_2$ (quantile bins containing the prediction of interest) are depicted by the solid lines. The line colors represent the explainer model coefficient signs and indicate whether the feature supports a prediction of `blue' (blue lines) or supports a prediction of `red' (red lines). This figure shows that the quantile bins are not flexible enough to capture the relationship between the random forest predictions and the $x_1$ and $x_2$ values.}
\label{fig:sine-exp-scatter}
\end{figure*}

%\subsection{A LIME explanation}

\paragraph{An Example LIME Explanation}

\kgc{I really like the idea of separating the description of the \emph{lime} figure from the explanation scatterplot, but the more I think about it, I don't like that it get it's own sub-section. I think it would be nicer to allocate the sub-sections to the visual diagnostic plots. Instead, I switched to using a heading like you created for introducing the sine data. I also changed the wording of the heading a bit. What do you think? Another option would be to drop the heading altogether and just say, "Here is an example of a LIME explanation...but it is not enough..."}

\hh{XXX I've just pulled the description of the left hand side of figure 4 out of the next section.} To demonstrate a LIME explanation, we focus on the misclassified point indicated in Figure \ref{fig:sine-plot-data} by the diamond as the prediction of interest. The left plot in Figure \ref{fig:sine-exp-scatter} shows a visualization of the explanation for the prediction of interest as suggested by Ribeiro et al and implemented in the \emph{lime} R package. The figure shows that the random forest returned a probability of \Sexpr{round(sine_poi_lime_default$label_prob[1], 2)} that the observation of interest belongs to category blue {(denoted as the ``label" in the figure).The lengths of the bars represent the explainer model coefficients associated with the indicator variables chosen by LIME as important. The color of the bar denotes the sign of the coefficient and whether the feature supports or contradicts a random forest classification of `blue'. The "explanation fit" value is the deviance ratio from the R package \emph{glmnet} \citep{simon:2011} for a ridge regression model. In other words, this is the $R^2$ value associated with the explainer model. In this case, it is \Sexpr{round(sine_poi_lime_default$model_r2[1], 2)} suggesting that the explainer model is not a good linear fit. However, it is commonly accepted that $R^2$ has limitations for assessing the quality of fit of a model \citep{sapra:2014}, and it should not be used as the only metric in a model assessment.

The explanation for the prediction of interest depicted in Figure \ref{fig:sine-exp-scatter} can be interpreted as follows. A random forest classification of blue is supported by the prediction of interest having a value of $x_2$ that is greater than \Sexpr{str_split(sine_poi_lime_default$feature_desc[[1]], pattern = " ")[[1]][1]}, but the prediction of interest having a value of $x_1$ that is greater than \Sexpr{str_split(sine_poi_lime_default$feature_desc[[2]], pattern = " ")[[1]][1]} and less than or equal to \Sexpr{str_split(sine_poi_lime_default$feature_desc[[2]], pattern = " ")[[1]][5]} provides support against a classification of blue. \hh{Because the support for blue by the value of feature $x_1$ outweighs the support for red by the value of feature $x_2$, the overall LIME explanation favors a label of blue for the prediction of interest.} %Since the weight associated with $x_2$ has a larger magnitude than the weight associated with $x_1$, LIME explains that the $x_1$ and $x_2$ locations of the prediction of interest provide more support for a random forest classification of blue over red.
This explanation agrees with the random forest probability of \Sexpr{sine_poi_lime_default$label_prob[1]} for blue. Note that both models classify the observation incorrectly. Based on the plot \hh{on the left of \autoref{fig:sine-exp-scatter}} alone, it is not possible to make an informed assessment of the explanation. 


\subsection{Explanation Scatterplots} \label{exp-scatter}
\hh{XXX I think we could reduce some of the example for the specific example a bit, now that the LIME explanation is moved out, there is not that much need to repeat the general ideas.} \kgc{I commented out the text that I think you were referring to. See what you think.}

\hh{For a further assessment of the explainer model}, we turn to an \emph{explanation scatterplot}: the \emph{explanation scatterplot} is a visual diagnostic for assessing the LIME claims of locality and fidelity for an individual explanation by juxtaposing the complex and explainer models in one plot. The format of an explanation scatterplot depends on the LIME simulation method. We introduce the explanation scatterplot here under the \emph{lime} R package default method in which the data are simulated uniformly from four quantile bins. In this scenario, LIME converts continuous predictor variables to indicator variables identifying whether the variable value falls in the same quantile bin as the prediction of interest or not. The indicator variables are used as the explainer model features. 

The explanation scatterplot is built by plotting the LIME simulated data for the top two features identified by the explanation in a scatterplot and coloring these points by the  predictions from the complex model. The size of the points represents the weight assigned by LIME.
\hh{In order to show the LIME results for the observation of interest, }
 lines \hh{are drawn on top of the points. These lines } represent the boundaries of the indicator variables used to fit the explainer model. The color of the lines denote whether LIME indicates that a feature supports or contradicts a class prediction. Appendix \ref{exp-scatter_plus} addresses the explanation scatterplot formats in other LIME simulation scenarios.

\hh{An explanation scatterplot} \sout{for the} \kge{associated with the LIME explanation depicted on the left in \autoref{fig:sine-exp-scatter}} \sout{example of the previous section} \hh{is shown on the right hand side of \autoref{fig:sine-exp-scatter}.} %The points are the simulated values of $x_2$ versus $x_1$ colored by the random forest predictions and sized by the proximity weight used to fit the ridge regression explainer model. The explainer model is represented by the solid horizontal and vertical lines identifying the boundaries of the quantiles that contain the prediction of interest. The color of the lines indicate whether the explainer model coefficient is positive (blue) or negative (red). Thus, the $x_2$ coefficient is positive, so the $x_2$ value of the prediction of interest supports a random forest prediction of `blue', and the $x_1$ coefficient is negative, so the $x_1$ value of the prediction of interest contradicts a random forest prediction of `blue'. The prediction of interest is represented by the diamond shaped point.
By juxtaposing the random forest predictions and the explainer model boundaries, we are able to assess the faithfulness and localness of the explainer model. First consider the claim of localness. The weights remain relatively high outside of the intersection of the two quantile bins suggesting that the LIME explanation is highly influenced by points outside of the bin containing the prediction of interest. While it is still difficult to say whether or not the claim of localness has been violated, it is possible to say that the weights assigned to the simulated data do not agree with the local region assigned by the intersection of the quantile bins. 

Now, consider the claim of faithfulness. Within the intersection of the $x_1$ and $x_2$ quantile bins, there are more random forest probabilities above 0.5. This indicates why the magnitude of the coefficient associated with $x_2$ is larger than that of $x_1$. However, the pattern of the random forest predictions in this plot suggest that better explanations for this prediction exist. One example of a better explanation is to say that the prediction of interest received a random forest prediction greater than 0.5 since the observation of interest falls in the region where $x_1$ is less than 2.5 and $x_2$ is greater 4.5. A more localized explanation is to say that the region around the prediction of interest shows that observations with $-0.3 < x_1 <= 2.5$ and $4.8 < x_2 <= 8.75$ are all assigned probabilities greater than 0.5. The procedure implemented by LIME is not flexible enough to capture either of these explanations.

The explanation scatterplot shows issues with a definition of locality and an oversimplification of the random forest \kge{\sout{model}}. LIME does provide an explanation for the prediction of interest, which makes sense based on the quantile bins used. However, the explanation scatterplot identifies that better explanations for the prediction of interest exist. It is difficult to assess linearity from an explanation scatterplot, but a residual plot of the explainer model could be used to check the linearity claim. See Appendix \ref{residual-plot} for the residual plot associated with the explanation considered in Figure \ref{fig:sine-exp-scatter} and shows a violation of the linearity claim.

The \data \ explanation only includes two features. In situations where more than two features are included in a LIME explanation, the explanation scatterplots can be extended to a generalized pairs plot \citep{emerson:2013} that includes all pairwise combinations of features. Generalized pairs plots (and scatterplot matrices in general) have diminishing value as the number of features increase \citep{jensen:2011} \citep{sweller:2011}. Machine learning models are commonly fit using a large number of features, and therefore, a generalized pairs plot of explanation scatterplots for all features would be ineffective. However, when applying LIME, the user selects the number of features to return in the explanation. In the \emph{lime} R package, \citet{pedersen:2020} encourage users to select less than 10 features to include in the explanation. As a result, specifying a small number of features to include in a LIME explanation makes it feasible to use generalized pairs plot of explanation scatterplots.

\subsection{Feature Heatmap} \label{feat-heat}

\begin{figure}[!thp]
<<concept-feat-heat, out.width = '0.48\\textwidth', fig.width = 6, fig.height = 3>>=
# Specify the number of cases and LIME input options
ncases = 10
ninputs = 5

# Create a good case of chosen feature example data
set.seed(20191008)
heatmap_data_good <- tibble(case = factor(rep(1:ncases, each = ninputs),
                                     levels = ncases:1),
                       input = factor(rep(1:ninputs, ncases)),
                       feature = factor(c(rep(1, 5),
                                          rep(2, 5),
                                          rep(3, 5),
                                          rep(1, 5),
                                          rep(1, 5),
                                          rep(4, 5),
                                          rep(1, 5),
                                          rep(3, 5),
                                          rep(1, 5),
                                          rep(4, 5)))) %>%
  mutate(input = forcats::fct_recode(input, "A" = "1", "B" = "2",
                                     "C" = "3", "D" = "4", "E" = "5"))

# Create the conceptual good heatmap
heatmap_plot_good <- ggplot(data = heatmap_data_good,
                            mapping = aes(x = input,
                                          y = case,
                                          fill = feature,
                                          color = feature)) +
  geom_tile() +
  labs(x = "Tuning Parameter",
       y = "Case",
       fill = "Feature",
       color = "Feature",
       title = "Situation 1: \nConsistent Explanations") +
  theme_bw(base_family = "Times", base_size = 12) +
  scale_fill_grey() +
  scale_color_grey() +
  theme(legend.position = "none")

# Create a bad case of chosen feature example data
set.seed(20190627)
heatmap_data_bad <- tibble(case = factor(rep(1:ncases, ninputs),
                                     levels = ncases:1),
                       input = factor(rep(1:ninputs, each = ncases)),
                       feature = factor(rep(c(1, 2, 4, 3, 2), each = 10))) %>%
  mutate(input = forcats::fct_recode(input, "A" = "1", "B" = "2",
                                     "C" = "3", "D" = "4", "E" = "5"))

# Create the conceptual bad heatmap
heatmap_plot_bad <- ggplot(data = heatmap_data_bad,
                            mapping = aes(x = input,
                                          y = case,
                                          fill = feature,
                                          color = feature)) +
  geom_tile() +
  labs(x = "Tuning Parameter",
       y = "Case",
       fill = "Feature",
       color = "Feature",
       title = "Situation 2: \nInconsistent Explanations") +
  theme_bw(base_family = "Times", base_size = 12) +
  scale_fill_grey() +
  scale_color_grey()

heatmap_leg <- get_legend(heatmap_plot_bad)
heatmap_plot_bad <- heatmap_plot_bad + theme(legend.position = 'none')

# Join the plots
plot_grid(heatmap_plot_good, heatmap_plot_bad, heatmap_leg,
          nrow = 1,
          rel_widths = c(0.45, 0.45, 0.1))

@
\caption{Hypothetical examples of feature heatmaps in two possible situations. The heatmaps show the top feature chosen for 10 cases across 5 different sets of LIME tuning parameters. The color of the cell indicates the feature chosen by LIME.}
\label{fig:concept-feat-heat}

\vspace*{\floatsep}

<<sine-feat-heat, out.width = '0.45\\textwidth', fig.width = 4, fig.height = 4>>=
plot_feature_heatmap(sine_lime_explain$explain,
                order_method = "PCA") +
  theme_bw(base_family = "Times", base_size = 10) +
  theme(legend.position = "bottom",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        strip.background = element_rect(color = "white", fill = "white")) +
  labs(y = "Case")
@
\caption{\emph{Feature Heatmap}. An example feature heatmap of the explanations from the applications of LIME with different tuning parameters to the \data. The cases from the test set are plotted on the y-axis, and the tuning parameter values (simulation methods here) are included on the x-axis. The colors of the tiles indicate the feature selected by LIME for the corresponding case and tuning parameter value. The plot is faceted by the first features selected by LIME (top facet) and the second most important features (bottom facet). The column facets separate the kernel density method from the quantile bin methods. The vertical striping indicates that the LIME explanations are not consistent across tuning parameters.}
\label{fig:sine-feat-heat}
\end{figure}

Explanations produced by LIME are likely to be affected by the choice of tuning parameters. An example of this is shown by Figures \ref{fig:concept-plot-good} and \ref{fig:concept-plot-bad} where the method used to weight the observations influenced the explanation. As of the time of writing this manuscript, we have encountered no recommendations for how to determine which method to use besides for the default settings in \emph{lime} \kgc{(confirm this)}. In order to compare the explanations produced by LIME using different tuning parameters, we visualize an overview the explanations in the \emph{feature heatmap} diagnostic plot. 

The feature heatmap uses colors to identify the features selected by LIME across tuning parameters for sets of explanations (referred to as cases here) faceted by the position of feature importance assigned by LIME. That is, for LIME applied with $t$ sets of tuning parameters to $n$ cases to select the $f$ top features, create $f$ heatmaps (one for each of the positions of importance determined by the magnitude of the explainer model coefficients) with the cases on the y-axis, the tuning parameters on the x-axis, and the cells colored by the feature chosen for the corresponding case and tuning parameter. Additional tuning parameters may also be included in the plot via facets. This plot is used assess the locality of the explanations and to compare results from different input options.

Two hypothetical examples of feature heatmaps are included in \autoref{fig:concept-feat-heat}. The plots were created with the assumption that LIME is applied to select the top feature out of $p=8$ features for $n=10$ cases with $t=5$ sets of tuning parameters. Situation 1 shows an example where the features selected are consistent across tuning parameters within a case but vary across cases within a tuning parameter. This is the ideal situation, because the LIME explanations do not depend on the tuning parameters but do depend on the location of the observation in the feature space. Situation 2 shows an example where the features selected vary across tuning parameters within a case but are consistent across cases within a tuning parameter. This situation indicates that the features selected by LIME are dependent on the tuning parameters, and the explanations are not local because the same feature is chosen regardless of the case. In practice, it is expected that the plot will exhibit a combination of these two situations.

\autoref{fig:sine-feat-heat} shows the feature heatmap for the LIME applications to the 100 observations in the \data \ test set. The most important (top facet) and second most important (bottom facet) features are shown in this plot. For the quantile bins, the original features prior to the indicator variable transformation are included since it is obvious that different features would be selected when the sizes of the bins change. This figure shows that for kernel density and 2 quantile bins, LIME selects $x_1$ as the most important feature and $x_2$ as the second most important feature across all cases in the test set. These explanations are not local. There is variability in the features selected by LIME for 3 to 6 quantile bins. For these tuning parameters, there is a mix of horizontal and vertical stripes, which suggests a dependence on tuning parameters. Note that the explanations from 3 and 5 quantile bins include the selection of the random noise variable ($x_3$) as an important variable in many predictions, which should not be the case. The pattern seen in the explanations for 3 to 6 quantile bins may suggest local explanations, but the dependence on tuning parameter makes it unclear which set of explanations to use.

\subsection{Assessment Metric Plot} \label{assess-metric}

\begin{figure}[!thp]
<<sine-assess-metric, out.width = '0.49\\textwidth', fig.width = 4.5, fig.height = 3.75, warning = FALSE>>=

# Create the metric plot
plot_metrics(sine_lime_explain$explain, rank_legend = "discrete") +
  theme_bw(base_family = "Times") + 
  theme(strip.background = element_rect(color = "white", fill = "white"))

@
\caption{\emph{Assessment metric plot}. The assessment metrics computed on the applications of LIME to the \data \ test set are included in this example assessment metric plot. Each horizontal facet corresponds to one of three metrics: average $R^2$, average fidelity, or MSEE. The LIME tuning parameter (simulation methods in this example) is plotted on the x-axis, and the metric values are shown on the y-axis. The points are colored by rank (within a metric) indicating best to worst (dark to light). The kernel density simulation methods performs well according to all three metrics.}
\label{fig:sine-assess-metric}
\end{figure}

The feature heatmap for the \data \ in the previous section shows an example of inconsistent LIME explanations across tuning parameters. In this situation, the user must determine which set of explanations to trust. One way to do this is to compute assessment metrics for each set of explanations to identify the optimal tuning parameters. We discuss three metrics for this purpose and present a visual comparison in an \emph{assessment metric plot}.

Each metric presented below is computed on a set of LIME explanations obtained using the the same tuning parameters. Here we provide a high level description of the metrics. Notation and formulas for these metrics are included in Appendix \ref{metric-details}.

\begin{itemize}
\item \emph{Average $R^2$}: Assess the model fit and linearity claim by computing the average of the explainer model $R^2$ values (deviance ratios from the R package \emph{glmnet}).

\item \emph{Average Fidelity}: Measures the faithfulness of the explainer model to the complex model by comparing their predictions. Computed as the average of the explainer model fidelity metrics: a metric presented in \citet{ribeiro:2016} (the weighted distance between explainer and complex model predictions for all observations in the LIME simulated data associated with an individual prediction of interest). 

\item \emph{Mean Squared Explanation Error (MSEE)}: Also measures the faithfulness of the explainer model to the complex model by comparing their predictions, but only the prediction of interest is used to compute an average squared deviation between explainer and complex model predictions.
\end{itemize}

\autoref{fig:sine-assess-metric} shows these metrics computed for each of the LIME applications to the \data \ test set. The simulation methods are listed on the x-axis. The plot is faceted by metric, and the metric values are plotted on the y-axis. The colors of the points represent the rank of the simulation methods performance based on a particular metric (darker indicates a better metric value and lighter indicates a worse metric value). Higher average $R^2$ values are better, and lower average fidelity and MSEE values are better. This example only includes one tuning parameter: the simulation method. If more than one tuning parameter is considered, the assessment metric plot is extended by adding additional facets or levels to the x-axis.

All three metrics suggest that the kernel density performed well, but the metrics disagree for the quantile bins methods. Average $R^2$ and average fidelity rank the performance of the number of quantile bins the same (2 quantile bins perform the best and 6 quantile bins perform the worst). In fact, these two metrics appear to have an inverse relationship in this example. MSEE provides different ranks of the simulation methods. MSEE suggests that 6 quantile bins perform the best, and 2 quantile bins perform the worst. This is the opposite of average $R^2$ and average fidelity. Since average fidelity and MSEE are similar metrics, it would be expected that they would agree, but that is so in this example. This may be due to the MSEE only taking into account the prediction of interest and not the full simulated dataset. The contradiction between metrics makes it difficult to identify which simulation method explanations to trust. 

Recall that \autoref{fig:sine-feat-heat} indicated that the LIME kernel density method selected the same feature across all cases in the test set for both the first and second features. It appears that a global trend may be the best explanation for this example, which may be reasonable considering that we know that both $x_1$ and $x_2$ are the two features that should be the features used by the random forest to distinguish between response categories.

\section{Application to Bullet Matching Data} \label{application}

In this section, we provide a discussion of the application of the visual diagnostics for LIME explanations to a practical data problem investigating the similarity of marks on fired bullets.

\subsection{Bullet Matching Data}

<<bullet-data>>=

# Load the hamby data
bullet_train <- read.csv("../data/hamby173and252_train.csv")
bullet_test <- read.csv("../data/hamby224_test.csv")

# Extract the features and order them based on feature importance
bullet_features <- rownames(bulletxtrctr::rtrees$importance)
bullet_features_ordered <-
  data.frame(feature = rownames(bulletxtrctr::rtrees$importance),
             MeanDecreaseGini = bulletxtrctr::rtrees$importance) %>%
  arrange(desc(MeanDecreaseGini)) %>%
  mutate(feature = fct_recode(feature,
                              "CCF" = "ccf",
                              "CMS" = "cms",
                              "Matches" = "matches",
                              "Mismatches" = "mismatches",
                              "Non-CMS" = "non_cms",
                              "Rough Correlation" = "rough_cor",
                              "Distance Std Dev" = "sd_D",
                              "Distance" = "D",
                              "Sum Peaks" = "sum_peaks"))

# Create an ordered version of the data
bullet_features_ordered <- bullet_features_ordered %>%
  mutate(feature = factor(feature, levels = bullet_features_ordered$feature))

@

In current practice, forensic firearm examiners evaluate whether two bullets are from the same source (fired from the same gun) or from different sources based on microscopic comparison of the striation patterns engraved on bullets during the firing process (see \autoref{fig:bullet}). The process is based on a visual and therefore subjective assessment of the evidence. The lack of objective evaluation and the associated absence of established error rates has first been criticized by the National Research Council \cite{nrc:2009} and later by the President's Council of Advisors on Science and Technology \cite{pcast:2016}.

In response, \citet{hare:2016} proposed an automated machine learning method for bullet matching to complement a visual inspection by firearm examiners. Based on high-resolution topological scans of land engraved areas \citet{hare:2016} obtain signatures of striations from two bullet lands (\autoref{fig:signatures}). Nine features quantifying the similarity of signatures, such as the cross-correlation function, the distance between signatures, and the number of matching striae, are extracted and used to train a random forest \kge{\sout{model}} (available in the \emph{bulletxtrctr} R package) to determine the probability of a comparison resulting from the same source (matching signatures) or from different sources (non-matching signatures). The model was trained on a set of scans of bullets from the James Hamby Consecutively Rifled Ruger Barrel Study \citep{hamby:2009}, which includes \Sexpr{dim(bullet_train)[1]} land-to-land comparisons.

\begin{figure}[!t]
\centering
\includegraphics[width=2in]{./figure-static/bullets.png}
\caption{(Top left) Traditionally rifled gun barrel. The grooves and lands alternate to give bullets a spin during the firing process, which create markings (striations) on a bullet when fired. (Top right) Image of a fired bullet. The vertical stripes along the lower half of the bullet show groove and land engraved areas. The land engraved areas contain the microscopic striations created when the bullet passed through the barrel of the gun. (Bottom) Close up of a land engraved area showing striations (vertical lines).}
\label{fig:bullet}

\vspace*{\floatsep}

\includegraphics[width=3.25in]{./figure-static/signatures.png}
\label{fig:signatures}
\caption{Example bullet signatures. The bullet signatures are from the same barrel-land and therefore have very similar patterns. The \citet{hare:2016} random forest uses features that measure the similarity between two such signatures.}
\end{figure}

\subsection{Application of LIME to Bullet Matching Data}

\begin{figure*}[!thp]
<<bullet-plot-rfpcp, out.width = '\\textwidth', warning = FALSE>>=

# Create (or load) the parallel coordinate plot
if (file.exists("./figure-static/bullet_pcp.png")) {

  # Load and print the plot
  knitr::include_graphics("./figure-static/bullet_pcp.png")

} else {

  # Create the plot
  bullet_pcp <- bullet_train %>%
    select(all_of(bullet_features), samesource, rfscore) %>%
    bind_rows(bullet_test %>%
                select(all_of(bullet_features), samesource, rfscore),
              .id = "set") %>%
    mutate(set = fct_recode(set,
                            "Training Set" = "1",
                            "Testing Set" = "2"),
           samesource = fct_recode(factor(samesource),
                                  "Match" = "TRUE",
                                  "Non-Match" = "FALSE")) %>%
    rename("CCF" = "ccf", "CMS" = "cms", "Matches" = "matches",
           "Mismatches" = "mismatches", "Non-CMS" = "non_cms",
           "Rough Correlation" = "rough_cor", "Distance Std Dev" = "sd_D",
           "Distance" = "D", "Sum Peaks" = "sum_peaks") %>%
    na.omit() %>%
    ggplot(aes(color = rfscore)) +
    geom_pcp(aes(vars = vars(bullet_features_ordered$feature)),
             alpha = 0.2) +
    facet_grid(set ~ samesource) +
    scale_color_gradient2(low = "grey50",
                          high = "darkorange",
                          midpoint = 0.5) +
    theme_bw(base_family = "Times", base_size = 10) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          strip.placement = "outside",
          strip.background = element_rect(color = "white",
                                          fill = "white")) +
    labs(x = "Features Ordered by Random Forest Variable Importance \n(highest to lowest from left to right)",
         y = "Scaled Feature Values",
         color = "Random \nForest \nProbability")

  # Save the plot
  ggplot2::ggsave(plot = bullet_pcp,
                  filename = "./figure-static/bullet_pcp.png",
                  width = 7.5,
                  height = 3.5,
                  units = "in")
}

@
\caption{Parallel coordinate plots of the \citet{hare:2016} random forest predictions from the training (top facet) and testing (bottom facet) data. The observations are separated by known matching (right column) and non-matching (left column) signatures. The y-axis shows the standardized feature values, and the x-axis shows the features used to fit the random forest (ordered by random forest impurity based feature importance). Each line corresponds to an observation, and the line color represents the associated random forest probability. There are clear relationships between the feature values and the random forest probabilities.}
\label{fig:bullet-plot-rfpcp}
\end{figure*}

Since firearm identification is commonly used as evidence for convictions in court cases, it is important to be able to understand and assess a model used to quantify the probability that a bullet is fired from a gun. LIME explanations would provide a local explanation for an individual prediction, but just as it is important to assess the model for this high-stakes application, it is also important to assess the LIME explanations. We will demonstrate an assessment of LIME explanations using the visual diagnostics introduced in this paper.

We apply the random forest \kge{\sout{model}} from \citet{hare:2016} to another set of bullets from the Hamby study with \Sexpr{dim(bullet_test)[1]} rows of land comparisons. We first consider a global visualization of the relationship between the random forest predictions and the model features by creating a parallel coordinate plot of the training data (top facet) and testing data (bottom facets) predictions (\autoref{fig:bullet-plot-rfpcp}). The majority of same source observations with random forest probabilities close to 1 have a clear pattern of corresponding feature values. The known same source observations where the random forest is wrong (does not return probabilities close to 1) have feature values that reflect those of observations where same source is known to be false.

LIME is implemented using the R package \emph{limeaid} to apply LIME multiple times to all test set observations using different tuning parameters: 12 sampling methods (2-6 equally spaced bins, 2-6 quantile bins, kernel density estimation, and normal approximation) and 3 Gower exponents (0.5, 1, and 10). Thus, a total of $12\times 3=36$ different applications of LIME were performed. We specify that each LIME explanation return 3 features and feature selection is performed using the highest weights method (default option).

<<bullet-lime>>=

# Apply LIME to all but two cases without returning the permutations
if (file.exists("../data/hamby_lime.rds")){

  # Load the files
  #bullet_lime_noperms <- readRDS("../data/hamby_lime.rds")
  bullet_explain_noperms <- readRDS("../data/hamby_explain.rds")

} else {

  # Apply lime with various input options to the hamby data
  bullet_lime_explain_noperms <- apply_lime(
    train = bullet_train %>% select(all_of(bullet_features)),
    test = bullet_test %>%
      select(all_of(bullet_features)) %>%
      na.omit(),
    model = bulletxtrctr::rtrees,
    label = as.character(TRUE),
    n_features = 3,
    sim_method = c('quantile_bins', 'equal_bins',
                   'kernel_density', 'normal_approx'),
    nbins = 2:6,
    feature_select = "auto",
    dist_fun = "gower",
    kernel_width = NULL,
    gower_pow = c(0.5, 1, 10),
    return_perms = FALSE,
    all_fs = FALSE,
    seed = 20190914)

  # Separate the lime and explain parts of the results
  bullet_lime_noperms <- bullet_lime_explain_noperms$lime
  bullet_explain_noperms <- bullet_lime_explain_noperms$explain

  # Save the output objects
  saveRDS(object = bullet_lime_noperms,
          file = "../data/hamby_lime.rds")
  saveRDS(object = bullet_explain_noperms,
          file = "../data/hamby_explain.rds")

}

@

<<bullet-lime-perms>>=

# Specify two cases of interest
bullet_poi_match <- unique(bullet_explain_noperms$case)[c(325)]
bullet_poi_nonmatch <- unique(bullet_explain_noperms$case)[c(20)]

# Apply LIME to two cases with the permutations returned
bullet_lime_explain_perms <- apply_lime(
  train = bullet_train %>% select(all_of(bullet_features)),
  test = bullet_test %>%
    filter(case %in% bullet_poi_match) %>%
    bind_rows(bullet_test %>%
    filter(case %in% bullet_poi_nonmatch)) %>%
    select(all_of(bullet_features)),
  model = bulletxtrctr::rtrees,
  label = as.character(TRUE),
  n_features = 3,
  sim_method = c('quantile_bins', 'equal_bins', 'kernel_density', 'normal_approx'),
  nbins = 3,
  feature_select = "auto",
  dist_fun = "gower",
  kernel_width = NULL,
  gower_pow = 0.5,
  return_perms = TRUE,
  all_fs = FALSE,
  seed = 20190914)

# Separate the lime and explain parts of the results
bullet_lime_perms <- bullet_lime_explain_perms$lime
bullet_explain_perms <- bullet_lime_explain_perms$explain %>%
  mutate(case = ifelse(case == 1,
                       bullet_poi_match,
                       bullet_poi_nonmatch))

@

<<bullet-lime-combined>>=

# Determine the application and case number of the poi
# in the no_perms data
poi_cases_no_perms <- bullet_explain_noperms %>%
  filter(case %in% c(bullet_poi_match, bullet_poi_nonmatch),
         nbins %in% c(3, NA),
         gower_pow == 0.5) %>%
  select(case, implementation) %>%
  unique()

# Join the no perms data (with poi removed) with the
# perms data (perms removed)
bullet_explain <- bullet_explain_noperms %>%
  anti_join(poi_cases_no_perms,
            by = c("implementation", "case")) %>%
  bind_rows(
    bullet_explain_perms %>%
      mutate(
        implementation = fct_recode(
          implementation,
          "2" = "1",
          "7" = "2",
          "31" = "3",
          "32" = "4"
        )
      ) %>%
      select(
        -perms_raw,
        -perms_numerified,-perms_pred_simple,
        -perms_pred_complex,-weights
      )
  )
@

\subsection{LIME Assessment Visualizations} \label{bullet-assess-ex}

To get an overview of the LIME explanations from the 36 applications, we consider a feature heatmap (\autoref{fig:bullet-feature-heatmap}). In addition to facets for simulation method and order of feature selected by LIME, this plot includes a vertical facet for Gower power and a horizontal facet for whether the observation is a known match or non-match. This plot highlights several key features of the LIME explanations from the bullet matching dataset.

First, the density simulation methods produce the same explanations for almost all cases and LIME tuning parameters suggesting the LIME explanations are global and not local. Second, within a bin based simulation method, the features selected by LIME for an observation often vary by the number of bins but do not appear to vary by the Gower power. With the equal bins, there are vertical stripes that suggest a dependence of the LIME explanations on the number of bins. The vertical stripes are not as apparent with the quantile bins. Lastly, there are clear differences between the LIME explanations produced by the bin based simulation methods for the matches and non-matches. This suggests that the features the random forest uses to classify a match or non-match are different. 

\begin{figure*}[!thp]
<<bullet-feature-heatmap, out.width = '\\textwidth', fig.width = 18, fig.height = 12>>=
# Create a feature heatmap
plot_feature_heatmap(
  bullet_explain %>%
    mutate(
      label = as.factor(label),
      feature = fct_recode(
        feature,
        "Rough Correlation" = "rough_cor",
        "Consecutively Matching Striae" = "cms",
        "Distance" = "D",
        "Matches" = "matches",
        "Mismatches" = "mismatches",
        "Non-Consecutively Matching Striae" = "non_cms",
        "Cross Correlation Function" = "ccf",
        "Sum of Peaks" = "sum_peaks",
        "Distance Standard Deviation" = "sd_D"
      )
    ),
  facet_var = bullet_test %>%
    mutate(samesource = fct_recode(
      factor(samesource),
      "Match" = "TRUE",
      "Non-Match" = "FALSE"
    )) %>%
    pull(samesource) %>%
    na.omit(),
  order_method = "PCA"
) +
  scale_fill_gretchenalbrecht(palette = "last_rays", discrete = TRUE) +
  scale_color_gretchenalbrecht(palette = "last_rays",
                               discrete = TRUE,
                               reverse = TRUE) +
  theme_bw(base_family = "Times", base_size = 16) +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    strip.background = element_rect(color = "white", fill = "white"),
    strip.text.y.right = element_text(angle = 0),
    legend.position = "bottom",
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  guides(fill = guide_legend(nrow = 3)) +
  labs(y = "Case", color = "Complex Model Feature", fill = "Complex Model Feature")

@
\caption{Feature heatmap of the 36 LIME applications to the bullet comparison data test set. In addition to faceting the results by simulation method and LIME feature selection order, facets for the Gower power and whether the observation is a match or non-match are included. The vertical stripes of features selected indicate a dependence between the LIME explanations and tuning parameters.}
\label{fig:bullet-feature-heatmap}

\vspace*{\floatsep}

<<bullet-assess-metric, out.width = '\\textwidth', warning = FALSE, fig.width = 10, fig.height = 4>>=

# Create a metric comparison plot
plot_metrics(bullet_explain %>% 
               mutate(label = as.factor(label)), 
             add_lines = TRUE) +
  theme_bw(base_family = "Times") +
  theme(strip.background = element_rect(color = "white", fill = "white"))
@
\caption{Plot of LIME assessment metrics for the applications of LIME on the bullet comparison data test set. The density simulation methods perform well for all metrics. The bin based metrics often do not agree in terms of performance across metrics.}
\label{fig:bullet-assess-metric}
\end{figure*}

To try to identify a set of LIME tuning parameters with the most trustworthy set of explanations, an assessment metric plot is considered (\autoref{fig:bullet-assess-metric}). All density simulation methods performed well based on the three metrics. This is an interesting result, since \autoref{fig:bullet-feature-heatmap} shows that the density methods results in global explanations. The metric values for the bin based methods do not agree across metrics. For example, 2 and 3 quantile bins performed well according to the average $R^2$ but poorly according to MSEE. As a result, with the bin based methods, it is difficult to know which method to recommend. The applications using a power of 0.5 perform the best or as well as the other powers across all simulation methods suggesting that the power that leads to a more global explanation is preferred by LIME.

Perhaps we can say that the kernel density method, 3 equal bins, or 3 quantile bins are some of the better performing methods, but unfortunately, \autoref{fig:bullet-assess-metric} leaves us without a clear indication of which set of explanations to use to explain the random forest. To better understand the explanations provided by these three methods, we take a closer look at explanations from these methods for two observations of interest, which will be referred to as case M (a known match) and case NM (a known non-match).

Figure \ref{fig:bullet-eois} include the explanation plots (from \emph{lime}) and explanation scatterplots for cases M and NM. Plots for 3 equal bins and 3 quantile bins are included. Plots for the kernel density simulation method are included in Figure \ref{fig:bullet-expl-scat-density}. The scatterplots provide several insights into the LIME explanations. Here we focus on the explanation for case NM with 3 quantile bins, but similar statements can be made about the other plots.

The explanation plot from \emph{lime} for case NM with 3 quantile bins shows that an observed CCF value greater than 0.320 supports a random forest prediction in favor of a match, and the scatter plots show that many of the simulated values with CCF greater then 0.320 have random forest predictions greater than 0.5. Additionally, the LIME explanation indicates that a value of matches less than or equal to 1.66 contradicts a prediction in favor of a match, and the scatter plots show that almost all simulated values with a value of matches less than or equal to 1.66 have random forest predictions close to 0. While the LIME explanation makes sense based on the observations made from the explanation scatter plot, the plot also indicates that LIME falls short of providing a good explanation of the random forest prediction for case NM. Based on the scatter plot of CCF versus matches, a better explanation would be that because case NM has a value of CCF less than 0.8 and a value of matches less than 7, the random forest provides a prediction supporting a non-match. The relationship between CCF and rough correlation does not provide much evidence to support the random forest prediction one way or the other, but there is a pentagon shaped region at the bottom of the scatter plot of matches versus rough correlation with mostly predictions close to 0 that case NM falls and supports the random forest prediction of a non-match.

<<bullet-eoi-plots, message = FALSE>>=
bullet_explain_perms <- bullet_explain_perms %>%
  mutate(case = ifelse(
    case == bullet_poi_nonmatch,
    paste(case, "(non-match)"),
    paste(case, "(match)")
  )) 

eoi1_3qb <-
  plot_explain_scatter(bullet_explain_perms[1:3, ], alpha = 0.9, weights = TRUE) +
  scale_color_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  scale_fill_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  theme_bw(base_family = "Times", base_size = 10) +
  theme(
    strip.placement = "outside",
    strip.background = element_rect(color = "white",
                                    fill = "white")
  ) + 
  guides(linetype = guide_legend(override.aes = list(color = c("darkorange"))))

eoi2_3qb <-
  plot_explain_scatter(bullet_explain_perms[4:6, ], alpha = 0.9, weights = TRUE) +
  scale_color_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  scale_fill_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  theme_bw(base_family = "Times", base_size = 10) +
  theme(
    strip.placement = "outside",
    strip.background = element_rect(color = "white",
                                    fill = "white")
  ) + 
  guides(linetype = guide_legend(override.aes = list(color = c("grey50", "darkorange"))))

eoi1_3eb <-
  plot_explain_scatter(bullet_explain_perms[7:9, ], alpha = 0.9, weights = TRUE) +
  scale_color_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  scale_fill_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  theme_bw(base_family = "Times", base_size = 10) +
  theme(
    strip.placement = "outside",
    strip.background = element_rect(color = "white",
                                    fill = "white")
  ) + 
  guides(linetype = guide_legend(override.aes = list(color = c("grey50", "darkorange"))))

eoi2_3eb <-
  plot_explain_scatter(bullet_explain_perms[10:12, ], alpha = 0.9, weights = TRUE) +
  scale_color_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  scale_fill_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  theme_bw(base_family = "Times", base_size = 10) +
  theme(
    strip.placement = "outside",
    strip.background = element_rect(color = "white",
                                    fill = "white")
  ) + 
  guides(linetype = guide_legend(override.aes = list(color = c("grey50", "darkorange"))))

# eoi1_kd <-
#   plot_explain_scatter(bullet_explain_perms[13:15, ], alpha = 0.9, weights = TRUE) +
#   scale_color_gradient2(
#     low = "grey50",
#     high = "darkorange",
#     midpoint = 0.5,
#     limits = c(0, 1)
#   ) +
#   scale_fill_gradient2(
#     low = "grey50",
#     high = "darkorange",
#     midpoint = 0.5,
#     limits = c(0, 1)
#   ) +
#   theme_bw(base_family = "Times", base_size = 8) +
#   theme(
#     strip.placement = "outside",
#     strip.background = element_rect(color = "white",
#                                     fill = "white")
#   )
# 
# eoi2_kd <-
#   plot_explain_scatter(bullet_explain_perms[16:18, ], alpha = 0.9, weights = TRUE) +
#   scale_color_gradient2(
#     low = "grey50",
#     high = "darkorange",
#     midpoint = 0.5,
#     limits = c(0, 1)
#   ) +
#   scale_fill_gradient2(
#     low = "grey45",
#     high = "darkorange",
#     midpoint = 0.5,
#     limits = c(0, 1)
#   ) +
#   theme_bw(base_family = "Times", base_size = 8) +
#   theme(
#     strip.placement = "outside",
#     strip.background = element_rect(color = "white",
#                                     fill = "white")
#   )

pf1_3qb <- plot_features(bullet_explain_perms[1:3,]) +
  scale_fill_manual(values = c("darkorange", "grey50")) +
  theme(text = element_text(family = "Times", size = 10))
pf2_3qb <- plot_features(bullet_explain_perms[4:6,]) +
  scale_fill_manual(values = c("darkorange", "grey50")) + 
  theme(text = element_text(family = "Times", size = 10))
pf1_3eb <- plot_features(bullet_explain_perms[7:9,]) +
  scale_fill_manual(values = c("darkorange", "grey50")) +
  theme(text = element_text(family = "Times", size = 10))
pf2_3eb <- plot_features(bullet_explain_perms[10:12,]) +
  scale_fill_manual(values = c("darkorange", "grey50")) +
  theme(text = element_text(family = "Times", size = 10))
# pf1_kd <- plot_features(bullet_explain_perms[13:15,]) +
#   scale_fill_manual(values = c("darkorange", "grey50"))
# pf2_kd <- plot_features(bullet_explain_perms[16:18,]) +
#   scale_fill_manual(values = c("darkorange", "grey50"))
@

\begin{figure*}[!thp]
<<bullet-eois, out.width = '\\textwidth', fig.width = 12, fig.height = 17, fig.align = "center", out.width="90%", message = FALSE>>=
# Join the plots
plot_grid(
  pf2_3qb,
  pf2_3eb,
  eoi2_3qb,
  eoi2_3eb,
  pf1_3qb,
  pf1_3eb,
  eoi1_3qb,
  eoi1_3eb,
  nrow = 4,
  rel_heights = c(0.2, 0.3, 0.2, 0.3)
)
@
\caption{Plots of LIME explanations and explanation scatterplots for one case M and NM in the bullet test data. The first column contains plots associated with 3 quantile bins and the second with 3 equally spaced bins. The plots provide insights into the LIME explanations and allow for the assessment of the explanation quality.}
\label{fig:bullet-eois}
\end{figure*}

Without applying LIME with multiple tuning parameters to the bullet test data or viewing diagnostic plots of the LIME explanations, it may be very possible to formulate reasons why the LIME explanations make sense. However, the sequence of plots in this section (Figures \ref{fig:bullet-feature-heatmap}, \ref{fig:bullet-assess-metric}, and \ref{fig:bullet-eois}) suggest that we should be cautious to trust any of these LIME explanations. It appears that either LIME needs to be further tuned to provide trustworthy and good explanations, or a different approach may provide better insight. 

\section{Discussion} \label{discussion}

%\hh{The discussion should not have subsection ... but we can keep them for now for structural purposes.} \kg{Okay. I took them out.}

This paper highlights that while an explainer model is meant to provide clarity, it actually adds another layer of complexity to predictive models by requiring yet another model that needs to be assessed. Without an assessment of the explainer model, LIME is a black-box procedure of its own requiring blind trust in the explainer model. %\hh{XXX what is the implication? bring it to a punch: asking the user to trust into explainer...}
We suggest the use of visual diagnostics to counteract the black-box nature of LIME and provide three diagnostic plots.

The visualizations are intended to provide insight on how LIME works, assess the ability of the explainer model to capture the complex predictive model, and compare LIME explanations produced by different tuning parameters. While the visualizations accomplish these tasks, they also expose examples of the failings of LIME. To address the discovered failings of LIME, we will reconsider each of the claims about the performance of LIME made by \citet{ribeiro:2016} in light of the insights gained from the diagnostic visualizations.

As previously discussed, the \textbf{interpretability} of the LIME explanations can be controlled by the complexity of the explainer model. For example, the number of bins selected for simulation can control the interpretability of the explanations. If too many bins are selected, the bin range that is reported in the LIME explanation will be too small to be meaningful in the context of the feature. An appropriate choice of the number of bins will keep the bin range meaningful. Thus, the claim of interpretability does not need to be assessed using the visualizations. However, diagnostic visualizations do present a different perspective on the meaning of interpretability.

Even though an explanation will be interpretable as long as the complexity of the explainer model is appropriately chosen, a lack of understanding of the process used to create the explanation could lead to an incorrect interpretation of the explanation. For example, the visualization of a LIME explanation available from the \emph{lime} R package \citep{pedersen:2020} (shown in Figures \ref{fig:sine-exp-scatter} and \ref{fig:bullet-eois}) is a major simplification of the explainer model, which could lead to under-interpreted or misinterpreted LIME explanation. Supplementing \citet{pedersen:2020}'s compact visualization of the explanation with an explanation scatterplot that shows a more detailed visualization of the explainer model (such as Figures \ref{fig:sine-exp-scatter} and \ref{fig:bullet-eois}) promotes a full interpretation of the explanation.

Even with an explainer model that is interpreted correctly, the interpretation is worthless if the explainer model is not \textbf{faithful} to the complex model. This claim can be assessed using the diagnostic plots suggested in this paper. Many of the visualizations in this paper highlight problems with the faithfulness of the explainer models. The explanation scatterplots allow for a comparison of the explainer model to the complex model. The examples in this paper show cases where the explainer model bins do not accurately capture the classification boundaries near the predictions of interest of the random forest\kge{s \sout{models}} by oversimplifying the model (Figures \ref{fig:sine-exp-scatter} and \ref{fig:bullet-eois}). Using less bins would clearly not help improve the faithfulness of the explainer model in these examples, and while an increase in bins would lead to a finer resolution of the random forest classification boundaries, interpretability of the explainer model would quickly be lost. Perhaps this could be improved by allowing the bin creation to account for the relationships between the features and response variable or a different number of bins for each feature.

In addition to assessing faithfulness by visually comparing the complex and explainer model, we propose a visual comparison of two faithfulness metrics (MSEE and average fidelity). The examples of faithfulness metric comparisons in this paper (Figures \ref{fig:sine-assess-metric} and \ref{fig:bullet-assess-metric}) both produced confusing results. The density based simulation methods resulted in the best or close to the best performance even though the feature heatmaps (Figures \ref{fig:sine-feat-heat} and \ref{fig:bullet-feature-heatmap}) showed that the density simulation methods produced global explanations with no variation in features selected by LIME. For the bin based simulation methods, the two metrics often did not agree or contradicted one another, which makes it difficult to decide on a recommendation of a set of tuning parameters that produces the explanations with the most faithful explainer model.

The metric comparison plot also includes a comparison of average $R^2$ values, which is a metric that can be used to assess the claim of \textbf{linearity}. Most of the average $R^2$ values in the examples from this paper are below 0.5 suggesting a poor linear fit of the explainer models. The poor linear fit of the explainer model is also seen with the residual plot (\autoref{fig:sine-plot-resids}).

The final claim, \textbf{localness}, is addressed by the feature heatmap and metric comparison plot. As stated, the feature heatmap revealed that the density simulation methods in the examples of this paper resulted in global explanations where the same features were repeatedly chosen across all (or almost all) observations in the set of explanations. This finding agrees with that of \citep{laugel:2018} who found LIME produced global explanations with the normal approximation simulation method. For the bin based simulation methods in the bullet data example, the feature heatmap showed that the features chosen for the explanations varied between the two classification categories (match versus non-match). This is an interesting finding that suggests that different features can play a role in the predictions of observations in different response categories, but the patterns of features selected by LIME within a classification category do not vary. Again, this is a suggestion of global explanations. Furthermore, while the metric comparison plot in the bullet example did not provide agreement between metrics on a best bin based method, all metrics agree that a Gower power of 0.5 for computing the model weights associated with distance of a simulated data point from the prediction of interest is best. This suggests that a less local explanation provided a better explanation of the performance of the random forest.

Some of the visualizations in the paper generalize easily to any application of LIME such as the feature heatmap and metric plot. Other plots such as the visualizations of the LIME procedure would require extensions such as the use of scatterplot matrices to compare explanations with more than two features. The addition of interactivity to the diagnostic plots would provide additional enhancement of the assessment process. For example, a diagnostic plot that provides a summary of multiple LIME explanations, such as the feature heatmap, could be displayed and clicked on to reveal more detailed figures associated with individual predictions of interest, such as plots of the simulated data and explainer model.

The largest limitation to the diagnostic visualizations is the dimensionality of the data shown, both in the number of dimensions or features as well as the number of observations. Fortunately, in the situation of LIME, both of these aspects are rather well controlled: LIME relies heavily on simulations to generate data scenarios that are close to the data observed but exhibits variability. Effects from overplotting should be relatively mild, because output from simulations is shown, which is expected to be (relatively) continuous \hh{such that overplotting only occurs for points with (relatively) similar values.} \kgc{What do you mean by continuous here?}. \hh{XXX Does that extra explanation help?} \kgc{Yes. I see what you mean now.} In that respect, the diagnostics shown for the \data\ and in the bullet example are representative of what is expected. But in cases where overplotting does become problematic, the user could either simply reduce the size of the simulations or use some well-studied binning techniques in the visualizations, as discussed for example in \citet{carr:1987} or \citet{unwin:2006}. 

While it would be ideal if LIME could be used as a method to provide easily understandable explanations for black-box models as \citep{ribeiro:2016} claim, that dream is not yet a reality. The examples using diagnostic plots to assess LIME in this paper show frequent issues with LIME. We hope that our plots provide motivation to assess LIME explanations, to not blindly use the default settings (even if it is not clear which tuning parameters to use), and to encourage work on improving LIME, so that it can be a lime and not a lemon.

\section*{Acknowledgments}
HH was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.

\subsection*{Author contributions}

\hh{A taxonomy of author contributions is suggested by Elsevier - I put the link in an email
}

{\bf Katherine Goode, Heike Hofmann}: Conceptualization, Methodology, Investigation, Writing- Reviewing and Editing.
{\bf Katherine Goode}: Writing Initial Draft, Software.

\subsection*{Financial disclosure}

None reported.

\subsection*{Conflict of interest}

The authors declare no potential conflict of interests.

\section*{Supporting information}

The following supporting information is available as part of the online article:


\bibliography{references}

% Do I need to include this?
% \section*{Author Biography}
% \begin{biography}
% {\includegraphics[width=60pt,height=70pt,draft]{empty}}
% {\textbf{Author Name.} This is sample author biography text this is sample author biography text}
% \end{biography}

% \textsc{\textbf{Ideas for describing proposed plots}}
% 
% \begin{itemize}
% \item start at the high level explanation (this is what we want to do and this is how we do it)
% \item go over a good and bad example
% \item include schematics of the plot first (good and bad) and then include the sine mini example version
% \item explain the expectation of the plot
% \end{itemize}
% 
% \textsc{\textbf{Comments for Editing}}
% 
% \begin{itemize}
% \item \hh{Generally: avoid 'can be'. Replace by 'is'.  }
% \item \hh{References like 'they', 'them' ... replace them by repeating the noun you are referring to to avoid any kind of ambiguity}
% \item \kgc{words to go back and make sure I am not using too often: ability, produce, understand}
% \item \kgc{decide on features or predictor variables}
% \item \hh{it's tuning parameters, not tunning} \kgc{Oops! I get that one wrong all of the time}
% \item \kgc{Adjust text sizes in plots so they all match}
% \item \kgc{Color scheme - comment from Heike:} \hh{I like the red-blue scheme for binary values, I also like the color scheme in the features heatmap. Generally papers and color don't go well together anyways, but we will solve that problem when we have to ... Most likely we will have to just pay the extra color fee}
% \item \kgc{Check that I am consistently referring to the perturbing as data simulation}
% \item \kgc{Clean up notation}
% \item \kgc{Referring to the linearity assumption:} \hh{Be a bit careful in the phrasing here - we can always approximate any function using a linear form (think Taylor expansion). The question is how big the error is.}
% \item \kgc{Make sure plots in the same figure are the same size}
% \item \kgc{Make sure I am distinguishing between RF predictions and probabilities correctly}
% \item \kgc{Use prediction of interest not case of interest}
% \item \kgc{Make sure I always treat the word 'data' as plural}
% \item \kgc{dataset or data set?}
% \item \kgc{scatterplot or scatter plot (check other plot names as well)}
% \item \kgc{choose whether to use the term feature or variable and make sure it is consistent throughout}
% \item \kgc{just include the details about LIME that are necessary to understand the plots and remove the rest to the appendix}
% \item \hh{avoid the use of implicit references like `this' and `that'. Make the reference explicit by adding what you are referring to, such as `this method', `that procedure'}
% \item \hh{make sure to use exact phrases. things like `LIME was developed in 2016' is factually questionable - but we know LIME was introduced in 2016. Also `the authors were interested in ...' implies that you know their intent. It's better to not guess any intent but stick to what we know.}
% \item \hh{`our` diagnostics ... you are using `our` quite often. It is good to claim work, but it is not good to claim things that we want others to use, because that is by definition excluding everybody but you. It's better to re-phrase with a focus on what you did. We introduce, we suggest, etc.}
% \item \kgc{check that I am always using present tense}
% \item \kgc{check that I am consistent in my use of the term black-box model}
% \end{itemize}
\newpage
\appendix

\section{LIME Tuning Parameter Options} \label{lime-details}

The following tuning parameters for the LIME algorithm are available in the \emph{lime} R package.

\begin{itemize}

\item Data simulation methods:

\begin{itemize}
\item Equally spaced bins: observations are uniformly sampled from equally spaced bins (number of bins may be specified)
\item Quantile bins: observations are uniformly sampled from quantile bins (number of bins may be specified)
\item Normal density approximation: observations are sampled from a normal distribution with mean and standard deviation computed from the corresponding feature
\item Kernel density approximation: observations are sampled from an kernel density approximation of the corresponding feature
\end{itemize}

\item Number of observations to simulate

\item Distance metric for determining proximity to the prediction of interest: Gower distance (where the power may be specified) or exponential kernel (where the kernel width may be specified)

\item Number of features to return in an explanation

\item Feature selection method for determining the features to return in an explanation: forward selection applied to a ridge regression, highest weights in ridge regression, LASSO, classification/regression tree splits

\end{itemize}

\section{Explanation Scatterplots Under Other Simulation Scenarios} \label{exp-scatter_plus}

Section \ref{exp-scatter} introduces explanation scatterplots under the default simulation method in the \emph{lime} R package: four quantile bins. The structure of an explanation scatterplot remains the same for if any bin based simulation method is used to create the simulated data (any number of quantile or equally spaced bins). However, if the kernel density or normal approximation simulation methods are used as the simulation method, the format of the explanation scatterplot changes. In the density based simulation method scenarios, LIME uses the standardized versions of the predictor variables to fit the explainer model. Thus, the explainer model needs to be represented differently in the explanation scatterplot.

When the kernel density or normal approximation simulation methods are applied, the explanation scatterplot depicts the complex model by plotting the complex model predictions versus a feature selected in LIME the explanation from the simulated data. The explainer model is included as a line on the figure where all features excluding the one plotted on the x-axis are set to the observed values of the prediction of interest. An explanation scatterplot is created for each feature included in the LIME explanation. As with the bin based simulation method, the size of the points represent the weight assigned by LIME.

Figure \ref{fig:sine-expl-scat-density} provides example explanation scatterplots for each feature in the default LIME explanation obtained using the kernel density simulation method for the \data \ prediction of interest.

\begin{figure*}[!thp]
<<sine-expl-scat-density, out.width = '\\textwidth', fig.width = 12, fig.height = 4.5, fig.align = "center">>=
plot_explain_scatter(
  sine_lime_explain$explain %>%
    filter(sim_method == "kernel_density", case == sine_poi %>% pull(case)),
  alpha = 0.75, title.opt = FALSE
) + 
  theme_bw(base_family = "Times") +
  theme(strip.background = element_rect(fill = "white", color = "white"),
        strip.placement = "outside")
@
\caption{Explanation scatterplots for the \data \ prediction of interest with the kernel density simulation method.}
\label{fig:sine-expl-scat-density}
\end{figure*}

Figure \ref{fig:bullet-expl-scat-density} includes explanation scatterplots for the explanations generated using kernel density simulation for the bullet example cases M and NM discussed in Section \ref{bullet-assess-ex}.

\begin{figure*}[!thp]
<<bullet-expl-scat-density, out.width = '\\textwidth', fig.width = 12, fig.height = 9, fig.align = "center">>=
bullet_es_NM <-
  plot_explain_scatter(
  bullet_explain_perms[13:15, ] %>% mutate(case = "M"),
  alpha = 0.75, title.opt = TRUE
) +
  theme_bw(base_family = "Times") +
  theme(strip.background = element_rect(fill = "white", color = "white"),
        strip.placement = "outside")

bullet_es_M <-
  plot_explain_scatter(
  bullet_explain_perms[16:18, ] %>% mutate(case = "NM"),
  alpha = 0.75, title.opt = TRUE
) +
  theme_bw(base_family = "Times") +
  theme(strip.background = element_rect(fill = "white", color = "white"),
        strip.placement = "outside")

plot_grid(bullet_es_NM, bullet_es_M, nrow = 2)
@
\caption{Explanation scatterplots for LIME explanations using kernel density simulation for the cases M and NM of the bullet comparison.}
\label{fig:bullet-expl-scat-density}
\end{figure*}

\section{Explainer Model Residual Plot} \label{residual-plot}

In order to assess the claim of linearity for the \data \ prediction of interest discussed in Section \ref{exp-scatter}, we use one of the most basic diagnostics in a statistician's tool box and draw a residual plot for the explainer model. This is shown in Figure \ref{fig:sine-plot-resids} with the explainer model residuals on the y-axis and explainer model predictions on the x-axis. The points along the x-axes have been jittered to ease the effect of the over-plotted points in the visualization. There is a clear increasing trend in the residuals as the explainer model predictions increase. This is a clear violation of the linearity assumption with the ridge regression model.

\begin{figure}[!thp]
<<sine-plot-resids, out.width = '0.5\\textwidth', fig.width = 5, fig.height = 4, warning = FALSE>>=

# Calculate the residuals from the explainer model
y <- sine_poi_perms$rfpred
sine_poi_explainer <-
  function(x1, x2) sine_b0 + (sine_b1 * x1) + (sine_b2 * x2)
sine_poi_perms <- sine_poi_perms %>%
  mutate(exppred = sine_poi_explainer(x1num, x2num)) %>%
  mutate(resid = exppred - rfpred)

# Create the residual plot
ggplot(sine_poi_perms, aes(x = exppred, y = resid)) +
  geom_jitter(alpha = 0.5, width = 0.02) +
  geom_hline(yintercept = 0) +
  theme_bw(base_family = "Times", base_size = 10) +
  labs(x = "Explainer Model Predictions",
       y = "Explainer Model Residuals",
       title = "Explainer Model Residual Plot")
@
\caption{Residual plot of the explainer model associated with \data \ prediction of interest from Section \ref{exp-scatter}. The residuals are plotted against the predicted values. The points are jittered in the x-direction to alleviate the over-plotting of points. There is an upward trend in the residuals as the explainer model predictions increase, which suggests a violation of the linearity assumption.}
\label{fig:sine-plot-resids}
\end{figure}

\section{Details on Assessment Metrics} \label{metric-details}

Suppose $f$ is a complex model, and let $\textbf{X}$ be a matrix of observed data with $K$ features and $E$ observations where $x_e$ is an observed feature vector for observation $e$. Let $f(x_e)$ be the complex model prediction for observation $e$. It is of interest to explain the predictions made by $f$ applied to $X$ using LIME. 

For $x_e$ and set of tuning parameters $t$, let $\textbf{X}_{e,t}'$ be the LIME simulated dataset with $K$ features and $S$ observations such that $x'_{e,t,s}$ is the feature vector for simulated data observation $s$. Let $\textbf{Z}'_{e,t}$ be the matrix of interpretability transformed simulated data with $K$ features and $S$ observations such that $z'_{e,t,s}$ is the interpretability transformed feature vector for observation/explanation $e$, tuning parameter $t$, and simulated data observation $s$. Note that $z_{e,t}$ will represent the interpretability transformed version of $x_e$.

Next, let $\omega_t$ represent a proximity distance metric. Then $\omega_t\left(x_e, x'_{e,t,s}\right)$ is the weight assigned to $x'_{e,t,s}$, which is the proximity between $x_e$ and $x'_{e,t,s}$. Allow $g_{e,t}$ to be the explainer model for an explanation $e$ and tuning parameter $t$. Thus, $g_{e,t}\left(z'_{e,s,t}\right)$ is the explainer model prediction for the interpretability transformed simulated data observation $s$. 

For a set of $E$ explanations and a set of tuning parameters $t$, we define the assessment metrics as follows:\\
\\
\emph{Average $R^2$} is denoted as $R^2_{\mbox{ave}}$ and computed as

  $$R^2_{\mbox{ave}} = \frac{1}{E}\sum_{e=1}^E R_{e,t}^2$$

where $R_{e,t}^2$ is the $R^2$ value for $g_{e,t}$.\\
\\
\emph{Average fidelity} is denoted by $\mathcal{L}_{\mbox{ave}}$ and computed as

\begin{eqnarray*} \mathcal{L}_{\mbox{ave}} & = & \frac{1}{E}\sum_{e=1}^E\mathcal{L}(f, \ g_{e,t}, \ \pi_{t}) \\ & = & \frac{1}{E}\sum_{e=1}^E\sum_{s=1}^{S}\omega_{t}\left(x_e, x'_{e,t,s}\right)\left(f\left(x'_{e,t,s}\right)-g_{e,t}\left(z_{e,t,s}'\right)\right)^2. \end{eqnarray*}\\
\\
\emph{Mean squared explanation error} is denoted by MSEE and computed as

$$MSEE=\frac{1}{E}\sum_{e=1}^E\left(f\left(x_e\right)-g_{e,t}\left(z_{e,t}\right)\right)^2.$$

\end{document}
