\documentclass[AMS,STIX2COL]{WileyNJD-v2}

% Packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{xcolor}

% Commands
\definecolor{orange}{rgb}{0.94, 0.59, 0.19}
\definecolor{purple}{rgb}{0.5, 0.0, 0.5}
\definecolor{teal}{rgb}{0, 0.502, 0.502}
\newcommand{\hh}[1]{\textcolor{orange}{#1}}
\newcommand{\kgc}[1]{\textcolor{purple}{#1}}
\newcommand{\kge}[1]{\textcolor{teal}{#1}}
\newcommand{\data}{sine data}
\renewcommand{\sout}[1]{\unskip}

% Set up for ASA data science journal
\articletype{Research Article}
\received{XX XX XXXX}
\revised{XX XX XXXX}
\accepted{XX XX XXXX}
\raggedbottom

\begin{document}

% Paper header (title and authors)
\title{Visual Diagnostics of a\kge{n} \sout{Model} Explainer \kge{Model} -- Tools for the Assessment of LIME Explanations}
\author[1]{Katherine Goode*}
\author[1,2]{Heike Hofmann}
\authormark{Goode and Hofmann}
\address[1]{\orgdiv{Department of Statistics}, \orgname{Iowa State University}, \orgaddress{\state{Iowa}, \country{United States}}}
\address[2]{\orgdiv{Center for Statistics and Applications in Forensic Evidence (CSAFE)}, \orgname{Iowa State University}, \orgaddress{\state{Iowa}, \country{United States}}}
\corres{*Katherine Goode, Department of Statistics, Iowa State University, Ames, IA. \email{kgoode@iastate.edu}}

% Abstract and keywords
\abstract[Abstract]{The importance of providing explanations for predictions made by black-box models has led to the development of \kge{explainer} model \sout{explainer} methods such as LIME (local interpretable model-agnostic explanations). LIME uses a surrogate model to explain the relationship between predictor variables and predictions from a black-box model in a local region around a prediction of interest. However, the quality of the resulting explanations relies on how well the explainer model captures the black-box model in a specified local region. Here we introduce three visual diagnostics to assess the quality of LIME explanations: (1) explanation scatterplots, (2) assessment metric plots, and (3) feature heatmaps. We apply the visual diagnostics to a forensics bullet matching dataset to show examples where LIME explanations depend on the tuning parameter values and the explainer model oversimplifies the black-box model. Our examples raise concerns about claims made of LIME that are similar to other criticisms in the literature.}
\keywords{explainable machine learning, black-box models, interpretability, statistical graphics, data science}

% Citation information
\jnlcitation{
\cname{\author{Goode K.}, \author{H. Hofmann}, }
\cyear{2020},
\ctitle{Visual Diagnostics of a\kge{n} \sout{Model} Explainer \kge{Model} -- Tools for the Assessment of LIME Explanations},
\cjournal{Stat Anal Data Min: The ASA Data Sci Journal},
\cvol{volume, number and page}.}

\maketitle

<<setup, echo = FALSE, include = FALSE>>=

# Specify global options
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  fig.keep = 'high',
  fig.align = 'center',
  dev = c("cairo_pdf", "cairo_ps"),
  dev.args = list(fallback_resolution = 800),
  dpi = 1200
)

# Load packages
library(bulletxtrctr) #remotes::install_github("heike/bulletxtrctr")
library(Cairo)
library(caret)
library(cowplot)
library(dplyr)
library(forcats)
library(ggforce)
library(ggplot2)
library(ggpcp) #remotes::install_github("yaweige/ggpcp")
library(glmnet)
library(gower)
library(graphics)
library(gretchenalbrecht) #remotes::install_github("dicook/gretchenalbrecht")
library(latex2exp)
library(lime) #remotes::install_github("goodekat/lime")
library(limeaid) #remotes::install_github("goodekat/limeaid")
library(magick)
library(purrr)
library(randomForest)
library(stringr)
library(tidyr)

# Specify the (base) font size (fs) and font (ff) for all plots
fs = 7
ff = "Helvetica"
@

\section{Introduction}

<<concept-data, eval = FALSE>>=

# # Simulate example data
# set.seed(20190624)
# lime_data <-
#   data.frame(
#     feature1 = sort(runif(250, 0, 1)),
#     feature2 = sample(x = 1:250, size = 250, replace = FALSE)) %>%
#   mutate(
#     feature1_stnd = (feature1 - mean(feature1)) / sd(feature1),
#     feature2_stnd = (feature2 - mean(feature2)) / sd(feature2),
#     prediction = if_else(feature1 >= 0 & feature1 < 0.1,
#                          (0.3 * feature1) + rnorm(n(), 0, 0.01),
#                  if_else(feature1 >= 0.1 & feature1 < 0.3,
#                          rbeta(n(), 1, 0.5),
#                  if_else(feature1 >= 0.3 & feature1 < 0.5,
#                          sin(pi* feature1) + rnorm(n(), 0, 0.5),
#                  if_else(feature1 >= 0.5 & feature1 < 0.8,
#                          -(sin(pi* feature1) + rnorm(n(), 0, 0.1)) + 1,
#                  if_else(feature1 >= 0.8 & feature1 < 0.9,
#                          0.5 + runif(n(), -0.5, 0.5),
#                          0.5 + rnorm(n(), 0, 0.3))))))) %>%
#   bind_rows(
#     data.frame(feature1 = rep(1, 30) + rnorm(5, 0, 0.05),
#            feature2 = rep(245, 30) + rnorm(5, 0, 1),
#            prediction = rep(0, 30) + rnorm(5, 0, 0.05)) %>%
#   mutate(feature1_stnd = (feature1 - mean(feature1)) / sd(feature1),
#          feature2_stnd = (feature2 - mean(feature2)) / sd(feature2)))
# 
# # Specify a prediction of interest
# prediction_of_interest <-
#   data.frame(feature1 = 0.07, feature2 = 200) %>%
#   mutate(feature1_stnd = (feature1 - mean(lime_data$feature1)) /
#            sd(lime_data$feature1),
#          feature2_stnd = (feature2 - mean(lime_data$feature2)) /
#            sd(lime_data$feature2),
#          prediction = 0.05,
#          color = factor("Prediction \nof Interest"))
# 
# # Specify the gower exponents
# good_gower_power <- 50
# bad_gower_power <- 1
# 
# # Compute the good distances between the prediction of interest
# # and all other observations
# lime_data$distance_good <-
#   (1 - gower_dist(x = prediction_of_interest %>%
#                     select(feature1_stnd, feature2_stnd),
#                   y = lime_data %>%
#                     select(feature1_stnd,
#                            feature2_stnd)))^good_gower_power
# 
# # Compute the bad distances between the prediction of interest
# # and all other observations
# lime_data$distance_bad <-
#   (1 - gower_dist(x = prediction_of_interest %>%
#                     select(feature1_stnd, feature2_stnd),
#                   y = lime_data %>%
#                     select(feature1_stnd,
#                            feature2_stnd)))^bad_gower_power
# 
# # Prepare the data for plotting
# lime_data_gathered <- lime_data %>%
#   gather(feature, feature_stnd_value,
#          feature1_stnd:feature2_stnd) %>%
#   gather(distance, distance_value, distance_good:distance_bad) %>%
#   mutate(distance = fct_recode(distance,
#                                "good" = "distance_good",
#                                "bad" = "distance_bad"),
#          feature = fct_recode(feature,
#                               "Feature 1" = "feature1_stnd",
#                               "Feature 2" = "feature2_stnd"))
# 
# # Prepare the prediction of interest data for plotting
# prediction_of_interest_gathered <- prediction_of_interest %>%
#   select(-feature1, -feature2) %>%
#   gather(feature, feature_value, feature1_stnd, feature2_stnd) %>%
#   mutate(feature = fct_recode(feature,
#                               "Feature 1" = "feature1_stnd",
#                               "Feature 2" = "feature2_stnd"))

@

<<concept-explainers, eval = FALSE>>=

# # Fit the good interpretable explainer model
# explainer_good <-
#   glmnet(x = lime_data %>% select(feature1_stnd, feature2_stnd)
#          %>% as.matrix(),
#          y = lime_data$prediction,
#          alpha = 0,
#          lambda = 1,
#          weights = lime_data$distance_good)
# 
# # Fit the bad interpretable explainer model
# explainer_bad <-
#   glmnet(x = lime_data %>% select(feature1_stnd, feature2_stnd)
#          %>% as.matrix(),
#          y = lime_data$prediction,
#          alpha = 0,
#          lambda = 1,
#          weights = lime_data$distance_bad)
# 
# # Join the coefficients from the explainer model into a dataframe
# coefs_data <- data.frame(case = c("good", "bad"),
#                          b0 = c(coef(explainer_good)[1],
#                                 coef(explainer_bad)[1]),
#                          b1 = c(coef(explainer_good)[2],
#                                 coef(explainer_bad)[2]),
#                          b2 = c(coef(explainer_good)[3],
#                                 coef(explainer_bad)[3])) %>%
#   mutate(feature1 = b0 + b2*prediction_of_interest$feature2_stnd,
#          feature2 = b0 + b1*prediction_of_interest$feature1_stnd) %>%
#   gather(key = feature, value = int, feature1:feature2) %>%
#   mutate(slope = c(coef(explainer_good)[2],
#                    coef(explainer_good)[3],
#                    coef(explainer_bad)[2],
#                    coef(explainer_bad)[3]),
#          feature = fct_recode(feature,
#                               "Feature 1" = "feature1",
#                               "Feature 2" = "feature2"))

@

<<concept-data-explainer>>=

# Specify the gower exponents
good_gower_power <- 0.00000000000000005
bad_gower_power <- 1

# Specify a prediction of interest
poi <-
  data.frame(
    poi = "Prediction \nof Interest",
    feature = 0.1,
    prediction = 0.05
  )

# Simulate example data
set.seed(20190624)
concept_data <-
  data.frame(poi = "Training Data", 
             feature = sort(c(runif(100, 0, 1), runif(250, 0, 7)))) %>%
  mutate(prediction = sin(feature) + rnorm(350, 0, 0.25)) %>%
  bind_rows(poi)

# Compute the good distances between the prediction of interest
# and all other observations
concept_data$weights_good <-
  1 - (gower_dist(
    x = poi %>% select(feature),
    y = concept_data %>% select(feature)
  )) ^ good_gower_power

# Compute the bad distances between the prediction of interest
# and all other observations
concept_data$weights_bad <-
  1 - (gower_dist(
    x = poi %>% select(feature),
    y = concept_data %>% select(feature)
  )) ^ bad_gower_power

# Fit the good and bad interpretable explainer models
explainer_good <-
  lm(prediction ~ feature, data = concept_data, weights = weights_good)
explainer_bad <-
  lm(prediction ~ feature, data = concept_data, weights = weights_bad)
  
# Create labels for the gower powers
good_power_label <-
  paste0("Faithful Local Explainer \n(exponent used to compute weights: ",
         good_gower_power,
         ")")
bad_power_label <-
  paste0("Unfaithful Local Explainer \n(exponent used to compute weights: ",
         bad_gower_power,
         ")")

# Join the coefficients from the explainer model into a dataframe
coefs_data <-
  data.frame(
    case = c(good_power_label, bad_power_label),
    b0 = c(coef(explainer_good)[1],
           coef(explainer_bad)[1]),
    b1 = c(coef(explainer_good)[2],
           coef(explainer_bad)[2]),
    feature = 6,
    prediction = 1.6
  ) %>%
  mutate(text = paste0("Slope: ", round(b1, 3)))

# Prepare data for plotting
concept_data_long <-
  concept_data %>%
  pivot_longer(names_to = "case",
               values_to = "weight",
               cols = weights_good:weights_bad) %>%
  mutate(case = ifelse(case == "weights_good", good_power_label, bad_power_label))
@

\begin{figure*}[!thp]
<<figure-01, out.width = '6.5in', fig.width = 7.95, fig.height = 3, warning = FALSE>>=

# Specify the figure size (for determining font size)
f1_ow = 6.5
f1_fw = 7.95
f1_fs = fs * (f1_fw / f1_ow)

# Plot of good explainer model
ggplot() +
  facet_grid(. ~ case) +
  geom_point(
    data = concept_data_long,
    mapping = aes(
      x = feature,
      y = prediction,
      fill = poi,
      color = poi,
      shape = poi,
      alpha = poi,
      size = weight
    )
  ) +
  geom_abline(
    data = coefs_data,
    mapping = aes(intercept = b0, slope = b1),
    size = 1
  ) +
  scale_fill_manual(values = c("#FAAA72", "grey30")) +
  scale_color_manual(values = c("grey30", "grey30")) +
  scale_alpha_manual(values = c(0.75, 0.6)) +
  scale_shape_manual(values = c(23, 21)) +
  geom_text(
    data = coefs_data,
    mapping = aes(x = feature,
                  y = prediction,
                  label = text),
    family = ff,
    size = f1_fs / 2.85
  ) +
  labs(x = "Black-Box Model Feature",
       y = "Black-Box Prediction",
       size = "Weight", 
       fill = "", 
       color = "",
       alpha = "",
       shape = "") +
  theme_linedraw(base_family = ff, base_size = f1_fs) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    strip.placement = "outside",
    strip.background = element_rect(color = "white", fill = "white"),
    strip.text.x = element_text(size = f1_fs, color = "black"),
    plot.title = element_text(size = f1_fs)
  ) +
  guides(
    fill = guide_legend(order = 1, override.aes = list(size = 5)),
    color = guide_legend(order = 1),
    shape = guide_legend(order = 1),
    alpha = guide_legend(order = 1),
    size = guide_legend(
      order = 2,
      override.aes = list(color = "grey30", alpha = 0.6)
    )
  )

# # Plot of good explainer model
# ggplot() +
#   facet_grid(. ~ feature, switch = "x") +
#   geom_point(
#     data = lime_data_gathered %>% mutate(
#       feature = fct_recode(
#         feature,
#         "Feature 1 (standardized)" = "Feature 1",
#         "Feature 2 (standardized)" = "Feature 2"
#       )
#     ),
#     mapping = aes(
#       x = feature_stnd_value,
#       y = prediction,
#       size = distance_value,
#       color = distance
#     ),
#     alpha = 0.6
#   ) +
#   geom_abline(
#     data = coefs_data %>%
#       filter(case == "good") %>%
#       mutate(
#         feature = fct_recode(
#           feature,
#           "Feature 1 (standardized)" = "Feature 1",
#           "Feature 2 (standardized)" = "Feature 2"
#           )
#         ),
#     mapping = aes(intercept = int, slope = slope),
#     size = 1
#   ) +
#   scale_color_manual(values = c(NA, "grey30"), guide = "none") +
#   geom_point(
#     data = prediction_of_interest_gathered %>% mutate(
#       feature = fct_recode(
#         feature,
#         "Feature 1 (standardized)" = "Feature 1",
#         "Feature 2 (standardized)" = "Feature 2"
#       )
#     ),
#     mapping = aes(x = feature_value,
#                   y = prediction,
#                   fill = color),
#     color = "black",
#     size = 5,
#     shape = 23,
#     alpha = 0.75
#   ) +
#   scale_fill_manual(values = "#FAAA72") +
#   geom_text(
#     data = data.frame(
#       feature_stnd_value = 1.2,
#       prediction = 1.6,
#       feature = c("Feature 1 (standardized)", "Feature 2 (standardized)"),
#       slope = c(
#         paste(
#           "Slope:",
#           coefs_data %>%
#             filter(case == "good",
#                    feature == "Feature 1") %>%
#             pull(slope) %>%
#             round(3)
#         ),
#         paste(
#           "Slope:",
#           coefs_data %>%
#             filter(case == "good",
#                    feature == "Feature 2") %>%
#             pull(slope) %>%
#             round(3)
#         )
#       )
#     ),
#     mapping = aes(x = feature_stnd_value,
#                   y = prediction,
#                   label = slope),
#     family = ff,
#     size = f1_fs / 2.85
#   ) +
#   labs(
#     x = "",
#     y = "Black-Box Prediction",
#     title = "Conceptual Depiction of a Faithful Local Explainer Model",
#     subtitle = paste("Gower Distance Metric Exponent:", good_gower_power),
#     fill = "",
#     alpha = "Weight",
#     size = "Weight"
#   ) +
#   theme_linedraw(base_family = ff, base_size = f1_fs) +
#   theme(
#     panel.grid.major = element_blank(),
#     panel.grid.minor = element_blank(),
#     axis.text.x = element_blank(),
#     axis.ticks.x = element_blank(),
#     axis.text.y = element_blank(),
#     axis.ticks.y = element_blank(),
#     strip.placement = "outside",
#     strip.background = element_rect(color = "white", fill = "white"),
#     strip.text.x = element_text(size = f1_fs, color = "black"),
#     plot.title = element_text(size = f1_fs)
#   ) +
#   guides(
#     fill = guide_legend(order = 1),
#     size = guide_legend(order = 2),
#     alpha = guide_legend(order = 2)
#   )

@
\caption{\kge{Hypothetical scenarios depicting the effect of different weights on LIME (Left) An example of an explainer model with appropriate weights to provide a faithful approximation of the complex model. (Right) An example of an explainer model for the same prediction but with different weights that results in an explanation unfaithful to the complex model.} \sout{A conceptual depiction of a faithful LIME explainer model in the immediate neighborhood of a prediction of interest \kge{for a hypothetical black-box model trained using two features: Feature 1 (left) and Feature 2 (right). The explainer model is a ridge regression weighted by the inverse Gower distance to the prediction of interest raised to the power of \Sexpr{good_gower_power}.}} \sout{The predictions from a hypothetical black-box model are plotted against the standardized values of the two hypothetical predictor variables. The diamond shaped points represent the location of a prediction of interest. The size and opacity of the circular points indicate the weight assigned based on the distance to the prediction of interest computed using the inverse of the Gower distance metric raised to the power of \Sexpr{good_gower_power}. The black lines represent a weighted ridge regression model used as an explainer model that reasonably captures the relationship between the black-box predictions and the features in a local region around the prediction of interest. That is, the explainer is faithful to the complex model and produces a reasonable explanation that Feature 1 plays a more important role in the prediction of interest than Feature 2 since the magnitude of the slope associated with Feature 1 is larger.}}
\label{fig:figure-01}
\end{figure*}

In the field of statistics, there are two main uses for models: inference and prediction. Machine learning models \sout{are often used for the latter purpose. These models} have proven to perform well \kge{in problems with the latter objective} \sout{in a wide range of prediction problems}, but the accuracy of many machine learning models comes at the cost of interpretability due to their algorithmic complexity (hence the phrase "black-box models"). Model interpretability allows for the understanding and assessment of how a model produces predictions. The lack of the ability to understand and assess a model makes it difficult to trust the model, especially in areas with high stakes decisions such as \kge{the} medical and forensics sciences. The increased use of machine learning models in applications and the introduction of the General Data Protection Regulation (GDPR) in 2018 \citep{goodman:2016} has resulted in a dramatic increase in explainable machine learning research, which focuses on developing ways to explain output from machine learning algorithms.

Throughout this paper, we distinguish between interpretability and explanability of models. We define {\it interpretability} as the ability to directly use model parameters to understand the relationships in the data captured by the model: e.g., a linear model coefficient associated with a predictor variable indicates the amount the response variable changes based on a change in the predictor variable. In contrast, define {\it explanability} as the ability to use the model in an indirect manner to understand the relationships in the data captured by the model: e.g., partial dependence plots depict the marginal relationship between model predictions and predictor variables \citep{friedman:2001}.

Numerous methods have been proposed to provide explanations for black-box model predictions \citep{gilpin:2018, guidotti:2018, ming:2017, molnar:2019}. Some are specific to one type of model (e.g. \citep{simonyan:2013} and \citep{urbanek:2008}), and others are model-agnostic (e.g. \citep{fisher:2018} and \citep{strumbelj:2014}). In this paper, we focus on \kge{the} \sout{one specific} model-agnostic method\sout{:} \kge{of} LIME \citep{ribeiro:2016}.

LIME (local interpretable model-agnostic explanations) \sout{is a method that} uses a surrogate model to relate predictor variables to black-box model predictions \sout{(i.e. a model explainer)} \citep{ribeiro:2016}. \sout{We distinguish between the terms of model explainer and explainer model: by \emph{model explainer} we denote the method for explaining a complex model using a surrogate model, while the \emph{explainer model}, or simply the \emph{explainer}, is the surrogate model.} While some \kge{explainer} model\kge{s} \sout{explainers} focus on understanding a model at the global level, LIME claims to provide explanations for individual predictions (local). Additionally, LIME is designed to work with any model (model-agnostic) and to produce easily understandable results (explanations) \citep{ribeiro:2016}. Conceptually, LIME fits a simple (interpretable) model, the explainer model, \kge{to approximate} \sout{meant to capture the behavior of} the \sout{(}complex\sout{) black-box} model in a local region around a prediction of interest. The simple model \kge{is interpreted to identify the} \sout{then provides interpretable estimates for} variables that most influence\sout{d} the \kge{complex model} prediction\sout{made by the complex model}.

\kge{While t}\sout{T}he concept of LIME is relatively simple\sout{: use an interpretable model to approximate a complex model in a local region. However}, a practical implementation of LIME is not straightforward, and research is being done to improve the procedure \citep{laugel:2018}. The current implementations of LIME \citep{pedersen:2020} \citep{ribeiro:2020} offer various tuning parameters (see Section \ref{background}) that affect the explainer model and ultimately, the explanation. Since the explainer model is an approximation of the complex model and not a direct interpretation, the explanations produced by an explainer model are subject to the quality of the approximation. Thus, in order to achieve \kge{reasonable} \sout{accurate} explanations, the tuning parameter values selected \kge{shoud} \sout{need to} be assessed.

\kge{To demonstrate the effect of a tuning parameter on LIME, we consider two different explainer models applied to the same prediction. Figure \ref{fig:figure-01} depicts two plots of the predictions from a hypothetical black-box model versus the feature used to train the model. The location of a prediction of interest is indicated by the diamonds. Globally, there is a clear non-linear relationship between the predictions and the feature, but the relationship could be approximated by a linear model in a local region around the prediction of interest. In both scenarios, a linear regression weighted by the proximity from an observation to the prediction of interest is used as the explainer model, but the proximities are computed differently as depicted by the varying sizes of the observations in the two plots.} 

\kge{On the left, the distance between an observation and the prediction of interest is computed using the Gower distance metric \citep{gower:1971} and then raised to the power of \Sexpr{good_gower_power} to emphasize a very local region. The distances are subtracted from 1 to obtain a proximity. Here, the explainer model is faithful to the complex model since it captures the linear relationship between the black-box predictions in an immediate neighborhood of the prediction of interest. The slope of \Sexpr{coefs_data %>% filter(case == good_power_label) %>% pull(b1) %>% round(3)} explains that the black-box predictions increase as the feature increases in the local region around the prediction of interest.}

\kge{For the scenario on the right, the computation of the proximities is the same except the distances are raised to a power of \Sexpr{bad_gower_power} (the default exponent in the \emph{lime} R package \citep{pedersen:2020}). This causes the observations that are further away from the prediction of interest to be given larger weights than the scenario on the left. Here, the explainer model does not capture the linear trend near the prediction of interest, but instead, the explainer captures a more global trend. Without an assessment of this explainer model, the slope of \Sexpr{coefs_data %>% filter(case == bad_power_label) %>% pull(b1) %>% round(3)} would incorrectly explain that the predictions decrease as the feature increases near the prediction of interest.} %\sout{This cannot be said of the explainer model in \autoref{fig:figure-02}. In addition, the magnitude of the slope associated with Feature 2  (\Sexpr{coefs_data %>% filter(case == "bad", feature == "Feature 2") %>% pull(b1) %>% round(3) %>% abs()}) is larger than that of Feature 1 (\Sexpr{coefs_data %>% filter(case == "bad", feature == "Feature 1") %>% pull(b1) %>% round(3) %>% abs()}). Thus, this explainer model actually provides a misleading explanation that Feature 2 plays a more important role in the prediction of interest.}

%\kge{To demonstrate the effect of tuning parameters on LIME,we consider two different explainer models applied to the same prediction. Suppose a hypothetical black-box model is trained using one feature. Both plots in Figure1show the black-box model predictions versus the feature used to train the model.The diamond shaped point in the plots indicates the location of the prediction of interest to explain. A weight ridge regression with the black-box predictions as the response variable and standardized versions the features as predictor variables will be used as the explainer model in both scenarios, but the weights will vary. Note that the standardized features allow for direct comparisons of the model coefficients.}

%\kge{Figure \ref{fig:figure-01} depicts the first scenario where the weights are computed as the inverse proximity of the point to the prediction of interest computed using the Gower distance metric \citep{gower:1971} raised to the power of \Sexpr{good_gower_power} to emphasize a very local region. The point size represents the weight, and the black lines depict the ridge regression model given the observed prediction of interest value from the opposite feature. For Feature 1 (left), the explainer model captures the relationship in an immediate neighborhood around the prediction of interest with a slope of \Sexpr{coefs_data %>% filter(case == "good", feature == "Feature 1") %>% pull(slope) %>% round(3)}, and for Feature 2 (right), the ridge regression has a small slope of \Sexpr{coefs_data %>% filter(case == "good", feature == "Feature 2") %>% pull(slope) %>% round(3)} that accurately reflects the absence of any global or local relationship between the black-box predictions and Feature 2. The magnitude of the slope associated with Feature 1 is larger than the slope of Feature 2, which suggests that Feature 1 plays a more important role in the prediction made by the black-box model for the prediction of interest. This explanation agrees with a visual assessment of the relationships between the predictions and predictor variables.}

%\sout{\autoref{fig:figure-01} provides a visualization of this conceptual understanding of LIME. The two plots show the predictions from a hypothetical black-box model plotted against the two hypothetical model predictor variables. The diamond shaped points represent the location of the prediction of interest. A Gower distance metric \citep{gower:1971} is used to define locality such that the size of the points represent the inverse of the Gower distance raised to some value and indicate the proximity to the prediction of interest. In the example of \autoref{fig:figure-01}, an exponent of \Sexpr{good_gower_power} is used to emphasize a very local region around the prediction of interest. A ridge regression model weighted by the proximity values is used as the explainer model with the black-box predictions as the response variable and standardized versions of two features in the data as predictor variables. Standardized features allow direct comparisons of the model coefficients. The explainer model is depicted by the black lines in the figure.}

%\sout{The plot on the left of \autoref{fig:figure-01} shows the relationship between the black-box predictions and Feature 1. Here, the explainer model is plotted with Feature 2 fixed at the observed value of the prediction of interest. The explainer model captures the relationship in an immediate neighborhood around the prediction of interest with a slope of \Sexpr{coefs_data %>% filter(case == "good", feature == "Feature 1") %>% pull(slope) %>% round(3)}. The plot on the right shows no global or local relationships between the black-box predictions and Feature 2. Here, the explainer model is plotted with Feature 1 set as the observed value of the prediction of interest, and it has an appropriately small slope of \Sexpr{coefs_data %>% filter(case == "good", feature == "Feature 2") %>% pull(slope) %>% round(3)}. The magnitude of the slope associated with Feature 1 is larger than the slope of Feature 2, which suggests that Feature 1 plays a more important role in the prediction made by the black-box model for the prediction of interest. This explanation agrees with a visual assessment of the relationships between the predictions and predictor variables.}

%\sout{The concept of LIME is relatively simple: use an interpretable model to approximate a complex model in a local region. However, a practical implementation of LIME is not straightforward, and research is being done to improve the procedure \citep{laugel:2018}. The current implementations of LIME \citep{ribeiro:2020} \citep{pedersen:2020} offer various tuning parameters (see Section \ref{background}) that affect the explainer model and ultimately, the explanation. Since the explainer model is an approximation of the complex model and not a direct interpretation, the explanations produced by an explainer model are subject to the quality of the approximation. Thus, in order to achieve accurate explanations, the tuning parameter values selected need to be assessed.}

%\kge{Figure \ref{fig:figure-02} depicts the second explainer model scenario. Again, the weights are computed using the Gower distance metric, but in this situation, the distances are raise to a power of \Sexpr{bad_gower_power} (the default exponent in the \emph{lime} R package).} \sout{We consider an example where the choice of exponent for the weights is crucial to the quality of the explanation. The plots in Figure \ref{fig:figure-02} show the same data as \autoref{fig:figure-01}, but the Gower distance metric exponent is decreased to \Sexpr{bad_gower_power} (the default exponent in the \emph{lime} R package).} This causes the observations that are further away from the prediction of interest to be given larger weights than before. In \autoref{fig:figure-01}, the explainer model captures the relationship between the black-box predictions in an immediate neighborhood of the prediction of interest. This cannot be said of the explainer model in \autoref{fig:figure-02}. In addition, the magnitude of the slope associated with Feature 2  (\Sexpr{coefs_data %>% filter(case == "bad", feature == "Feature 2") %>% pull(slope) %>% round(3) %>% abs()}) is larger than that of Feature 1 (\Sexpr{coefs_data %>% filter(case == "bad", feature == "Feature 1") %>% pull(slope) %>% round(3) %>% abs()}). Thus, this explainer model actually provides a misleading explanation that Feature 2 plays a more important role in the prediction of interest.

% \begin{figure*}[!thp]
% <<figure-02, out.width = '6.25in', fig.width = 7.95, fig.height = 3.25, warning = FALSE, eval = FALSE>>=
% 
% # Specify the figure size (for determining font size)
% f2_ow = 6.25
% f2_fw = 7.95
% f2_fs = fs * (f2_fw / f2_ow)
% 
% # Plot of good explainer model
% ggplot() +
%   facet_grid(. ~ feature, switch = "x") +
%   geom_point(
%     data = lime_data_gathered %>% 
%       mutate(
%         feature = fct_recode(
%           feature,
%           "Feature 1 (standardized)" = "Feature 1",
%           "Feature 2 (standardized)" = "Feature 2"
%           )
%         ),
%     mapping = aes(
%       x = feature_stnd_value,
%       y = prediction,
%       size = distance_value,
%       color = distance
%     ), 
%     alpha = 0.6
%   ) +
%   geom_abline(
%     data = coefs_data %>% 
%       filter(case == "bad") %>% 
%       mutate(
%         feature = fct_recode(
%           feature,
%           "Feature 1 (standardized)" = "Feature 1",
%           "Feature 2 (standardized)" = "Feature 2"
%           )
%         ),
%     mapping = aes(intercept = int, slope = slope),
%     size = 1
%   ) +
%   scale_color_manual(values = c("grey30", NA), guide = "none") +
%   geom_point(
%     data = prediction_of_interest_gathered %>% 
%       mutate(
%         feature = fct_recode(
%           feature,
%           "Feature 1 (standardized)" = "Feature 1",
%           "Feature 2 (standardized)" = "Feature 2"
%           )
%         ),
%     mapping = aes(x = feature_value,
%                   y = prediction,
%                   fill = color),
%     color = "black",
%     size = 5,
%     shape = 23,
%     alpha = 0.75
%   ) +
%   scale_fill_manual(values = "#FAAA72") +
%   geom_text(
%     data = data.frame(
%       feature_stnd_value = 1.2,
%       prediction = 1.6,
%       feature = c("Feature 1 (standardized)", "Feature 2 (standardized)"),
%       slope = c(
%         paste(
%           "Slope:",
%           coefs_data %>%
%             filter(case == "bad",
%                    feature == "Feature 1") %>%
%             pull(slope) %>%
%             round(3)
%         ),
%         paste(
%           "Slope:",
%           coefs_data %>%
%             filter(case == "bad",
%                    feature == "Feature 2") %>%
%             pull(slope) %>%
%             round(3)
%         )
%       )
%     ),
%     mapping = aes(x = feature_stnd_value,
%                   y = prediction,
%                   label = slope),
%     family = ff,
%     size = f2_fs / 3
%   ) +
%   labs(
%     x = "",
%     y = "Black-Box Prediction",
%     title = "Conceptual Depiction of an Unfaithful Local Explainer Model",
%     subtitle = paste("Gower Distance Metric Exponent:", bad_gower_power),
%     fill = "",
%     alpha = "Weight",
%     size = "Weight"
%   ) +
%   theme_linedraw(base_family = ff, base_size = f2_fs) +
%   theme(
%     panel.grid.major = element_blank(),
%     panel.grid.minor = element_blank(),
%     axis.text.x = element_blank(),
%     axis.ticks.x = element_blank(),
%     axis.text.y = element_blank(),
%     axis.ticks.y = element_blank(),
%     strip.placement = "outside",
%     strip.background = element_rect(color = "white", fill = "white"),
%     strip.text.x = element_text(size = f2_fs, color = "black"),
%     plot.title = element_text(size = f2_fs)
%   ) +
%   guides(
%     fill = guide_legend(order = 1),
%     size = guide_legend(order = 2),
%     alpha = guide_legend(order = 2)
%   )
% 
% @
% \caption{A conceptual depiction of an unfaithful local explainer model in the immediate neighborhood of a prediction of interest. A\kge{gain, a weighted} ridge regression \kge{is used as the explainer model} \sout{is fit to the same hypothetical black-box model predictions and predictor variables as in \autoref{fig:figure-01}}, but the \kge{Gower distance is raised to a power} \sout{weights are computed using a Gower exponent} of 1. \sout{The explainer model is unfaithful to the complex model in any immediate neighborhood of the prediction of interest.}}
% \label{fig:figure-02}
% \end{figure*}

\kge{\autoref{fig:figure-01} demonstrates a simple example in which the explanation quality varies drastically when different weights are used. Similar concerns with LIME have been raised in the literature.} \sout{Several sources in the literature discuss the performance of LIME.} \kge{\citet{laugel:2018} and \citet{molnar:2019} discuss the difficulty specifying a local region with LIME due to both} \sout{One of the biggest difficulties with LIME is determining how to specify a local region \citep{laugel:2018} \citep{molnar:2019}. This is due to} an unclear definition of \kge{what is} a "local region" and how to apply LIME to achieve an appropriate local region\sout{ as demonstrated by \autoref{fig:figure-02}}. \kge{A different concern about LIME is raised by} \citet{alvarezmelis:2018} \sout{raise a concern} pertaining to the robustness of explanations from LIME and other \kge{explainer} model\kge{s} \sout{explainers}: they find that even small changes in predictor variables can lead to very different LIME explanations. \kge{Even the original authors of LIME,} \sout{Additionally,} \citet{ribeiro:2016}\kge{,} acknowledge that if a linear model is used as the explainer, LIME relies on a linear approximation of the explainer model to the complex model and state "if the underlying model is highly non-linear even in the locality of the prediction, there may not be a faithful explanation".

\kge{In this paper, we stress the importance of assessing LIME by introducing several diagnostic plots for LIME and using the plots to highlight key issues with LIME.} \sout{As a result of the various ways LIME can fail, it is important to assess LIME explanations. We suggest the use of visual diagnostics for assessment. In this paper,} \kge{To do this,} we lay out the set of claims about LIME made by \citet{ribeiro:2016} and propose three visualizations for the assessment of these claims: (1) \emph{explanation scatterplots}, (2) \emph{feature heatmaps}, (3) \emph{assessment metric plots}. While LIME is implemented for image, tabular, and text data, we only focus on tabular data. For additional simplicity, we only discuss classification prediction models with a dichotomous response variable and continuous predictor variables. However, the proposed diagnostics may be extended to \kge{other} \sout{a wider range of} situations.

The remainder of the paper is structured as follows. Section \ref{background} provides background and claims made by \citet{ribeiro:2016} about LIME. We introduce the suggested diagnostic plots in Section \ref{diagnostics}. Then in Section \ref{application}, we demonstrate the use of the diagnostics to assess LIME explanations for a random forest  fit to a forensics bullet matching dataset. Section \ref{discussion} concludes with a discussion on extensions and limitations of the diagnostic plots and concerns about LIME in regards to the claims made by \citet{ribeiro:2016} brought about by the visualization examples in this paper that agree with \citet{alvarezmelis:2018}, \citet{laugel:2018}, and \citet{molnar:2019}.

All runs of LIME in this paper are executed in a forked version (https://github.com/goodekat/lime) of the R package \emph{lime} (version \Sexpr{packageVersion("lime")}) by \citet{pedersen:2020}. The forked version is functionally indistinguishable from \citeauthor{pedersen:2020}'s implementation but allows us to export internal values relevant for an assessment of the explainer. The diagnostic plots included in this paper are created using the R package \emph{limeaid} \citep{goode:2020}.

\section{Background on LIME} \label{background}

\citet{ribeiro:2016} provide an implementation of LIME in a Python package \cite{ribeiro:2020}. An adaption of the Python package in R has been implemented and made available by Thomas Lin-Pedersen \citep{pedersen:2020}. The discussion of parameter choices and implementation details of LIME in this paper are based on the R package.

The general form of the LIME algorithm can be divided into three steps \citep[see also][]{laugel:2018}:

\begin{enumerate}

\item \emph{Data Simulation and Interpretable Transformation}: Simulate a dataset from the original data used to fit the black-box model. Apply a transformation to the simulated data and the prediction of interest that will allow for interpretable explanations.

\item \emph{Explainer Model Fitting}: Apply the black-box model to the simulated data to obtain predictions. Compute the distance between each of the simulated data points and the prediction of interest. Perform feature selection. Fit an interpretable model with the black-box predictions from the simulated data as the response, the selected features from the transformed simulated data as the predictors, and the distances as weights. This model is the explainer model.

\item \emph{Explainer Model Interpretation}: Interpret the explainer model to determine which features played the most important role in the prediction of interest.

\end{enumerate}

During the application of LIME, the user is asked to select various tuning parameter options: the number of features to return in the explanation, the simulation method, the feature selection method, and how the weights are computed. An overview of the options available for the tuning parameters is included in Appendix~\ref{lime-details}.

In the original paper, \citet{ribeiro:2016} make the following set of claims regarding the performance of LIME:

\begin{itemize}
\item \emph{Interpretability}: The explainer model can be easily interpreted to provide meaningful explanations.
\item \emph{Faithfulness}: The explainer model sufficiently captures the relationship between the complex model predictions and the features in the local region around a prediction of interest to produce explanations that are faithful to the complex model.
\item \emph{Linearity}: By using a ridge regression model as the explainer model, it is assumed that there is a linear relationship between complex model predictions and the features in the local region around a prediction of interest.
\item \emph{Localness}: The explanations produced by LIME are local in regards to a prediction of interest.
\end{itemize}

The assumption of interpretability only depends on the complexity of the model used as explainer model. If the model is too complex to provide meaningful explanations (e.g. there are too many variables in the model), it is clear that the assumption of interpretability is violated. The other three assumptions are not as easy to assess, and for those, we suggest the use of diagnostic plots. 

<<sine-data, echo = FALSE, include = FALSE>>=

# Functions for rotating the data
rot_x <- function(x, y, theta) (x * cos(theta)) - (y * sin(theta))
rot_y <- function(x, y, theta) (x * sin(theta)) + (y * cos(theta))

# Generate the data
theta = -0.9
min = -10
max = 10
set.seed(20190913)
sine_data <- data.frame(x1 = runif(600, min, max),
                        x2 = sort(runif(600, min, max))) %>%
  mutate(x1new = rot_x(x1, x2, theta),
         x2new = rot_y(x1, x2, theta),
         y = ifelse(x2new > 5 * sin(x1new), "blue", "red")) %>%
  slice(sample(1:n())) %>%
  mutate(x3 = rnorm(600),
         case = 1:600) %>%
  select(case, everything())

# Separate the data into training and testing parts
set.seed(20191003)
rs <- sample(1:600, 500, replace = FALSE)
sine_data_train <- sine_data[rs,]
sine_data_test <- sine_data[-rs,]

# Fit a random forest
set.seed(20191003)
rfsine <- randomForest(x = sine_data_train %>% select(x1, x2, x3),
                       y = factor(sine_data_train$y))

# Obtain predictions on the training and testing data
sine_data_train$rfpred <- predict(rfsine)
sine_data_train <- cbind(sine_data_train, predict(rfsine, type = "prob"))
sine_data_train <- sine_data_train %>% 
  rename(rfprob_blue = blue, rfprob_red = red)
sine_data_test$rfpred <- predict(rfsine, sine_data_test %>% select(x1, x2, x3))
sine_data_test <- cbind(sine_data_test,
                        predict(rfsine, sine_data_test
                                %>% select(x1, x2, x3),
                                type = "prob"))
sine_data_test <- sine_data_test %>% 
  rename(rfprob_blue = blue, rfprob_red = red)

# Extract the prediction of interest from the sine test data
sine_poi <- sine_data_test %>% filter(y != rfpred, x1 > 0, x1 < 5, x2 > 5)

@

\begin{figure*}[!thp]
\centering
<<figure-02, out.width = '6.5in', fig.width = 8, fig.height = 3.1>>=

# Specify the figure size (for determining font size)
f3_ow = 6.5
f3_fw = 8
f3_fs = fs * (f3_fw / f3_ow)
f3_ls = 0.5 * (f3_fw / f3_ow)

# Create points representing the rotated since curve
sinefun_data <- data.frame(xnew = seq(min(sine_data$x1new),
                                      max(sine_data$x1new),
                                      by = 0.01)) %>%
  mutate(ynew = 5 * sin(xnew)) %>%
  mutate(x = rot_x(xnew, ynew, -theta),
         y = rot_y(xnew, ynew, -theta)) %>%
  filter(y >= -10, y <= 10)

# Specify colors for predictions
sinecolor_red = "firebrick"
sinecolor_blue = "steelblue"

# Plot the training data observed classes
sine_plot_obs <- ggplot(sine_data_train, aes(x = x1, y = x2, color = y)) +
  geom_point(alpha = 0.8) +
  geom_path(data = sinefun_data, aes(x = x, y = y),
            color = "black", 
            size = f3_ls) +
  scale_colour_manual(values = c(sinecolor_blue, sinecolor_red)) +
  theme_bw(base_family = ff, base_size = f3_fs) +
  theme(aspect.ratio = 1, 
        plot.title = element_text(size = f3_fs)) +
  labs(x = TeX("$x_1$"),
       y = TeX("$x_2$"),
       color = TeX("y"),
       title = "Training Data")

# Plot the testing data rf predictions
sine_plot_pred <- ggplot() +
  geom_point(data = sine_poi %>%
               mutate(shape = "Prediction \nof Interest"),
             mapping = aes(x = x1,
                           y = x2,
                           shape = shape),
             size = 5,
             alpha = 0.8,
             color = "black") +
  geom_point(data = sine_data_test %>% filter(y != rfpred),
             mapping = aes(x = x1, y = x2),
             shape = 1,
             size = 3,
             color = "black") +
  geom_point(data = sine_data_test,
             mapping =  aes(x = x1,
                            y = x2,
                            color = rfprob_blue,
                            fill = rfprob_blue),
             shape = 21,
             alpha = 0.8) +
  geom_path(data = sinefun_data, aes(x = x, y = y),
            color = "black", 
            size = f3_ls) +
  scale_color_gradient2(low = sinecolor_red,
                        mid = '#f7f7f7',
                        high = sinecolor_blue,
                        midpoint = 0.5) +
  scale_fill_gradient2(low = sinecolor_red,
                        mid = '#f7f7f7',
                        high = sinecolor_blue,
                        midpoint = 0.5) +
  scale_shape_manual(values = 23) +
  theme_bw(base_family = ff, base_size = f3_fs) +
  theme(aspect.ratio = 1,
        plot.title = element_text(size = f3_fs)) +
  labs(x = TeX("$x_1$"),
       y = TeX("$x_2$"),
       color = "Random \nForest \nProbability \nfor 'blue'",
       fill = "Random \nForest \nProbability \nfor 'blue'",
       title = "Testing Data",
       shape = "") +
  guides(shape = guide_legend(order = 1))

# Join the plots
plot_grid(sine_plot_obs, sine_plot_pred,
          nrow = 1,
          rel_widths = c(0.4825, 0.5175))

@
\caption{Plots of $x_2$ versus $x_1$ from the training (left) and testing (right) sets of the \data \ introduced in Section \ref{diagnostics}. The true classification boundary is shown as the solid black line in both plots. The color of the training data  represents the value of the observed response variable ($y$). The color of the testing data  represents the random forest probability that an observation belongs to the category of blue. The \Sexpr{dim(sine_data_test %>% filter(y != rfpred))[1]} cases that are misclassified by the random forest are identified by black circles. The prediction of interest to explain is indicated by a diamond.}
\label{fig:figure-02}
\end{figure*}

\section{Visual Diagnostics for LIME} \label{diagnostics}

<<sine-lime>>=

# Apply LIME with various tuning parameters to the sine data
if (!file.exists("data/sine-lime-explain.rds")) {

    # Apply lime with various input options
    sine_lime_explain <- apply_lime(
      train = sine_data_train %>% select(x1, x2, x3),
      test = sine_data_test %>% select(x1, x2, x3),
      model = rfsine,
      label = "blue",
      n_features = 2,
      sim_method = c('quantile_bins', "kernel_density"),
      nbins = 2:6,
      feature_select = "auto",
      dist_fun = "gower",
      kernel_width = NULL,
      gower_pow = 1,
      return_perms = TRUE,
      all_fs = TRUE,
      seed = 20190914)

    saveRDS(sine_lime_explain, "data/sine-lime-explain.rds")
  
    } else {

    sine_lime_explain <- readRDS("data/sine-lime-explain.rds")

  }

@

<<sine-lime-default>>=

# Extract the explanations from the default lime application
sine_lime_default <- sine_lime_explain$explain %>% filter(nbins == 4)

# Extract the explanations from the default lime application
# for the prediction of interest only
sine_poi_lime_default <- sine_lime_default %>%
  filter(case == sine_poi %>% pull(case)) %>%
  mutate(case = c("Prediction of Interest", "Prediction of Interest"))

@

In this section, we introduce three visual diagnostic plots for the assessment of LIME. The plots focus on different levels of application of LIME (e.g. on explanation versus a set of explanations) to assess the LIME claims from different perspectives:

\begin{enumerate}
\item \emph{Explanation Scatterplot} (Section \ref{exp-scatter}): Comparison of the explainer and complex models for an individual prediction of interest.
\item \emph{Feature Heatmap} (Section \ref{feat-heat}): Comparison of features selected by LIME across applications of LIME with different tuning parameter values.
\item \emph{Assessment Metric Plot} (Section \ref{assess-metric}): Comparison of performance metrics for LIME across applications of LIME with different tuning parameter values. 
\end{enumerate}

\paragraph{The \data}

To demonstrate the visual diagnostics, we generate an example dataset that will be referred to as the \data. The \data \ contains \Sexpr{dim(sine_data)[1]} observations with three features and one response variable. The features, $x_1$, $x_2$, and $x_3$, are randomly sampled from Unif(\Sexpr{min}, \Sexpr{max}), Unif(\Sexpr{min}, \Sexpr{max}), and $\mbox{N}(0,1)$ distributions, respectively. A binary response variable $y$ is  created using a rotated sine curve. In particular, let $x'_1=x_1\cos(\theta)-x_2\sin(\theta)$ and $x'_2=x_1\sin(\theta)+x_2\cos(\theta)$ where $\theta=-0.9$. Then $y$ is defined as
\begin{eqnarray}\label{eq:data}
  y=\begin{cases}
  \mbox{blue} & \mbox{ if } x'_2 > \sin\left(x'_1\right) \\
  \mbox{red} & \mbox{ if } x'_2 \le \sin\left(x'_1\right) \ . %\nolabel
  \end{cases}
\end{eqnarray}
Note that due to the creation of $y$ in this manner, $y$ is dependent on $x_1$ and $x_2$ and independent of $x_3$. The dataset is divided into training and testing sets of \Sexpr{dim(sine_data_train)[1]} and \Sexpr{dim(sine_data_test)[1]} observations, respectively.  A random forest  is fit using the R package \emph{randomForest} (version \Sexpr{packageVersion("randomForest")}) \citep{liaw:2002} with the default settings. The model is applied to the test set to obtain predictions. 

\autoref{fig:figure-02} shows scatterplots of $x_2$ versus $x_1$ from the training data (left) and the testing data (right). Both plots include the true classification boundary of the rotated sine function plotted as the solid black line. The training data are colored by the observed response variable ($y$), and the testing data are colored by random forest  prediction probabilities. The random forest  misclassifies \Sexpr{dim(sine_data_test %>% filter(y != rfpred))[1]} points, which are all located near  the classification boundary. These are identified by open circles in \autoref{fig:figure-02}. 

From these scatterplots, we also see that the global relationship between response $y$ and features $x_1$ and $x_2$ is linear: the probability for label blue increases with the difference between features $x_2$ and $x_1$. Locally, the relationship between $y$ and features $x_2$ and $x_1$ varies a lot more around the line of identity. Here, the relationship is determined by the sine waves. However, the sine is a good-natured function that can be approximated well linearly in local regions.

We apply LIME using six sets of tuning parameter values to all observations in the \data \ test set to observe variability across tuning parameter values. A quantile bin based simulation method (samples are simulated uniformly from a specified number of quantile bins) is used for five of the LIME applications with the number of bins varying from 2 to 6 by application. We use 6 bins as the maximum, because the complexity of the explanations increases with the number of bins. Note that 4-quantile-bins are the default method in \emph{lime}. The sixth application of LIME uses a kernel density simulation method (samples are drawn from kernel density approximations of the feature distributions). The default methods for feature selection (forward selection) and the computation of the weights (Gower distance raised to an exponent of 1) are used for all applications. (See Appendix~\ref{lime-details} for descriptions of the tuning parameters.)

For the presentation of the explanation scatterplot, we focus on the misclassified point with $(x_1, x_2, x_3)$ coordinates of (\Sexpr{round(sine_poi$x1, 2)}, \Sexpr{round(sine_poi$x2, 2)}, \Sexpr{round(sine_poi$x3, 2)}) indicated by a diamond in \autoref{fig:figure-02}. Misclassified points are often of interest to explain since they may provide information about ways to improve the model. For the introduction of the other two  plots, we consider the LIME explanations for  all observations in the \data \ test data.

<<warning = FALSE>>=

# Obtain the simulated data associated with the poi
sine_poi_perms <- sine_poi_lime_default %>%
  slice(1) %>%
  select(perms_raw, perms_pred_complex, perms_numerified, weights) %>%
  mutate(perms_numerified =
           map(perms_numerified,
               .f = function(x) rename(x,
                                       "x1num" = "x1",
                                       "x2num" = "x2",
                                       "x3num" = "x3"))) %>%
  unnest(cols = c(perms_raw, perms_pred_complex, perms_numerified, weights)) %>%
  select(-red) %>%
  rename(rfpred = blue) %>%
  mutate(case = factor(1:n())) %>%
  select(case, everything())

# Determine the bin cuts for the default lime application
sine_lime_default_bin_cuts <- sine_lime_default %>%
  select(feature, feature_desc) %>%
  unique() %>%
  separate(feature_desc, c("other", "bin_cut"), sep = " <= ") %>%
  select(-other) %>%
  na.omit() %>%
  mutate(bin_cut = as.numeric(bin_cut)) %>%
  arrange(feature) %>%
  mutate(case = rep(1:3, 2)) %>%
  spread(feature, bin_cut)

@

<<sine-poi-explainer, warning = FALSE>>=

# Determine the lime explanation cutoffs
sine_poi_bounds <- sine_poi_lime_default %>%
  select(case, feature, feature_value, feature_desc) %>%
  separate(feature_desc, c("other", "upper"), sep = " <= ") %>%
  separate(other, c("lower", "feature2"), sep = " < ") %>%
  mutate(upper = ifelse(is.na(upper), "Inf", upper)) %>%
  select(-feature2) %>%
  mutate_at(.vars = c("lower", "upper"), .funs = as.numeric)

# Extract the coefficients from the explainer
sine_b0 <- sine_poi_lime_default$model_intercept[1]
sine_b1 <- sine_poi_lime_default$feature_weight[2]
sine_b2 <- sine_poi_lime_default$feature_weight[1]

# Extract the bounds of the bins
x1_lower <- sine_poi_bounds %>% filter(feature == "x1") %>% pull(lower)
x1_upper <- sine_poi_bounds %>% filter(feature == "x1") %>% pull(upper)
x2_lower <- sine_poi_bounds %>% filter(feature == "x2") %>% pull(lower)
x2_upper <- sine_poi_bounds %>% filter(feature == "x2") %>% pull(upper)

# Function for computing the predicted value via the explainer model
sine_explainer <- function(z1, z2) sine_b0 + sine_b1 * z1 + sine_b2 * z2

@

\begin{figure*}[!thp]
<<figure-03, out.width = '6.5in', fig.width = 12, fig.height = 5, warning = FALSE>>=

# Specify the figure size (for determining font size)
f4_ow = 6.5
f4_fw = 12
f4_fs = fs * (f4_fw / f4_ow)
f4_ls = 0.5 * (f4_fw / f4_ow)

# Create the lime R package explanation visualization for the sine data
sine_poi_exp <- 
  sine_poi_lime_default %>% 
  plot_features() + 
  theme(text = element_text(family = ff, size = f4_fs),
        strip.text = element_text(size = f4_fs, face = "plain"))

# Create the explanation scatterplot for the lime explanation for the sine data
exp_scatter <-
  plot_explain_scatter(
    sine_poi_lime_default,
    alpha = 0.4,
    line_size = f4_ls
  ) +
  facet_grid(switch = "both") +
  labs(
    x = TeX("x_1"),
    y = TeX("x_2"),
    size = "Weight",
    shape = "",
    fill = "Random \nForest \nProbability \nfor 'blue'",
    color = "Random \nForest \nProbability \nfor 'blue'",
    linetype = "",
    title = "Explanation Scatterplot"
  ) +
  theme_bw(base_family = ff, base_size = f4_fs) +
  guides(shape = guide_legend(order = 1),
         size = guide_legend(
           nrow = 2,
           byrow = T,
           override.aes = list(alpha = 1)
         )) +
  theme(aspect.ratio = 1,
        plot.title = element_text(size = f4_fs)) 

# Join the plots
plot_grid(sine_poi_exp, exp_scatter) 

# Save the figure to use in the GitHub repo readme
# ggsave(
#   filename = "figure-readme.png",
#   plot = plot_grid(sine_poi_exp, exp_scatter),
#   width = 12,
#   height = 5
# )

@
\caption{(left) Visualization from the \emph{lime} R package of a LIME explanation for the \data \ prediction of interest identified in \autoref{fig:figure-02}. (right) \emph{Explanation Scatterplot}. The points represent the simulated data colored by the random forest probabilities and sized by the assigned LIME weights. The prediction of interest is shown as the diamond shaped point. The explainer model indicator variables associated with $x_1$ and $x_2$ (quantile bins containing the prediction of interest) are depicted by the solid lines. The line colors represent the explainer model coefficient signs and indicate whether the feature supports a prediction of `blue' (blue lines) or supports a prediction of `red' (red lines). This figure shows that the quantile bins are not flexible enough to capture the relationship between the random forest predictions and the $x_1$ and $x_2$ values.}
\label{fig:figure-03}
\end{figure*}

\paragraph{A Visual Representation of a LIME Explanation}

Before introducing the visual diagnostics, let us consider a commonly used visualization of a LIME explanation that is shown in Figure \ref{fig:figure-03} (left). The plot is created using the \emph{lime} R package and provides a visual representation of a LIME explanation for the prediction of interest indicated in \autoref{fig:figure-02}. In particular, the explanation is obtained using 4-quantile-bins to simulate the data. In this scenario, LIME converts continuous predictor variables to indicator variables identifying whether the variable value falls in the same quantile bin as the prediction of interest or not. The indicator variables are used as the explainer model features.  

The plot reports \Sexpr{round(sine_poi_lime_default$label_prob[1], 2)} as  the random forest  probability  that the observation of interest belongs to category blue (denoted as the ``label" in the plot). The lengths of the bars represent the  coefficients from a ridge regression model (the explainer model) fit using the R package \emph{glmnet} \citep{simon:2011} associated with the indicator variables chosen via feature selection. The color of a  bar denotes the sign of the coefficient and whether the feature "supports" or "contradicts" a random forest classification of `blue'. The "explanation fit" is the deviance ratio from  \emph{glmnet}. In other words, this is the $R^2$ value associated with the explainer model. In this case, it is \Sexpr{round(sine_poi_lime_default$model_r2[1], 2)} suggesting that the explainer model is not a good linear fit. However, it is commonly accepted that $R^2$ has limitations for assessing the quality of fit of a model \citep{sapra:2014} and  should not be used as the only metric in a model assessment.

The explanation for the prediction of interest depicted in Figure \ref{fig:figure-03} is interpreted as follows. A random forest classification of blue is supported by the prediction of interest having a value of $x_2$ that is greater than \Sexpr{str_split(sine_poi_lime_default$feature_desc[[1]], pattern = " ")[[1]][1]}, but the prediction of interest having a value of $x_1$ that is greater than \Sexpr{str_split(sine_poi_lime_default$feature_desc[[2]], pattern = " ")[[1]][1]} and less than or equal to \Sexpr{str_split(sine_poi_lime_default$feature_desc[[2]], pattern = " ")[[1]][5]} provides support against a classification of blue. Since the weight associated with $x_2$ has a larger magnitude than the weight associated with $x_1$, LIME explains that $x_2$ plays a more important role in the random forest prediction. Additionally, consider that since the support for blue by $x_2$ outweighs the support for red by $x_1$, the explainer model overall favors a label of blue for the prediction of interest, which agrees with the random forest probability of \Sexpr{sine_poi_lime_default$label_prob[1]} for blue. Note that both models classify the observation incorrectly.

\subsection{Explanation Scatterplots} \label{exp-scatter}

Based on the plot on the left of \autoref{fig:figure-03} alone, it is not possible to make an informed assessment of the explanation. For a further assessment of the explainer model, we turn to an \emph{explanation scatterplot}: a visual diagnostic for assessing the LIME claims of locality and fidelity for an individual explanation by juxtaposing the complex and explainer models in one plot. The format of an explanation scatterplot depends on the LIME simulation method. We introduce the explanation scatterplot here under the \emph{lime} R package default method of 4-quantile-bins.

The explanation scatterplot applies the concept of plotting the model in the data space discussed in \citet{wickham:2015}. The plot is built by plotting the LIME simulated data for the top two features identified by the explanation in a scatterplot and coloring these points by the predictions from the complex model. The point size represents the weight assigned by LIME. In order to show the LIME results for the observation of interest, lines are drawn on top of the points. These lines  represent the boundaries of the indicator variables used to fit the explainer model. The line color  denotes whether LIME indicates that a feature supports or contradicts a class prediction. Appendix \ref{exp-scatter_plus} addresses the explanation scatterplot formats in other LIME simulation scenarios.

An explanation scatterplot corresponding to the LIME explanation depicted on the left in \autoref{fig:figure-03} is shown on the right hand side of \autoref{fig:figure-03}. By juxtaposing the random forest predictions and the explainer model boundaries, we are able to assess the faithfulness and localness of the explainer model. 

First, consider the claim of localness. The weights decay relatively slowly outside of the intersection of the 2-quantile-bin suggesting that the LIME explanation is highly influenced by points outside of the bins containing the prediction of interest. However, it is difficult to say if the claim of localness has been violated, because a definition of a local region is not specified. Depending on what region a viewer considers to be local, an argument could be made in favor of or against a violation of localness. The explanation scatterplot raises awareness of the unclear definition of a local region with LIME. Nevertheless, it is possible to say that the weights assigned to the simulated data do not capture  the local region identified by the intersection of the quantile bins. 

Now, consider the claim of faithfulness: It can be said that the majority of the points in the $x_2$ quantile support a prediction of blue, which is captured by the bar supporting a prediction of blue in the LIME explanation, and a similar statement can be made about the $x_1$ quantile-bin. These statements validate the explanation produced by LIME. However, the explanation scatterplot plot shows that the random forest performs well at capturing the sine curve classification boundary by creating various sized rectangles consisting of predictions with similar probabilities, which the LIME explanation does not pick up on. Thus, LIME does provide an explanation for the prediction, but it is a very poor explanation in terms of faithfulness to the random forest prediction regions.

It is difficult to assess linearity from this explanation scatterplot, but a residual plot of the explainer model could be used to check the linearity claim. See Appendix~\ref{residual-plot} for the residual plot associated with the explanation considered in Figure \ref{fig:figure-03}, which shows a violation of the linearity claim.

This example explanation only includes two features. In situations where more than two features are included in a LIME explanation, the explanation scatterplots can be extended to a generalized pairs plot \citep{emerson:2013} that includes all pairwise combinations of features. Generalized pairs plots (and scatterplot matrices in general) have diminishing value when the number of features increase \citep{jensen:2011} \citep{sweller:2011}. Machine learning models are commonly fit using a large number of features, and therefore, a generalized pairs plot of explanation scatterplots for all features would be ineffective. However, when applying LIME, the user selects the number of features to return in the explanation. In the \emph{lime} R package, \citet{pedersen:2020} encourage users to select less than 10 features. As long as a small number of features are returned in the LIME explanation,  it is feasible to use a generalized pairs plot of explanation scatterplots. An example is shown in Section~\ref{bullet-assess-ex}.

\subsection{Feature Heatmap} \label{feat-heat}

\begin{figure}[!thp]
<<figure-04, out.width = '3.125in', fig.width = 6, fig.height = 3>>=

# Specify the figure size (for determining font size)
f5_ow = 3.125
f5_fw = 6
f5_fs = fs * (f5_fw / f5_ow)

# Specify the number of cases and LIME input options
ncases = 10
ninputs = 5

# Create a good case of chosen feature example data
set.seed(20191008)
heatmap_data_good <-
  tibble(
    case = factor(rep(1:ncases, each = ninputs),
                  levels = ncases:1),
    input = factor(rep(1:ninputs, ncases)),
    feature = factor(c(
      rep(1, 5),
      rep(2, 5),
      rep(3, 5),
      rep(1, 5),
      rep(1, 5),
      rep(4, 5),
      rep(1, 5),
      rep(3, 5),
      rep(1, 5),
      rep(4, 5)
    ))
  ) %>%
  mutate(input = forcats::fct_recode(
    input,
    "A" = "1",
    "B" = "2",
    "C" = "3",
    "D" = "4",
    "E" = "5"
  ))

# Create the conceptual good heatmap
heatmap_plot_good <- ggplot(data = heatmap_data_good,
                            mapping = aes(
                              x = input,
                              y = case,
                              fill = feature,
                              color = feature
                            )) +
  geom_tile() +
  labs(
    x = "Tuning Parameter Value",
    y = "Case",
    fill = "Feature",
    color = "Feature",
    title = "Situation 1",
    subtitle = "Consistent Explanations"
  ) +
  theme_bw(base_family = ff, base_size = f5_fs) +
  scale_fill_grey() +
  scale_color_grey() +
  theme(legend.position = "none", 
        plot.title = element_text(size = f5_fs))

# Create a bad case of chosen feature example data
set.seed(20190627)
heatmap_data_bad <- tibble(
  case = factor(rep(1:ncases, ninputs),
                levels = ncases:1),
  input = factor(rep(1:ninputs, each = ncases)),
  feature = factor(rep(c(1, 2, 4, 3, 2), each = 10))
) %>%
  mutate(input = forcats::fct_recode(
    input,
    "A" = "1",
    "B" = "2",
    "C" = "3",
    "D" = "4",
    "E" = "5"
  ))

# Create the conceptual bad heat map
heatmap_plot_bad <- ggplot(data = heatmap_data_bad,
                           mapping = aes(
                             x = input,
                             y = case,
                             fill = feature,
                             color = feature
                           )) +
  geom_tile() +
  labs(
    x = "Tuning Parameter Value",
    y = "Case",
    fill = "Feature",
    color = "Feature",
    title = "Situation 2",
    subtitle = "Inconsistent Explanations"
  ) +
  theme_bw(base_family = ff, base_size = f5_fs) +
  theme(plot.title = element_text(size = f5_fs)) +
  scale_fill_grey() +
  scale_color_grey()

heatmap_leg <- get_legend(heatmap_plot_bad)
heatmap_plot_bad <-
  heatmap_plot_bad + theme(legend.position = 'none')

# Join the plots
plot_grid(
  heatmap_plot_good,
  heatmap_plot_bad,
  heatmap_leg,
  nrow = 1,
  rel_widths = c(0.43, 0.43, 0.14)
)

@
\caption{Hypothetical examples of feature heatmaps in two possible situations. The heatmaps show the top feature chosen for 10 cases across 5 different sets of tuning parameter values. The color of the cell indicates the feature chosen by LIME. Situation 1 is the ideal, because the explanations vary across cases but do not dependent on specific tuning parameter values.}
\label{fig:figure-04}

\vspace*{\floatsep}

<<figure-05, out.width = '3.125in', fig.width = 4, fig.height = 4.5>>=

# Specify the figure size (for determining font size)
f6_ow = 3.125
f6_fw = 4
f6_fs = fs * (f6_fw / f6_ow)

# Create the example feature heatmap with the sine data
plot_feature_heatmap(sine_lime_explain$explain,
                     order_method = "PCA") +
  theme_bw(base_family = ff, base_size = f6_fs) +
  theme(
    legend.position = "bottom",
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    strip.background = element_rect(color = "white", fill = "white")
  ) +
  labs(y = "Case")

@
\caption{\emph{Feature Heatmap}. An example feature heatmap of the explanations from the applications of LIME with different tuning parameter values to the \data \ test set. The cases from the test set are plotted on the y-axis, and the tuning parameter values (simulation methods here) are included on the x-axis. The colors of the tiles indicate the feature selected by LIME for the corresponding case and tuning parameter value. The plot is faceted by the first and second most important features selected by LIME (top and bottom facets, respectively). The vertical facets separate the  kernel density method from the quantile bin methods. The vertical striping indicates that the LIME explanations are not consistent across tuning parameter values.}
\label{fig:figure-05}
\end{figure}

Explanations produced by LIME are likely to be affected by the choice of tuning parameter values. A hypothetical example of this is shown by Figures \ref{fig:figure-01} and \ref{fig:figure-02} where the method used to weight the observations influenced the explanation. As of the time of writing this manuscript, we have encountered no recommendations for how to specify the parameter values besides for the default settings in \emph{lime}. In order to compare the explanations produced by LIME using different tuning parameter values and provide another perspective for assessing localness, we visualize an overview of the explanations with the \emph{feature heatmap} diagnostic plot. 

The feature heatmap uses colors to identify the features selected by LIME across multiple predictions (referred to as cases here) and tuning parameter values organized by the  feature importance assigned by LIME. That is, for LIME applied with $t$ sets of tuning parameter values to $n$ cases to select the $f$ top features, create $f$ heatmaps (one for each of the positions of importance determined by the magnitude of the explainer model coefficients) with the cases on the $y$-axis, the tuning parameter values on the $x$-axis, and the cells colored by the feature chosen for the corresponding case and tuning parameter value. Additional tuning parameters may also be included in the plot via facets.

Two hypothetical examples of feature heatmaps are included in \autoref{fig:figure-04}. The plots are created with the assumption that LIME is applied to select the top feature out of $p=4$  features for $n=10$ cases with $t=5$ sets of tuning parameter values. Situation 1 is  an example where the features selected are consistent across tuning parameter values within a case but vary across cases within a tuning parameter value. This is the ideal situation, because the LIME explanations do not depend on the tuning parameters but do depend on the location of the observation in the feature space. Situation 2 is an example where the selected features vary across tuning parameter values within a case but are consistent across cases within a tuning parameter value. This situation indicates that the features selected by LIME are dependent on the tuning parameters, and the explanations may not be  local, because the same feature is chosen regardless of the case. In practice, it is expected that the plot will exhibit a combination of these two situations.

\autoref{fig:figure-05} shows a feature heatmap for the LIME applications to the 100 observations in the \data \ test set. The most important and second most important features selected by lime are shown in in the top and bottom facets, respectively. For the quantile bins, the original features prior to the indicator variable transformation are included since it is obvious that different features would be selected when the sizes of the bins change. This figure shows that for kernel density and 2-quantile-bins, LIME selects $x_1$ as the most important feature and $x_2$ as the second most important feature across all cases in the test set. These explanations are not local. There is variability in the features selected by LIME for 3- to 6-quantile-bins suggesting more local explanations. There are signs of vertical striping, which suggests a dependence on tuning parameters. Note that the explanations from 3- and 5-quantile-bins include the selection of the random noise variable ($x_3$) as an important variable in many predictions, which should not be the case. The pattern seen in the explanations for 3- to 6-quantile-bins may suggest local explanations, but the dependence on tuning parameters makes it unclear which set of explanations to use.

\subsection{Assessment Metric Plot} \label{assess-metric}

\begin{figure}[!thp]
<<figure-06, out.width = '3.25in', fig.width = 4.5, fig.height = 3.75, warning = FALSE>>=

# Specify the figure size (for determining font size)
f7_ow = 3.25
f7_fw = 4.5
f7_fs = fs * (f7_fw / f7_ow)

# Create the metric plot
plot_metrics(sine_lime_explain$explain, rank_legend = "discrete") +
  theme_bw(base_family = ff, base_size = f7_fs) +
  theme(
    strip.background = element_rect(color = "white", fill = "white"),
    strip.placement = "outside",
    strip.text.y = element_text(size = f7_fs)
  )

@
\caption{\emph{Assessment metric plot}. The assessment metrics computed on the applications of LIME to the \data \ test set are included in this example assessment metric plot. Each horizontal facet corresponds to one of three metrics: average $R^2$, average fidelity, or MSEE. The LIME tuning parameter values (simulation methods in this example) are plotted on the x-axis, and the metric values are shown on the y-axis. The points are colored by rank (within a metric) indicating best to worst (dark to light). The kernel density simulation methods performs well across all three metrics.}
\label{fig:figure-06}
\end{figure}

The feature heatmap for the \data \ in the previous section shows an example of inconsistent LIME explanations across tuning parameter values. In this situation, the user must determine which set of explanations to trust. One way to do this is to compute assessment metrics for each set of explanations to identify the optimal tuning parameter values. We discuss three metrics for this purpose and present a visual comparison in an \emph{assessment metric plot}.

Each metric presented below is computed on  LIME explanations for a set of predictions obtained using the same tuning parameter values. Here we provide a high level description of the metrics. Notation and formulas for these metrics are included in Appendix \ref{metric-details}.

\begin{itemize}
\item \emph{Average $R^2$}: Assess the model fit and linearity claim by computing the average of the explainer model $R^2$ values (deviance ratios from the R package \emph{glmnet}).

\item \emph{Average Fidelity}: Measures the faithfulness of the explainer model to the complex model by comparing their predictions. Computed as the average of the explainer model fidelity metrics: a metric presented in \citet{ribeiro:2016} (the weighted distance between explainer and complex model predictions for all observations in the LIME simulated data associated with an individual prediction of interest). 

\item \emph{Mean Squared Explanation Error (MSEE)}: Also measures the faithfulness of the explainer model to the complex model by comparing their predictions, but only the prediction of interest is used to compute an average squared deviation between explainer and complex model predictions.
\end{itemize}

\autoref{fig:figure-06} shows an example assessment metric plot. The three metrics are computed for each of the LIME applications to the \data \ test set. The simulation methods are listed on the x-axis. The plot is faceted by metric, and the metric values are plotted on the y-axis. The colors of the points represent the rank of the simulation methods performance based on a particular metric (darker indicates a better metric value and lighter indicates a worse metric value). Higher average $R^2$ values are better, and lower average fidelity and MSEE values are better. This example only includes one tuning parameter: the simulation method. If more than one tuning parameter is considered, the assessment metric plot is extended by adding additional facets or levels to the x-axis.

All three metrics suggest that the kernel density method performed well, but the metrics disagree for the quantile bins methods. Average $R^2$ and average fidelity rank the performance of the number of quantile bins the same (2-quantile-bins perform the best and 6-quantile-bins perform the worst). In fact, these two metrics appear to have a mirrored  relationship in this example. MSEE provides almost the exact opposite results with  6-quantile-bins performing the best and 2-quantile-bins performing the worst. Average fidelity and MSEE are similar metrics, so it is not surprising that they would agree in this example. Another factor might be that MSEE only takes the prediction of interest  into account and not the full simulated dataset. The contradiction between metrics makes it difficult to identify which simulation method to trust. 

Recall that \autoref{fig:figure-05} indicated that the kernel density method selected the same feature across all cases in the test set for both the first and second features. It appears that a global trend may be the best explanation for this example, which may be reasonable considering that we know that both $x_1$ and $x_2$ are the two features that should be the features used by the random forest to distinguish between response categories.

\section{Application to Bullet Matching Data} \label{application}

In this section, we provide a discussion of the application of the visual diagnostics for LIME explanations to a practical data problem investigating the similarity of marks on fired bullets.

\subsection{Bullet Matching Data}

<<bullet-data>>=
# Load the bullet matching training and testing data
bullet_train <- readr::read_csv("data/bullet-train.csv.zip")
bullet_test <- readr::read_csv("data/bullet-test.csv.zip")
@

<<bullet-rf>>=

# Extract the features from the original random forest model (rtrees)
bullet_features <- rownames(bulletxtrctr::rtrees$importance)
bullet_rf_ntrees <- bulletxtrctr::rtrees$ntree
bullet_rf_mtry <- bulletxtrctr::rtrees$mtry

# Train a new random forest that mimics the rtrees models 
# (or load if already trained)
bullet_rf_file = "./data/bullet-rf.rds"
if (!file.exists(bullet_rf_file)) {
  set.seed(20200923) # the day that the updated data was found :)
  bullet_rf <-
    randomForest(
      y = factor(bullet_train$samesource),
      x = bullet_train %>% select(all_of(bullet_features)),
      ntree = bullet_rf_ntrees,
      mtry = bullet_rf_mtry
    )  
  saveRDS(object = bullet_rf, file = bullet_rf_file)
} else {
  bullet_rf = readRDS(file = bullet_rf_file)
}

# Add variables to both training and testing data with the random forest probability
# that the signatures are a match from the model bullet_rf
bullet_train$rfscore = predict(bullet_rf, bullet_train %>% select(all_of(bullet_features)), type = "prob")[,2]
bullet_test$rfscore = predict(bullet_rf, bullet_test %>% select(all_of(bullet_features)), type = "prob")[,2]

# Compute OOB model metrics
bullet_rf_oob_acc = round(sum(predict(bullet_rf) == bullet_train$samesource) / length(bullet_train$samesource), 2)
bullet_rf_oob_fp = round(bullet_rf$confusion[,3][2], 2)
bullet_rf_oob_fn = round(bullet_rf$confusion[,3][1], 4)

# Obtain model predictions and confusion matrix from test data
bullet_test_pred = predict(bullet_rf, bullet_test %>% select(all_of(bullet_features)))
bullet_test_confus = table(bullet_test_pred, bullet_test$samesource)

# Compute test data model metrics
bullet_rf_test_acc = round(sum(bullet_test_pred == bullet_test$samesource) / length(bullet_test$samesource), 2)
bullet_rf_test_fp = round(bullet_test_confus[2,1] / (bullet_test_confus[2,1] + bullet_test_confus[2,2]), 2)
bullet_rf_test_fn = round(bullet_test_confus[1,2] / (bullet_test_confus[1,1] + bullet_test_confus[1,2]), 4)
@

<<bullet-feature-importance>>=
# Order the features based on feature importance
bullet_features_ordered <-
  data.frame(
    feature = rownames(bullet_rf$importance),
    MeanDecreaseGini = bullet_rf$importance
  ) %>%
  arrange(desc(MeanDecreaseGini)) %>%
  mutate(
    feature = fct_recode(feature,
      "Cross Correlation\nFunction" = "ccf",
      "Consecutively\nMatching Striae" = "cms",
      "Matches" = "matches",
      "Mismatches" = "mismatches",
      "Non-Consecutively\nMatching Striae" = "non_cms",
      "Rough\nCorrelation" = "rough_cor",
      "Distance Standard\nDeviation" = "sd_D",
      "Distance" = "D",
      "Sum of Peaks" = "sum_peaks"
    )
  )

# Create an ordered version of the data
bullet_features_ordered <-
  bullet_features_ordered %>%
  mutate(feature = factor(feature, levels = bullet_features_ordered$feature))

@

In current practice, forensic firearm examiners evaluate whether two bullets are from the same source (fired from the same gun) or from different sources based on microscopic comparison of the striation patterns engraved on bullets during the firing process (see \autoref{fig:figure-07}). The process is based on a visual and therefore subjective assessment of the evidence. The lack of objective evaluation and the associated absence of established error rates has first been criticized by the National Research Council \cite{nrc:2009} and later by the President's Council of Advisors on Science and Technology \cite{pcast:2016}.

In response, \citet{hare:2017} proposed an automated machine learning method for bullet matching to complement a visual inspection by firearm examiners. Based on high-resolution topological scans of land engraved areas, \citet{hare:2017} obtain signatures of striations from two bullet lands (\autoref{fig:figure-08}). Nine features quantifying the similarity of signatures, such as the cross-correlation function, the distance between signatures, and the number of matching striae, are extracted and used to train a random forest to determine the probability of a comparison resulting from the same source (matching signatures) or from different sources (non-matching signatures). The model in \citet{hare:2017} was trained on a set of scans of bullets from the James Hamby Consecutively Rifled Ruger Barrel Study \citep{hamby:2009}, which included 10,384 land-to-land comparisons \citet{hare:2017}. See the supporting information for additional information on the signature similarity features.

\begin{figure}[!t]
\centering
<<figure-07, out.width = '2.25in', fig.width = 6, fig.height = 3, warning = FALSE>>=

# Convert the bullet figure to eps (if needed)
if (!file.exists("figure-static/figure-07-1.eps")) {
  unzip(zipfile = "figure-static/figure-07-1.png.zip", exdir = "figure-static")
  bullets <- image_read("figure-static/figure-07-1.png")
  image_write(bullets, path = "figure-static/figure-07-1.eps", format = "eps")
}

# Load and print the plot
knitr::include_graphics("figure-static/figure-07-1.eps")

@
\caption{(Top left) Traditionally rifled gun barrel. The grooves and lands alternate to give bullets a spin during the firing process, which create markings (striations) on a bullet when fired. (Top right) Image of a fired bullet. The vertical stripes along the lower half of the bullet show groove and land engraved areas. The land engraved areas contain the microscopic striations created when the bullet passed through the barrel of the gun. (Bottom) Close up of a land engraved area showing striations (vertical lines).}
\label{fig:figure-07}

\vspace*{\floatsep}

<<figure-08, out.width = '3.125in', fig.width = 6, fig.height = 3.5, warning = FALSE>>=

# Specify the figure size (for determining font size)
f9_ow = 3.125
f9_fw = 6
f9_fs = fs * (f9_fw / f9_ow)

# Load the signatures data
signatures <- readr::read_csv("data/example-signatures.csv.zip")

# Create plot of the signatures
signatures %>%
  ggplot(aes(x = x/1000, y = y)) + 
  geom_line() +
  facet_grid(land ~ .) +
  ylim(c(-4,6)) +
  theme_bw(base_family = ff, base_size = f9_fs) +
  theme(strip.background = element_blank()) +
  ylab("Surface Measurement (in microns)") +
  xlab("Relative Location (in millimeters)") 

@
\caption{Example bullet signatures. The bullet signatures correspond to the same land and therefore have very similar patterns. The \citet{hare:2017} random forest is fit using various features that measure the similarity between two such signatures.}
\label{fig:figure-08}
\end{figure}

\subsection{Application of LIME to Bullet Matching Data}

\begin{figure*}[!thp]
<<figure-09, out.width = '6.5in', warning = FALSE>>=

# File names for parallel coordinate plot
pcp_file_name_png = "figure-static/figure-09-1.png"
pcp_file_name_eps = "figure-static/figure-09-1.eps"

# Create the parallel coordinate plot (if needed)
if (!file.exists(pcp_file_name_png) | !file.exists(pcp_file_name_eps)) {

  # Specify the figure size (for determining font size)
  f10_ow = 6.5
  f10_fw = 7.5
  f10_fs = 7 * (f10_fw / f10_ow)
  f10_ls = 0.5 * (f10_fw / f10_ow)

  # Create the plot
  bullet_pcp <-
    bullet_train %>%
    select(all_of(bullet_features), samesource, rfscore) %>%
    bind_rows(bullet_test %>%
                select(all_of(bullet_features), samesource, rfscore),
              .id = "set") %>%
    mutate(
      set = fct_recode(set,
                       "Training Set" = "1",
                       "Testing Set" = "2"),
      samesource = fct_recode(
        factor(samesource),
        "Match" = "TRUE",
        "Non-Match" = "FALSE"
      )
    ) %>%
    rename(
      "Cross Correlation\nFunction" = "ccf",
      "Consecutively\nMatching Striae" = "cms",
      "Matches" = "matches",
      "Mismatches" = "mismatches",
      "Non-Consecutively\nMatching Striae" = "non_cms",
      "Rough\nCorrelation" = "rough_cor",
      "Distance Standard\nDeviation" = "sd_D",
      "Distance" = "D",
      "Sum of Peaks" = "sum_peaks"
    ) %>%
    arrange(rfscore) %>%
    ggplot(aes(color = rfscore)) +
    geom_pcp(
      mapping = aes(vars = vars(bullet_features_ordered$feature)),
      alpha = 0.2,
      size = f10_ls
    ) +
    facet_grid(set ~ samesource) +
    scale_color_gradient2(low = "grey50",
                          high = "darkorange",
                          midpoint = 0.5) +
    theme_bw(base_family = ff, base_size = f10_fs) +
    theme(
      axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
      strip.placement = "outside",
      strip.background = element_rect(color = "white",
                                      fill = "white")
    ) +
    labs(x = "Features Ordered by Random Forest Variable Importance \n(highest to lowest from left to right)",
         y = "Scaled Feature Values",
         color = "Random \nForest \nProbability")

  # Save the plot
  ggplot2::ggsave(
    plot = bullet_pcp,
    filename = pcp_file_name_png,
    width = 7.5,
    height = 3.5,
    units = "in",
    dpi = 400
  )

  # Load the figure and then save as an EPS file
  bullet_pcp <- image_read(pcp_file_name_png)
  image_write(bullet_pcp, path = pcp_file_name_eps, format = "eps")

}

# Load and print the plot
knitr::include_graphics(pcp_file_name_eps)

@
\caption{Parallel coordinate plots of the \citet{hare:2017} random forest predictions from the training (top facet) and testing (bottom facet) data. The observations are separated by known matching (right column) and non-matching (left column) signatures. The y-axis shows the standardized feature values, and the x-axis shows the features used to fit the random forest (ordered by random forest impurity based feature importance). Each line corresponds to an observation, and the line color represents the associated random forest probability. There are clear relationships between the feature values and the random forest probabilities.}
\label{fig:figure-09}
\end{figure*}

Since firearm identification is commonly used as evidence for convictions in court cases, it is important to be able to understand and assess a model used to quantify the probability that a bullet is fired from a gun. LIME explanations would provide a local explanation for an individual prediction, but just as it is important to assess the model for this high-stakes application, it is also important to assess the LIME explanations. We will demonstrate an assessment of LIME explanations using the visual diagnostics introduced in this paper.

A random forest model fit to an expanded dataset of \Sexpr{format(dim(bullet_train)[1], big.mark = ",")} land-to-land comparisons from two sets of bullets in the James Hamby Consecutively Rifled Ruger Barrel Study \citep{hamby:2009} was validated in \citet{vanderplas:2020}. Here, we train a random forest model to mimic the model validated in \citet{vanderplas:2020} using the same data, model structure, and features (the nine similarity measures developed in \citet{hare:2017}). We apply the trained random forest model to 6 bullets from another set of the Hamby study with \Sexpr{dim(bullet_test)[1]} rows of land comparisons. See the supporting information for further descriptions of the data. 

The newly trained random forest has an out-of-bag accuracy of \Sexpr{bullet_rf_oob_acc} and out-of-bag false positive and false negative rates of \Sexpr{bullet_rf_oob_fp} and \Sexpr{bullet_rf_oob_fn}, respectively. On the test data, the random forest performance decreases with an accuracy of \Sexpr{bullet_rf_test_acc} and false positive and false negative rates of \Sexpr{bullet_rf_test_fp} and \Sexpr{bullet_rf_test_fn}, respectively. This is an example where explanations of the model predictions could provide insight to the cause of the decrease in model performance on the test data.

We consider a global visualization of the relationship between the random forest predictions and the model features with a parallel coordinate plot of the training data (top facet) and testing data (bottom facets)  (\autoref{fig:figure-09}). In the training data we observe a clear difference in feature values based on whether the corresponding random forest scores are close to 1 or 0. High values of rough correlation, cross correlation function, and number of matches are indicative of random forest scores close to 1; similarly, low values of mismatches, distance, and non-consecutively matching striae are also associated with random forest scores close to 1. Observations in the test data that are classified incorrectly by the random forest tend to have feature values similar to observations in the training data with similar random forest scores. For example, the observations in the test set known to be a match but have a random forest score below 0.5 have relatively low values of rough correlation, cross correlation function, and matches and relatively large values of mismatches, distance, and non-consecutively matching striae compared to observations with a random forest probability close to 1.

LIME is applied to all test set observations using different tuning parameter values: 12 sampling methods (2 to 6 equally spaced bins, 2 to 6 quantile-bins, kernel density estimation, and normal approximation) and 3 Gower exponents (0.5, 1, and 10). Thus, a total of $12\times 3=36$ different applications of LIME are performed. We specify that each LIME explanation return 3 features, and feature selection is performed using the default option in LIME, which selects the features with the highest weights in a ridge regression model.

<<bullet-lime>>=

# Apply LIME to all but two cases without returning the permutations
if (!file.exists("data/bullet-lime.rds") | !file.exists("data/bullet-explain.rds")){

  # Apply lime with various input options to the Hamby data
  bullet_lime_explain_noperms <- apply_lime(
    train = bullet_train %>%
      select(all_of(bullet_features)),
    test = bullet_test %>%
      select(all_of(bullet_features)),
    model = bullet_rf,
    label = as.character(TRUE),
    n_features = 3,
    sim_method = c('quantile_bins', 'equal_bins',
                   'kernel_density', 'normal_approx'),
    nbins = 2:6,
    feature_select = "auto",
    dist_fun = "gower",
    kernel_width = NULL,
    gower_pow = c(0.5, 1, 10),
    return_perms = FALSE,
    all_fs = FALSE,
    seed = 20190914)

  # Separate the lime and explain parts of the results
  bullet_lime_noperms <- bullet_lime_explain_noperms$lime
  bullet_explain_noperms <- bullet_lime_explain_noperms$explain

  # Save the output objects
  saveRDS(object = bullet_lime_noperms, file = "data/bullet-lime.rds")
  saveRDS(object = bullet_explain_noperms, file = "data/bullet-explain.rds")
  
} else {

  # Load the files
  bullet_explain_noperms <- readRDS("data/bullet-explain.rds")

}

@

<<bullet-lime-perms>>=

# Specify two cases of interest
bullet_poi_match <- 
  bullet_test %>% 
  filter(land_id1 == "Hamby224-Set11-B1-L5", 
         land_id2 == "Hamby224-Set11-B2-L2") %>% 
  pull(case)
bullet_poi_nonmatch <- bullet_test %>% 
  filter(land_id1 == "Hamby224-Set1-B2-L5", 
         land_id2 == "Hamby224-Set1-BUnk-L3") %>% 
  pull(case)

# Apply LIME to two cases with the permutations returned
bullet_lime_explain_perms <- apply_lime(
  train = bullet_train %>% select(all_of(bullet_features)),
  test = bullet_test %>%
    filter(case %in% bullet_poi_match) %>%
    bind_rows(bullet_test %>%
                filter(case %in% bullet_poi_nonmatch)) %>%
    select(all_of(bullet_features)),
  model = bullet_rf,
  label = as.character(TRUE),
  n_features = 3,
  sim_method = c(
    'quantile_bins',
    'equal_bins',
    'kernel_density',
    'normal_approx'
  ),
  nbins = 4,
  feature_select = "auto",
  dist_fun = "gower",
  kernel_width = NULL,
  gower_pow = 0.5,
  return_perms = TRUE,
  all_fs = FALSE,
  seed = 20190914
)

# Separate the lime and explain parts of the results
bullet_lime_perms <- bullet_lime_explain_perms$lime
bullet_explain_perms <- bullet_lime_explain_perms$explain %>%
  mutate(case = ifelse(case == 1,
                       bullet_poi_match,
                       bullet_poi_nonmatch) %>% as.character())

@

<<bullet-lime-combined>>=

# Determine the application and case number of the poi
# in the no_perms data
poi_cases_no_perms <- bullet_explain_noperms %>%
  filter(case %in% c(bullet_poi_match, bullet_poi_nonmatch),
         nbins %in% c(4, NA),
         gower_pow == 0.5) %>%
  select(case, implementation) %>%
  unique()

# Join the no perms data (with poi removed) with the
# perms data (perms removed)
bullet_explain <- bullet_explain_noperms %>%
  anti_join(poi_cases_no_perms,
            by = c("implementation", "case")) %>%
  bind_rows(
    bullet_explain_perms %>%
      mutate(implementation = fct_recode(implementation,
          "3" = "1",
          "8" = "2",
          "31" = "3",
          "32" = "4"
        )
      ) %>%
      select(
        -perms_raw,-perms_numerified,
        -perms_pred_simple,-perms_pred_complex,
        -weights
      )
  )

@

\subsection{LIME Assessment Visualizations} \label{bullet-assess-ex}

To get an overview of the LIME explanations from the 36 applications, we consider a feature heatmap (\autoref{fig:figure-10}). In addition to facets for simulation method and LIME feature importance, this plot includes a vertical facet for Gower power and a horizontal facet for whether the observation is a known match or non-match. This plot highlights several key features of the LIME explanations from the bullet matching dataset.

First, applications of 2-quantile-bin simulations produce the same explanations for almost all cases and LIME tuning parameter values. This suggests that the LIME explanations are global and not local. Second, within a simulation method, the features selected by LIME for an observation do not appear to vary by the Gower power. However, the LIME explanation for an observation often varies across simulation methods. With the equal-bins, there are vertical stripes that suggest a dependence of the LIME explanations on the number of bins. The vertical stripes are not as apparent with the quantile bins. Lastly, there are clear differences between the LIME explanations produced by the bin based simulation methods for the matches and non-matches. This suggests that different features are of importance in the random forest, depending on whether the observation corresponds to match or non-match.

\begin{figure*}[!thp]
<<figure-10, out.width = '6.5in', fig.width = 18, fig.height = 14>>=

# Specify the figure size (for determining font size)
f11_ow = 6.5
f11_fw = 18
f11_fs = fs * (f11_fw / f11_ow)

# Create a feature heatmap
plot_feature_heatmap(
  bullet_explain %>%
    mutate(
      label = as.factor(label),
      feature = fct_recode(
        feature,
        "Rough Correlation" = "rough_cor",
        "Consecutively Matching Striae" = "cms",
        "Distance" = "D",
        "Matches" = "matches",
        "Mismatches" = "mismatches",
        "Non-Consecutively Matching Striae" = "non_cms",
        "Cross Correlation Function" = "ccf",
        "Sum of Peaks" = "sum_peaks",
        "Distance Standard Deviation" = "sd_D"
      )
    ),
  facet_var = bullet_test %>%
    mutate(samesource = fct_recode(
      factor(samesource),
      "Match" = "TRUE",
      "Non-Match" = "FALSE"
    )) %>%
    pull(samesource),
  order_method = "PCA"
) +
  scale_fill_gretchenalbrecht(palette = "last_rays", discrete = TRUE) +
  scale_color_gretchenalbrecht(palette = "last_rays",
                               discrete = TRUE,
                               reverse = TRUE) +
  theme_bw(base_family = ff, base_size = f11_fs) +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    strip.background = element_rect(color = "white", fill = "white"),
    strip.text.y.right = element_text(angle = 0, size = f11_fs*0.7),
    strip.text.x = element_text(size = f11_fs*0.7),
    legend.position = "bottom",
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  guides(fill = guide_legend(nrow = 3)) +
  labs(y = "Case", color = "Complex Model Feature", fill = "Complex Model Feature")

@
\caption{Feature heatmap of 36 LIME applications to the bullet comparison data test set. In addition to faceting the results by simulation method and LIME feature selection order, facets for the Gower power and whether the observation is a match or non-match are included. The vertical stripes of features selected indicate a dependence between the LIME explanations and tuning parameter values.}
\label{fig:figure-10}

\vspace*{\floatsep}

<<figure-11, out.width = '6.5in', warning = FALSE, fig.width = 10, fig.height = 4.25>>=

# Specify the figure size (for determining font size)
f12_ow = 6.5
f12_fw = 10
f12_fs = fs * (f12_fw / f12_ow)
f12_ls = 0.5 * (f12_fw / f12_ow)

# Create a metric comparison plot
plot_metrics(
  bullet_explain %>%
    mutate(label = as.factor(label)),
  add_lines = TRUE,
  line_alpha = 0.75,
  line_size = f12_ls
) +
  theme_bw(base_family = ff, base_size = f12_fs) +
  theme(
    strip.background = element_rect(color = "white", fill = "white"),
    strip.placement = "outside",
    strip.text.y = element_text(size = f12_fs)
  )

@
\caption{Assessment metric plot for the applications of LIME on the bullet comparison data test set. There are discrepancies in performance across metrics for a set of tuning parameter values, but based on the overall performance of all three metrics, 4-equal-bins with a Gower power of 0.5 appears to perform the best.}
\label{fig:figure-11}
\end{figure*}

To try to identify a set of LIME tuning parameter values with the most trustworthy set of explanations, an assessment metric plot is considered (\autoref{fig:figure-11}). The performance of a set of tuning parameter values often do not agree across metrics. For example, both density methods perform well according to average fidelity but poorly according to MSEE. All quantile bin methods perform well according to average $R^2$ but poorly based on average fidelity and MSEE. However, there is consistency in results across different Gower powers. The applications using a power of 0.5 perform the best or as well as the other powers across all simulation methods suggesting that the power that leads to a more global explanation is preferred by LIME.  It is not apparent which set of tuning parameters is the best. Considering all three metrics, we might conclude that the 4-equal-bins with a Gower power of 0.5 performs better than the other tuning parameter values considered.

To provide a more detailed view of explanations obtained using different tuning parameter values, we take a closer look at explanations for two observations of interest, which will be referred to as case M (a known match) and case NM (a known non-match), using explanation scatterplots. Figure~\ref{fig:figure-12} includes the visual representation plots of explanations (from \emph{lime}) and explanation scatterplots for cases M and NM. Plots for 4-equal-bins and 4-quantile-bins are included to depict tuning parameter values with good (4-equal-bins) and mediocre (4-quantile-bins) performance based on the assessment metric plot. Plots for the kernel density simulation method are included in Appendix~\ref{exp-scatter_plus} (Figure \ref{fig:figure-B2}). 

We focus on the explanation for case NM with 4-quantile-bins. The explanation plot from \emph{lime} shows that the observed values of NM of mismatches less than or equal to 7.17, rough correlation (rough\_cor) greater than 0.05, and D less than or equal to 0.002 support a random forest prediction in favor of a match. The scatter plots show that many of the simulated values in the rectangular regions with mismatches less than 7.17, rough correlation greater than 0.05, and distance less than or equal to 0.002 have random forest predictions greater than 0.5, which agrees with the LIME explanation. However, the random forest assigns a probability of 0.21 to case NM, which is not explained by the LIME explanation that provides all explanations in support of a match. Instead, consider that a better explanation based on the scatterplots would be that the random forest assigned a prediction probably below 0.5 since NM has observed values of mismatches less than 5, rough correlation greater than 0.25, and distance greater than 0.001.

The other explanation scatterplots can be interpreted in a similar manner. The three plots depict situations where the explanations are faithful to the random forest, but the regions captured by the intersections of the bins do not align well with the regions containing similar probabilities produced by the random forest.

\begin{figure*}[!thp]
<<figure-12, out.width = '6.25in', fig.width = 13, fig.height = 17.5, message = FALSE>>=

# Specify the figure size (for determining font size)
f13_ow = 6.25
f13_fw = 13
f13_fs = fs * (f13_fw / f13_ow)
f13_ls = 0.5 * (f13_fw / f13_ow)

# Adjust names of the cases
bullet_explain_perms_clean <- bullet_explain_perms %>%
  mutate(case = ifelse(case == bullet_poi_nonmatch, "NM", "M"))

# Create a theme for the lime explanation visualizations
bullet_explain_plot_theme <-
  list(
    scale_fill_manual(values = c("darkorange", "grey50")),
    theme(
      text = element_text(family = ff, size = f13_fs),
      strip.text.x = element_text(face = "plain", size = f13_fs)
    )
  )

# Create the lime explanation visualizations
pfM_3qb <- plot_features(bullet_explain_perms_clean[1:3,]) + bullet_explain_plot_theme
pfNM_3qb <- plot_features(bullet_explain_perms_clean[4:6,]) + bullet_explain_plot_theme
pfM_3eb <- plot_features(bullet_explain_perms_clean[7:9,]) + bullet_explain_plot_theme
pfNM_3eb <- plot_features(bullet_explain_perms_clean[10:12,]) + bullet_explain_plot_theme

# Create theme for explanation scatterplots
bullet_eoi_plot_theme <-
  list(
    scale_color_gradient2(
      low = "grey50",
      high = "darkorange",
      midpoint = 0.5,
      limits = c(0, 1)
    ),
    scale_fill_gradient2(
      low = "grey50",
      high = "darkorange",
      midpoint = 0.5,
      limits = c(0, 1)
    ),
    theme_bw(base_family = ff, base_size = f13_fs),
    theme(
      aspect.ratio = 1,
      plot.title = element_text(size = f13_fs),
      strip.text.x = element_text(size = f13_fs),
      strip.text.y = element_text(size = f13_fs),
      strip.placement = "outside",
      strip.background = element_rect(color = "white", fill = "white")
    ),
    guides(size = guide_legend(nrow = 2, byrow = T), 
           linetype = guide_legend(override.aes = list(color = c("grey50", "darkorange"))))
  )

# Create the explanation scatterplots
eoiM_3qb <-
  plot_explain_scatter(
    bullet_explain_perms_clean[1:3, ],
    alpha = 0.9,
    weights = TRUE,
    line_size = f13_ls
  ) +
  bullet_eoi_plot_theme +
  guides(linetype = guide_legend(override.aes = list(color = c("darkorange"))))
eoiNM_3qb <-
  plot_explain_scatter(
    bullet_explain_perms_clean[4:6, ],
    alpha = 0.9,
    weights = TRUE,
    line_size = f13_ls
  ) +
  bullet_eoi_plot_theme + 
  guides(linetype = guide_legend(override.aes = list(color = c("darkorange"))))
eoiM_3eb <-
  plot_explain_scatter(
    bullet_explain_perms_clean[7:9, ],
    alpha = 0.9,
    weights = TRUE,
    line_size = f13_ls
  ) +
  bullet_eoi_plot_theme + 
  guides(linetype = guide_legend(override.aes = list(color = c("darkorange"))))
eoiNM_3eb <-
  plot_explain_scatter(
    bullet_explain_perms_clean[10:12,],
    alpha = 0.9,
    weights = TRUE,
    line_size = f13_ls
  ) +
  bullet_eoi_plot_theme

# Join the plots
plot_grid(
  pfNM_3qb,
  pfNM_3eb,
  eoiNM_3qb,
  eoiNM_3eb,
  pfM_3qb,
  pfM_3eb,
  eoiM_3qb,
  eoiM_3eb,
  nrow = 4,
  rel_heights = c(0.2, 0.3, 0.2, 0.3)
)

@
\caption{Plots of LIME explanations (first and third rows) and explanation scatterplots (second and fourth rows) for  cases M and NM in the bullet test data for two tuning parameter values: 4-quantile-bins (first column) and 4-equal-bins (second column).  The plots provide insights into the LIME explanations and allow for the assessment of the explanation quality.}
\label{fig:figure-12}
\end{figure*}

Without applying LIME with multiple tuning parameter values to the bullet test data or viewing diagnostic plots of the LIME explanations, it may be very possible to formulate reasons why the LIME explanations make sense. However, the sequence of plots in this section (Figures \ref{fig:figure-10}, \ref{fig:figure-11}, and \ref{fig:figure-12}) suggest that we should be cautious to trust any of these LIME explanations. It appears that either LIME needs to be further tuned to provide trustworthy and good explanations, or a different approach may provide better insight. 

\section{Discussion} \label{discussion}

This paper highlights that while an explainer model is meant to provide clarity, it actually adds another layer of complexity to predictive models by requiring yet another model that needs to be assessed. Without an assessment of the explainer model, LIME is a black-box procedure of its own requiring blind trust in the explainer model. We suggest the use of visual diagnostics to counteract the black-box nature of LIME and provide three diagnostic plots.

The visualizations are intended to provide insight on how LIME works, assess the ability of the explainer model to capture the complex predictive model, and compare LIME explanations produced by different tuning parameter values. While the visualizations accomplish these tasks, they also expose examples of the failings of LIME. To address the discovered failings of LIME, we reconsider each of the claims about the performance of LIME made by \citet{ribeiro:2016} in light of the insights gained from the diagnostic visualizations.

As previously discussed, the \textbf{interpretability} of the LIME explanations is controllable  by the complexity of the explainer model. For example, the number of bins selected for simulation can control the interpretability of the explanations. If too many bins are selected, the bin range that is reported in the LIME explanation will be too small to be meaningful in the context of the feature. An appropriate choice of the number of bins will keep the bin range meaningful. Thus, the claim of interpretability does not need to be assessed using the visualizations. However, diagnostic visualizations do present a different perspective on the meaning of interpretability.

Even though an explanation will be interpretable as long as the complexity of the explainer model is appropriately chosen, a lack of understanding of the process used to create the explanation could lead to an incorrect interpretation of the explanation. For example, the visualization of a LIME explanation available from the \emph{lime} R package \citep{pedersen:2020} (shown in Figures \ref{fig:figure-03} and \ref{fig:figure-12}) is a major simplification of the explainer model, which could lead to under-interpreted or misinterpreted LIME explanation. Supplementing \citet{pedersen:2020}'s compact visualization of the explanation with an explanation scatterplot that shows a more detailed visualization of the explainer model promotes a more complete understanding of the explanation (such as Figures~\ref{fig:figure-03} and \ref{fig:figure-12}).

Even with an explainer model that is interpreted correctly, the interpretation is worthless if the explainer model is not \textbf{faithful} to the complex model. This claim can be assessed using the diagnostic plots suggested in this paper. Many of the visualizations in this paper highlight problems with the faithfulness of the explainer models. The explanation scatterplots allow for a comparison of the explainer model to the complex model. The examples in this paper show cases where the explainer model bins do not accurately capture the regions with similar random forest probabilities that contain the prediction of interest and oversimplify the model (Figures \ref{fig:figure-03} and \ref{fig:figure-12}). Using fewer bins would clearly not help improve the faithfulness of the explainer model in these examples, and while an increase in the number of bins would lead to a finer resolution of the random forest classification boundaries, interpretability of the explainer model would quickly be lost. Perhaps this could be improved by allowing the bin creation to account for the relationships between the features and response variable or a different number of bins for each feature.

In addition to assessing faithfulness by visually comparing the complex and explainer model, we propose a visual comparison of two faithfulness metrics (MSEE and average fidelity). The examples of faithfulness metric comparisons in this paper both produced conflicting results (Figures \ref{fig:figure-06} and \ref{fig:figure-11}). This makes it difficult to decide on a recommendation of a set of tuning parameter values that produce the explanations with the most faithful explainer model.

The metric comparison plot also includes a comparison of average $R^2$ values, which is a metric that can be used to assess the claim of \textbf{linearity}. Most of the average $R^2$ values in the examples from this paper are below 0.5 suggesting a poor linear fit of the explainer models. A poor linear fit of the explainer model is also seen with the residual plot (\autoref{fig:figure-C3}).

The final claim, \textbf{localness}, is addressed by the feature heatmap and metric comparison plots. As stated, the feature heatmap reveals that the density simulation methods in the \data \ example  results in global explanations where the same features are repeatedly chosen across all (or almost all) observations in the set of explanations. This finding agrees with that of \citet{laugel:2018} who found LIME produced global explanations with the normal approximation simulation method. For the bin based simulation methods in the bullet data example, the feature heatmap shows that the features chosen for the explanations vary between the two classification categories (match versus non-match). This is an interesting finding that suggests that different features can play a role in the predictions of observations in different response categories. Furthermore, while the metric comparison plot in the bullet example does not provide agreement between metrics on a best bin based method, all metrics agree that a Gower power of 0.5 for computing the model weights associated with distance of a simulated data point from the prediction of interest is the best. This suggests that a less local explanation provides a better explanation of the performance of the random forest.

Some of the visualizations in the paper generalize easily to any application of LIME such as the feature heatmap and metric plot. Other plots such as the visualizations of the LIME procedure would require extensions such as the use of scatterplot matrices to compare explanations with more than two features. The addition of interactivity to the diagnostic plots would provide additional enhancement of the assessment process. For example, a diagnostic plot that provides a summary of multiple LIME explanations, such as the feature heatmap, could be displayed and clicked on to reveal more detailed figures associated with individual predictions of interest, such as an explanation scatterplot.

The largest limitation to the diagnostic visualizations is the dimensionality of the data shown, both in the number of dimensions or features as well as the number of observations. Fortunately, in the situation of LIME, both of these aspects are rather well controlled: LIME relies heavily on simulations to generate data scenarios that are close to the data observed but exhibit variability. Effects from overplotting should be relatively mild, because output from simulations is shown, which is expected to be (relatively) continuous such that overplotting only occurs for points with (relatively) similar values. In that respect, the diagnostics shown for the \data \ and the bullet example are representative of what is expected. But in cases where overplotting does become problematic, the user could either simply reduce the size of the simulations or use some well-studied binning techniques in the visualizations, as discussed for example in \citet{carr:1987} or \citet{unwin:2006}. 

While it would be ideal if LIME could be used as a method to provide easily understandable explanations for black-box models as \citet{ribeiro:2016} claim, that dream is not yet a reality. The examples using diagnostic plots to assess LIME in this paper show frequent issues with LIME. We hope that our plots provide motivation to assess LIME explanations, to not blindly use the default settings (even if it is not clear how to tune the parameters), and to encourage work on improving LIME, so that it can be a lime and not a lemon.

\section*{Acknowledgments}

HH was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.

\subsection*{Author contributions}

{\bf Katherine Goode, Heike Hofmann}: Conceptualization, Methodology, Investigation, Visualizations, Writing - Reviewing and Editing. {\bf Katherine Goode}: Writing - Initial Draft, Software.

\subsection*{Financial disclosure}

None reported.

\subsection*{Conflict of interest}

The authors declare no potential conflict of interests.

\section*{Supporting information}

The code used to produce the manuscript and the data from examples in Section \ref{application} are available in the following GitHub Repository: \href{https://github.com/goodekat/LIME-diagnostics-paper}{https://github.com/goodekat/LIME-diagnostics-paper}.

\bibliography{references}

\newpage

\appendix

\section{LIME Tuning Parameter Options} \label{lime-details}

The following tuning parameters for the LIME algorithm are available in the \emph{lime} R package \citep{pedersen:2020}.

\begin{itemize}

\item Data simulation methods:

\begin{itemize}
\item Equally spaced bins: observations are uniformly sampled from equally spaced bins (number of bins may be specified)
\item Quantile bins: observations are uniformly sampled from quantile bins (number of bins may be specified)
\item Normal density approximation: observations are sampled from a normal distribution with mean and standard deviation computed from the corresponding feature
\item Kernel density approximation: observations are sampled from an kernel density approximation of the corresponding feature
\end{itemize}

\item Number of observations to simulate

\item Distance metric for determining proximity to the prediction of interest: Gower distance (where the power may be specified) or exponential kernel (where the kernel width may be specified)

\item Number of features to return in an explanation

\item Feature selection method for determining the features to return in an explanation: forward selection applied to a ridge regression, features with the largest magnitude coefficients in ridge regression, LASSO, classification/regression tree splits

\end{itemize}

\section{Explanation Scatterplots Under Other Simulation Scenarios} \label{exp-scatter_plus}

Section \ref{exp-scatter} introduces explanation scatterplots under the default simulation method in the \emph{lime} R package: 4-quantile-bins. The structure of an explanation scatterplot remains the same if any bin based simulation method is used, i.e., any number of quantile or equally spaced bins. However, if the kernel density or normal approximation simulation methods are used, the format of the explanation scatterplot changes. In the density based simulation method scenarios, LIME uses the standardized versions of the predictor variables to fit the explainer model. Thus, the explainer model needs to be represented differently in the explanation scatterplot.

When the kernel density or normal approximation simulation methods are applied, the explanation scatterplot depicts the complex model by plotting the complex model predictions versus a feature selected in LIME the explanation from the simulated data. The explainer model is included as a line on the figure where all features excluding the one plotted on the x-axis are set to the observed values of the prediction of interest. An explanation scatterplot is created for each feature included in the LIME explanation. As with the bin based simulation method, the size of the points represent the weight assigned by LIME.

Figure \ref{fig:figure-B1} provides example explanation scatterplots for each feature in the default LIME explanation obtained using the kernel density simulation method for the \data \ prediction of interest. Figure \ref{fig:figure-B2} includes explanation scatterplots for the explanations generated using kernel density simulation for the bullet example cases M and NM discussed in Section \ref{bullet-assess-ex}.

\begin{figure*}[!thp]
<<figure-B1, out.width = '6.5in', fig.width = 12, fig.height = 5, fig.align = "center">>=

# Specify the figure size (for determining font size and line size)
fb1_ow = 6.5
fb1_fw = 12
fb1_fs = fs * (fb1_fw / fb1_ow)
fb1_ls = 0.5 * (fb1_fw / fb1_ow)

# Create the explanation scatterplot
plot_explain_scatter(
  sine_lime_explain$explain %>%
    filter(sim_method == "kernel_density", case == sine_poi %>% pull(case)),
  alpha = 0.75,
  title.opt = FALSE, 
  line_size = fb1_ls 
) +
  theme_bw(base_family = ff, base_size = fb1_fs) +
  theme(
    strip.background = element_rect(fill = "white", color = "white"),
    strip.placement = "outside",
    strip.text.x = element_text(size = fb1_fs)
  )

@
\caption{Explanation scatterplots for the \data \ prediction of interest with the kernel density simulation method.}
\label{fig:figure-B1}
\end{figure*}

\begin{figure*}[!thp]
<<figure-B2, out.width = '6.5in', fig.width = 12, fig.height = 10, fig.align = "center">>=

# Specify the figure size (for determining font size and line size)
fb2_ow = 6.5
fb2_fw = 12
fb2_fs = fs * (fb2_fw / fb2_ow)
fb2_ls = 0.5 * (fb2_fw / fb2_ow)

# Create the explanation scatterplot: case M, kernel density
bullet_es_M <-
  plot_explain_scatter(
    bullet_explain_perms[16:18, ] %>% mutate(case = "NM"),
    alpha = 0.75,
    title.opt = TRUE,
    line_size = fb2_ls
  ) +
  scale_fill_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  theme_bw(base_family = ff, base_size = fb2_fs) +
  theme(
    aspect.ratio = 1,
    strip.background = element_rect(fill = "white", color = "white"),
    strip.placement = "outside",
    strip.text.x = element_text(size = fb2_fs),
    plot.title = element_text(size = fb2_fs)
  )

# Create the explanation scatterplot: case NM, kernel density
bullet_es_NM <-
  plot_explain_scatter(
    bullet_explain_perms[13:15, ] %>% mutate(case = "M"),
    alpha = 0.75,
    title.opt = TRUE, 
    line_size = fb2_ls
  ) +
  scale_fill_gradient2(
    low = "grey50",
    high = "darkorange",
    midpoint = 0.5,
    limits = c(0, 1)
  ) +
  theme_bw(base_family = ff, base_size = fb2_fs) +
  theme(
    aspect.ratio = 1,
    strip.background = element_rect(fill = "white", color = "white"),
    strip.placement = "outside",
    strip.text.x = element_text(size = fb2_fs), 
    plot.title = element_text(size = fb2_fs)
  )

# Join the plots
plot_grid(bullet_es_NM, bullet_es_M, nrow = 2)

@
\caption{Explanation scatterplots for LIME explanations using kernel density simulation for the cases M and NM of the bullet comparison test data.}
\label{fig:figure-B2}
\end{figure*}

\section{Explainer Model Residual Plot} \label{residual-plot}

In order to assess the claim of linearity for the \data \ prediction of interest discussed in Section \ref{exp-scatter}, we use one of the most basic diagnostics in a statistician's tool box and draw a residual plot for the explainer model. This is shown in Figure \ref{fig:figure-C3} with the explainer model residuals on the y-axis and explainer model predictions on the x-axis. The points along the x-axis have been jittered to ease the effect of the over-plotted points in the visualization. There is a clear increasing trend in the residuals as the explainer model predictions increase. This is a clear violation of the linearity assumption with the ridge regression model.

\begin{figure}[!thp]
<<figure-C3, out.width = '3.125in', fig.width = 5, fig.height = 4, warning = FALSE>>=

# Specify the figure size (for determining font size and line size)
fc3_ow = 3.125
fc3_fw = 5
fc3_fs = fs * (fc3_fw / fc3_ow)
fc3_ls = 0.5 * (fc3_fw / fc3_ow)

# Calculate the residuals from the explainer model
y <- sine_poi_perms$rfpred
sine_poi_explainer <-
  function(x1, x2)
    sine_b0 + (sine_b1 * x1) + (sine_b2 * x2)
sine_poi_perms <- sine_poi_perms %>%
  mutate(exppred = sine_poi_explainer(x1num, x2num)) %>%
  mutate(resid = exppred - rfpred)

# Create the residual plot
ggplot(sine_poi_perms, aes(x = exppred, y = resid)) +
  geom_hline(yintercept = 0, size = fc3_ls) +
  geom_jitter(alpha = 0.5, width = 0.02) +
  theme_bw(base_family = ff, base_size = fc3_fs) +
  theme(plot.title = element_text(size = fc3_fs)) +
  labs(x = "Explainer Model Predictions",
       y = "Explainer Model Residuals",
       title = "Explainer Model Residual Plot")

@
\caption{Residual plot of the explainer model associated with \data \ prediction of interest from Section \ref{exp-scatter}. The residuals are plotted against the predicted values. The points are jittered in the x-direction to alleviate the over-plotting of points. There is an upward trend in the residuals as the explainer model predictions increase, which suggests a violation of the linearity assumption.}
\label{fig:figure-C3}
\end{figure}

\section{Details on Assessment Metrics} \label{metric-details}

Suppose $f$ is a complex model, and let $\textbf{X}$ be a matrix of observed data with $K$ features and $E$ observations where $x_e$ is an observed feature vector for observation $e$. Let $f(x_e)$ be the complex model prediction for observation $e$. It is of interest to explain the predictions made by $f$ applied to $X$ using LIME. 

For $x_e$ and a set of tuning parameter values $t$, let $\textbf{X}_{e,t}'$ be the LIME simulated dataset with $K$ features and $S$ rows such that $x'_{e,t,s}$ is the feature vector for simulated data point $s$ corresponding to explanation $e$ and tuning parameter values $t$. Let $\textbf{Z}'_{e,t}$ be the matrix of simulated data transformed to bin indicator variables (for bin based simulation methods) or standardization (for density based simulation methods) with $K$ features and $S$ observations such that $z'_{e,t,s}$ is the interpretability transformed feature vector for explanation $e$, tuning parameter values $t$, and simulated data point $s$. Note that $z_{e,t}$ will represent the  transformed version of $x_e$.

Next, let $\omega_t$ represent a proximity distance metric corresponding to tuning parameter values $t$. Then $\omega_t\left(x_e, x'_{e,t,s}\right)$ is the weight assigned to $x'_{e,t,s}$, which is the proximity between $x_e$ and $x'_{e,t,s}$. Allow $g_{e,t}$ to be the explainer model for an explanation $e$ and tuning parameter values $t$. Thus, $g_{e,t}\left(z'_{e,s,t}\right)$ is the explainer model prediction for the interpretability transformed simulated data point $s$. 

For a set of $E$ explanations and a set of tuning parameter values $t$, we define the assessment metrics as follows:\\
\\
\emph{Average $R^2$} is denoted as $R^2_{\mbox{ave}}$ and computed as

  $$R^2_{\mbox{ave}} = \frac{1}{E}\sum_{e=1}^E R_{e,t}^2$$

\noindent where $R_{e,t}^2$ is the $R^2$ value for $g_{e,t}$.\\
\\
\emph{Average fidelity} is denoted by $\mathcal{L}_{\mbox{ave}}$ and computed as

\begin{eqnarray*} \mathcal{L}_{\mbox{ave}} & = & \frac{1}{E}\sum_{e=1}^E\mathcal{L}(f, \ g_{e,t}, \ \pi_{t}) \\ & = & \frac{1}{E}\sum_{e=1}^E\sum_{s=1}^{S}\omega_{t}\left(x_e, x'_{e,t,s}\right)\left(f\left(x'_{e,t,s}\right)-g_{e,t}\left(z_{e,t,s}'\right)\right)^2. \end{eqnarray*}

\noindent where $\mathcal{L}$ is the fidelity metric originally defined in \citet{ribeiro:2016}.\\
\\
\emph{Mean squared explanation error} is denoted by MSEE and computed as

$$MSEE=\frac{1}{E}\sum_{e=1}^E\left(f\left(x_e\right)-g_{e,t}\left(z_{e,t}\right)\right)^2.$$

\end{document}
